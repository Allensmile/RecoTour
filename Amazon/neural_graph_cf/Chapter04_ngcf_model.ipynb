{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Graph Collaborative Filtering\n",
    "\n",
    "Here I intend to describe the algorithm presented in [Wang Xiang et al. Neural Graph Collaborative Filtering](https://arxiv.org/pdf/1905.08108.pdf) and my implementation using pytorch. \n",
    "\n",
    "I will go step by step (where step is mostly defined by my understanding) with snippets in tensorflow and their corresponding \"translation\" into pytorch.\n",
    "\n",
    "Here we go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import random as rd\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from torch import nn\n",
    "from utils.dataset import Data\n",
    "from utils.metrics import *\n",
    "from utils.parser import parse_args\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be \"imperatively\" comparing `tf` and `pytorch`, and for that we need `tf` eagter execution enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using our toy example so this notebook can initially run in any laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_users=1000, n_items=2000\n",
      "n_interactions=30780\n",
      "n_train=24228, n_test=6552, sparsity=0.01539\n",
      "already load adj matrix (3000, 3000) 0.0105438232421875\n"
     ]
    }
   ],
   "source": [
    "data_path = \"Data/toy_data/\"\n",
    "batch_size = 32\n",
    "# for the toy dataset I did not create a validation set\n",
    "data_generator = Data(data_path, batch_size, val=False)\n",
    "_, _, mean_adj = data_generator.get_adj_mat()\n",
    "adjacency_matrix = mean_adj + sp.eye(mean_adj.shape[0])\n",
    "n_users = data_generator.n_users\n",
    "n_items = data_generator.n_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "emb_size = 12\n",
    "layers = [12, 6]\n",
    "n_layers = len(layers)\n",
    "node_dropout = 0.1\n",
    "mess_dropout = [0.1]*len(layers)\n",
    "regularization = 1e-5\n",
    "lr = 0.01\n",
    "n_fold = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialise weights. They do it internally in their `NGCF` class, considering already the use of pre-trained weights. In my case, I will simply initialise all weights as if there were not pretrained and I will deal with pretrained weights outside the class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_data = None\n",
    "def _init_weights_tf():\n",
    "    all_weights = dict()\n",
    "\n",
    "    initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "    if pretrain_data is None:\n",
    "        all_weights['user_embedding'] = tf.Variable(initializer([n_users, emb_size]), name='user_embedding')\n",
    "        all_weights['item_embedding'] = tf.Variable(initializer([n_items, emb_size]), name='item_embedding')\n",
    "        print('using xavier initialization')\n",
    "    else:\n",
    "        all_weights['user_embedding'] = tf.Variable(initial_value=pretrain_data['user_embed'], trainable=True,\n",
    "                                                    name='user_embedding', dtype=tf.float32)\n",
    "        all_weights['item_embedding'] = tf.Variable(initial_value=pretrain_data['item_embed'], trainable=True,\n",
    "                                                    name='item_embedding', dtype=tf.float32)\n",
    "        print('using pretrained initialization')\n",
    "\n",
    "    weight_size_list = [emb_size] + layers\n",
    "    for k in range(n_layers):\n",
    "        # k = 0 are the embeddings\n",
    "        all_weights['W_gc_%d' %k] = tf.Variable(\n",
    "            initializer([weight_size_list[k], weight_size_list[k+1]]), name='W_gc_%d' % k)\n",
    "        all_weights['b_gc_%d' %k] = tf.Variable(\n",
    "            initializer([1, weight_size_list[k+1]]), name='b_gc_%d' % k)\n",
    "\n",
    "        all_weights['W_bi_%d' % k] = tf.Variable(\n",
    "            initializer([weight_size_list[k], weight_size_list[k + 1]]), name='W_bi_%d' % k)\n",
    "        all_weights['b_bi_%d' % k] = tf.Variable(\n",
    "            initializer([1, weight_size_list[k+1]]), name='b_bi_%d' % k)\n",
    "    return all_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using xavier initialization\n"
     ]
    }
   ],
   "source": [
    "weights = _init_weights_tf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['user_embedding', 'item_embedding', 'W_gc_0', 'b_gc_0', 'W_bi_0', 'b_bi_0', 'W_gc_1', 'b_gc_1', 'W_bi_1', 'b_bi_1'])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 12)\n",
      "(2000, 12)\n",
      "(12, 12)\n",
      "(12, 6)\n"
     ]
    }
   ],
   "source": [
    "print(weights['user_embedding'].shape)\n",
    "print(weights['item_embedding'].shape)\n",
    "print(weights['W_gc_0'].shape)\n",
    "print(weights['W_gc_1'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch\n",
    "\n",
    "As I mentioned, I will simply initialise the weights as with no pretrained and I will deal with pretrained weights outside the model class. \n",
    "\n",
    "In pytorch I simply do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not run\n",
    "def _init_weights_tf(self):\n",
    "    for m in self.modules():\n",
    "        if isinstance(m, nn.Embedding):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's now focus on the 4 helpers that will be needed to create the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert sparse matrix to sparse tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_sp_mat_to_sp_tensor_tf(X):\n",
    "    coo = X.tocoo().astype(np.float32)\n",
    "    indices = np.mat([coo.row, coo.col]).transpose()\n",
    "    return tf.SparseTensor(indices, coo.data, coo.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Node Dropout a for sparse tensor (see below, I'd call this edge dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _dropout_sparse(X, keep_prob, n_nonzero_elems):\n",
    "    \"\"\"\n",
    "    Dropout for sparse tensors.\n",
    "    \"\"\"\n",
    "    noise_shape = [n_nonzero_elems]\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += tf.random_uniform(noise_shape)\n",
    "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "    pre_out = tf.sparse_retain(X, dropout_mask)\n",
    "    \n",
    "    return pre_out * tf.math.divide(1., keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the large sparse adjancecy matrix in n folds, with/without node dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_A_hat(X):\n",
    "    \"split the Adjancency matrix so is tractable\"\n",
    "    A_fold_hat = []\n",
    "\n",
    "    fold_len = (n_users + n_items) // n_fold\n",
    "    for i_fold in range(n_fold):\n",
    "        start = i_fold * fold_len\n",
    "        if i_fold == n_fold -1:\n",
    "            end = n_users + n_items\n",
    "        else:\n",
    "            end = (i_fold + 1) * fold_len\n",
    "\n",
    "        A_fold_hat.append(_convert_sp_mat_to_sp_tensor_tf(X[start:end]))\n",
    "    return A_fold_hat\n",
    "\n",
    "def _split_A_hat_node_dropout(X):\n",
    "    A_fold_hat = []\n",
    "\n",
    "    fold_len = (n_users + n_items) // n_fold\n",
    "    for i_fold in range(n_fold):\n",
    "        start = i_fold * fold_len\n",
    "        if i_fold == n_fold -1:\n",
    "            end = n_users + n_items\n",
    "        else:\n",
    "            end = (i_fold + 1) * fold_len\n",
    "\n",
    "        temp = _convert_sp_mat_to_sp_tensor_tf(X[start:end])\n",
    "        n_nonzero_temp = X[start:end].count_nonzero()\n",
    "        A_fold_hat.append(_dropout_sparse(temp, 1 - node_dropout, n_nonzero_temp))\n",
    "\n",
    "    return A_fold_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving into the code, I would like stop one second in their method `_dropout_sparse`. This method is intended to drop out nodes, and all their connections. However, I do not think the code in their original repo (the same as above) does that. I think their code drops edges. Let's see. After running: \n",
    "\n",
    "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "   \n",
    "they have a boolean tensor indicating **the locations** to keep (see [here](https://www.tensorflow.org/api_docs/python/tf/sparse/retain)). Therefore, when applying `dropout_mask` through: \n",
    "\n",
    "    pre_out = tf.sparse_retain(X, dropout_mask)\n",
    "    \n",
    "they are removing specific locations, i.e. edges, not a node. Dropping a node (with all its connections) would imply dropping an entire row of the adjancency matrix, not only locations.\n",
    "\n",
    "Anyway, let's see how those functions work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3000x3000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 51456 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remember\n",
    "adjacency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_tensor = _convert_sp_mat_to_sp_tensor_tf(adjacency_matrix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 3000)\n"
     ]
    }
   ],
   "source": [
    "print(adjacency_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_fold_hat = _split_A_hat_node_dropout(adjacency_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(300), Dimension(3000)])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_fold_hat[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randint(1,5, (5, 3))\n",
    "X_csr = sp.csc_matrix(X)\n",
    "X_coo = X_csr.tocoo().astype(np.float32)\n",
    "indices = np.mat([X_coo.row, X_coo.col]).transpose()\n",
    "res = tf.SparseTensor(indices, X_coo.data, X_coo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.framework.sparse_tensor.SparseTensor at 0x7fe29463bfd0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _dropout_sparse(X, keep_prob, n_nonzero_elems):\n",
    "    \"\"\"\n",
    "    Dropout for sparse tensors.\n",
    "    \"\"\"\n",
    "    noise_shape = [n_nonzero_elems]\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += tf.random_uniform(noise_shape)\n",
    "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "    pre_out = tf.sparse_retain(X, dropout_mask)\n",
    "    \n",
    "    return pre_out * tf.math.divide(1., keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go step by step to understand what the `_dropout_sparse` function does. Let's create a sparse tensor, which is the input that `_dropout_sparse` yakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((10,10))\n",
    "coord = [(np.random.randint(10), np.random.randint(10)) for i in range(5)]\n",
    "for i,j in coord: X[i,j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sp_mtx = sp.csr_matrix(X)\n",
    "n_nonzero_elems = X_sp_mtx.count_nonzero()\n",
    "X_sp_tsr = _convert_sp_mat_to_sp_tensor(X_sp_mtx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a tensor with values uniformly randomly distributed between 0 and 1 to which we have added `1 - node_dropout[0] -> the keep_prob` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_shape = [n_nonzero_elems]\n",
    "random_tensor = 1 - node_dropout[0]\n",
    "random_tensor += tf.random_uniform(noise_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we create a mask dropping rows (i.e. nodes) according to the `node_dropout` param."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if you run this in eager execution (just don't use `placeholders`) you will see that dropout_mask containes booleans, that will indicate whether we drop or not a certain row/node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Cast:0' shape=(5,) dtype=bool>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0807 13:58:25.327770 140613546653440 deprecation.py:323] From /home/ubuntu/anaconda3/envs/ngcf/lib/python3.6/site-packages/tensorflow/python/ops/sparse_ops.py:1719: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "pre_out = tf.sparse_retain(X_sp_tsr, dropout_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just now split the Adjancency matrix so is tractable, nothing major here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_A_hat(X):\n",
    "    \"split the Adjancency matrix so is tractable\"\n",
    "    A_fold_hat = []\n",
    "\n",
    "    fold_len = (n_users + n_items) // n_fold\n",
    "    for i_fold in range(n_fold):\n",
    "        start = i_fold * fold_len\n",
    "        if i_fold == n_fold -1:\n",
    "            end = n_users + n_items\n",
    "        else:\n",
    "            end = (i_fold + 1) * fold_len\n",
    "\n",
    "        A_fold_hat.append(_convert_sp_mat_to_sp_tensor(X[start:end]))\n",
    "    return A_fold_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_A_hat_node_dropout(X):\n",
    "    A_fold_hat = []\n",
    "\n",
    "    fold_len = (n_users + n_items) // n_fold\n",
    "    for i_fold in range(n_fold):\n",
    "        start = i_fold * fold_len\n",
    "        if i_fold == n_fold -1:\n",
    "            end = n_users + n_items\n",
    "        else:\n",
    "            end = (i_fold + 1) * fold_len\n",
    "\n",
    "        temp = _convert_sp_mat_to_sp_tensor(X[start:end])\n",
    "        n_nonzero_temp = X[start:end].count_nonzero()\n",
    "        A_fold_hat.append(_dropout_sparse(temp, 1 - node_dropout[0], n_nonzero_temp))\n",
    "\n",
    "    return A_fold_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_dropout_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a set of adjacency sub-matrix.\n",
    "if node_dropout_flag:\n",
    "    # node dropout.\n",
    "    A_fold_hat = _split_A_hat_node_dropout(norm_adj)\n",
    "else:\n",
    "    A_fold_hat = _split_A_hat(norm_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(A_fold_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(708), Dimension(70839)])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_fold_hat[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here they build embeddings based on, or thinking in ego-networks, which, quoting directly [this page](http://www.analytictech.com/networks/egonet.htm): Ego networks consist of a focal node (\"ego\") and the nodes to whom ego is directly connected to (these are called \"alters\") plus the ties, if any, among the alters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_embeddings = tf.concat([weights['user_embedding'], weights['item_embedding']], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(70839), Dimension(64)])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ego_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook we only use 1 layer. This is, in their Figure 2 there will be only one `Embedding Propagation Layer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`A_fold_hat[0]` is a subset of the adjacency matrix, with dimensions `TensorShape([Dimension(708), Dimension(70839)])`. This matrix has non-zero elements in those locations where there are node connections. For example, the first row will correspond to the 1st user that interacted with the 1st 127 items (0 to 126). \n",
    "`row_0 (1st element is the user_id) = 0 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "nz_idx=np.where(norm_adj.todense()[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0, 29858, 29859, 29860, 29861, 29862, 29863, 29864, 29865,\n",
       "       29866, 29867, 29868, 29869, 29870, 29871, 29872, 29873, 29874,\n",
       "       29875, 29876, 29877, 29878, 29879, 29880, 29881, 29882, 29883,\n",
       "       29884, 29885, 29886, 29887, 29888, 29889, 29890, 29891, 29892,\n",
       "       29893, 29894, 29895, 29896, 29897, 29898, 29899, 29900, 29901,\n",
       "       29902, 29903, 29904, 29905, 29906, 29907, 29908, 29909, 29910,\n",
       "       29911, 29912, 29913, 29914, 29915, 29916, 29917, 29918, 29919,\n",
       "       29920, 29921, 29922, 29923, 29924, 29925, 29926, 29927, 29928,\n",
       "       29929, 29930, 29931, 29932, 29933, 29934, 29935, 29936, 29937,\n",
       "       29938, 29939, 29940, 29941, 29942, 29943, 29944, 29945, 29946,\n",
       "       29947, 29948, 29949, 29950, 29951, 29952, 29953, 29954, 29955,\n",
       "       29956, 29957, 29958, 29959, 29960, 29961, 29962, 29963, 29964,\n",
       "       29965, 29966, 29967, 29968, 29969, 29970, 29971, 29972, 29973,\n",
       "       29974, 29975, 29976, 29977, 29978, 29979, 29980, 29981, 29982,\n",
       "       29983, 29984])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nz_idx[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember we added a diagonal to account for node-self-connections. We can see that the location of the 1st non-zero element, apart from that in the diagonal (1,1) is 29859, so all good. When multiplying that row by the tensor `ego_embeddings` you will get the summation of the embeddings of the user 0 plus the weighted embeddings (remember the matrix is normalised) corresponding to items from 0-126. \n",
    "\n",
    "(1,70839) x (70839 x 64) = (1,64)\n",
    "\n",
    "Let's have a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = tf.sparse_tensor_dense_matmul(A_fold_hat[0], ego_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(708), Dimension(64)])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we move down the adjacency matrix to row index 29858, remember from there in advance is filled with `R.T`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "nz_idx_2=np.where(norm_adj.todense()[29858, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,   140,   241,  1056,  1445,  2539,  3216,  3403,  5216,\n",
       "        9443, 24111, 26313, 29184, 29858])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nz_idx_2[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the users that interacted with item 0, so when we multiply the corresponding A_fold_hat with  ego_embeddings we get the summation of the embeddings for those users plus the embedding for item 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_embed = []\n",
    "for f in range(n_fold):\n",
    "    temp_embed.append(tf.sparse_tensor_dense_matmul(A_fold_hat[f], ego_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "side_embeddings = tf.concat(temp_embed, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(70839), Dimension(64)])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "side_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 1st row of this matrix contains the weighted sum of all the item embeddings that user_id 0 interacted with plus the embeddings of that user while rwo # n_users+1 contains the summation of all user embeddings that interacted with item_id 0 plus the embeddings of that item.  \n",
    "\n",
    "At this stage we have multiplied all embeddings by the adjacency matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(64), Dimension(64)])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights['W_gc_0'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This below corresponds to the 1st term in their expression (3). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_embeddings = tf.nn.leaky_relu(\n",
    "    tf.matmul(side_embeddings, weights['W_gc_%d' % k]) + weights['b_gc_%d' % k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(70839), Dimension(64)])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, ego_embeddings is simply the concatenation (over rows) of the user and item embeddings and  side_embeddings contains the weighted sum of the item embeddings that a certain user interacted with (plus the embeddings of that user) and the weighted sum of the user embeddings that an item \"interacted\" with (plus the embeddings of that item). \n",
    "\n",
    "We now move to the second term of their expression (3). They call `bi_embeddings` to the element-wise multiplication of the two tensors (and then we apply the same operation as before). In their own words: \"we additionally\n",
    "encode the interaction between ei and eu into the message being passed via ei ⊙ eu , where ⊙ denotes the element-wise product. This makes the message dependent on the affinity between ei and eu , e.g., passing more messages from the similar items. This not only increases the model representation ability, but also boosts the performance for recommendation \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bi messages of neighbors.\n",
    "bi_embeddings = tf.multiply(ego_embeddings, side_embeddings)\n",
    "# transformed bi messages of neighbors.\n",
    "bi_embeddings = tf.nn.leaky_relu(\n",
    "    tf.matmul(bi_embeddings, weights['W_bi_%d' % k]) + weights['b_bi_%d' % k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(70839), Dimension(64)])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Their expression (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_embeddings = sum_embeddings + bi_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0807 14:23:39.308569 140613546653440 deprecation.py:506] From <ipython-input-60-c7abd9f97775>:2: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# message dropout.\n",
    "ego_embeddings = tf.nn.dropout(ego_embeddings, 1 - mess_dropout[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here they add an extra normalization that I can't find in the paper, since the Laplacian norm that they refer to is already applied we built the Adjacency Matrix..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the distribution of embeddings.\n",
    "norm_embeddings = tf.math.l2_normalize(ego_embeddings, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(70839), Dimension(64)])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper they show how they concatenate the output from all the `Embedding Propagation Layer` + the so called `ego_embeddings`. This would be done recursively if n_layers would be > 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = [ego_embeddings] + [norm_embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = tf.concat(all_embeddings, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(70839), Dimension(128)])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_g_embeddings, i_g_embeddings = tf.split(all_embeddings, [n_users, n_items], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29858, 128)\n",
      "(40981, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(u_g_embeddings.shape), print(i_g_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it, that is the model. Nonetheless, and even though we will refer to this function in the corresponding training notebook, let's described here the pairwise Bayesian Personalized Ranking (BPR) loss as implemented in their paper. This loss considers the relative order between observed and unobserved user-item interactions. Specifically, BPR assumes that the observed interactions, which are more reflective of a user’s preferences, should be assigned higher prediction values than unobserved ones. Let's see it in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bpr_loss(users, pos_items, neg_items):\n",
    "    pos_scores = tf.reduce_sum(tf.multiply(users, pos_items), axis=1)\n",
    "    neg_scores = tf.reduce_sum(tf.multiply(users, neg_items), axis=1)\n",
    "\n",
    "    # regularization term\n",
    "    regularizer = tf.nn.l2_loss(users) + tf.nn.l2_loss(pos_items) + tf.nn.l2_loss(neg_items)\n",
    "    regularizer = regularizer/batch_size\n",
    "    emb_loss = decay * regularizer\n",
    "\n",
    "    # First term of their expression 11\n",
    "    maxi = tf.log(tf.nn.sigmoid(pos_scores - neg_scores))\n",
    "    mf_loss = tf.negative(tf.reduce_mean(maxi))\n",
    "\n",
    "    # ? no idea...a 0.0 constant\n",
    "    reg_loss = tf.constant(0.0, tf.float32, [1])\n",
    "\n",
    "    return mf_loss, emb_loss, reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_g_embeddings = tf.nn.embedding_lookup(tf.identity(u_g_embeddings), users)\n",
    "pos_i_g_embeddings = tf.nn.embedding_lookup(i_g_embeddings, pos_items)\n",
    "neg_i_g_embeddings = tf.nn.embedding_lookup(i_g_embeddings, neg_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_loss, emb_loss, reg_loss = create_bpr_loss(u_g_embeddings,pos_i_g_embeddings,neg_i_g_embeddings)\n",
    "loss = mf_loss + emb_loss + reg_loss\n",
    "opt = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_8:0' shape=(1,) dtype=float32>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
