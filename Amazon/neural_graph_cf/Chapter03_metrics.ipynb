{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Metrics\n",
    "\n",
    "Before we move onto the model, and how is trained and tested, let's quickly go through the metrics that we will use here. The first part of the code below is either a direct copy/paste from the original [repo](https://github.com/xiangwang1223/neural_graph_collaborative_filtering) or a minor adaptation. When this is not the case I will explain the corresponding details. Therefore, **all credit for the authors**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import heapq\n",
    "\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.random.randint(2, size=20)\n",
    "k = 10\n",
    "n_inter = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(r, k, n_inter):\n",
    "    \"\"\"recall @ k\n",
    "    Parameters:\n",
    "    ----------\n",
    "    r: Iterable\n",
    "        binary iterable (nonzero is relevant).\n",
    "    k: Int\n",
    "        number of recommendations to consider\n",
    "    n_inter: Int\n",
    "        number of interactions\n",
    "    Returns:\n",
    "    ----------\n",
    "    recall @ k\n",
    "    \"\"\"\n",
    "    r = np.asfarray(r)[:k]\n",
    "    return np.sum(r) / n_inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_at_k(r, k, n_inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(r, k):\n",
    "    \"\"\"precision @ k\n",
    "    Parameters:\n",
    "    ----------\n",
    "    r: Iterable\n",
    "        binary iterable (nonzero is relevant).\n",
    "    k: Int\n",
    "        number of recommendations to consider\n",
    "    Returns:\n",
    "    ----------\n",
    "    Precision @ k\n",
    "    \"\"\"\n",
    "    assert k >= 1\n",
    "    r = np.asarray(r)[:k]\n",
    "    return np.mean(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_at_k(r, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_at_k(r, k, method=1):\n",
    "    \"\"\" discounted cumulative gain (dcg) @ k\n",
    "    Parameters:\n",
    "    ----------\n",
    "    r: Iterable\n",
    "        Relevance is positive real values. If binary, nonzero is relevant.\n",
    "    k: Int\n",
    "        number of recommendations to consider\n",
    "    method: Int\n",
    "        one of 0 or 1. Simply, different dcg implementations\n",
    "    Returns:\n",
    "    ----------\n",
    "    dcg @ k\n",
    "    \"\"\"\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        if method == 0:\n",
    "            return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\n",
    "        elif method == 1:\n",
    "            return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "        else:\n",
    "            raise ValueError('method must be 0 or 1.')\n",
    "    return 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.25667559290693"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcg_at_k(r, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.736711950964459"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcg_at_k(r, k, method=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_at_k(r, k, method=1):\n",
    "    \"\"\" Normalized discounted cumulative gain @ k\n",
    "    \"\"\"\n",
    "    dcg_max = dcg_at_k(sorted(r, reverse=True), k, method)\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k, method) / dcg_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49667571720465364"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndcg_at_k(r, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_at_k(r, k):\n",
    "    \"\"\"hit ratio @ k\n",
    "    Parameters:\n",
    "    ----------\n",
    "    r: Iterable\n",
    "        binary iterable (nonzero is relevant).\n",
    "    k: Int\n",
    "        number of recommendations to consider\n",
    "    Returns:\n",
    "    ----------\n",
    "    hit ratio @ k\n",
    "    \"\"\"\n",
    "    r = np.array(r)[:k]\n",
    "    if np.sum(r) > 0:\n",
    "        return 1.\n",
    "    else:\n",
    "        return 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hit_at_k(r,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auc(item_score, user_pos_test):\n",
    "    \"\"\"Wrap up around sklearn's roc_auc_score\n",
    "    Parameters:\n",
    "    ----------\n",
    "    item_score: Dict\n",
    "        Dict. keys are item_ids, values are predictions\n",
    "    user_pos_test: List\n",
    "        List with the items that the user actually interacted with\n",
    "    Returns:\n",
    "    ----------\n",
    "    res: Float\n",
    "        roc_auc_score\n",
    "    \"\"\"\n",
    "    item_score = sorted(item_score.items(), key=lambda kv: kv[1])\n",
    "    item_score.reverse()\n",
    "    item_id = [x[0] for x in item_score]\n",
    "    score = [x[1] for x in item_score]\n",
    "\n",
    "    r = []\n",
    "    for i in item_id:\n",
    "        if i in user_pos_test:\n",
    "            r.append(1)\n",
    "        else:\n",
    "            r.append(0)\n",
    "\n",
    "    try:\n",
    "        res = roc_auc_score(r, score)\n",
    "    except Exception:\n",
    "        res = 0.\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the function inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example...let's assume 100 items in total\n",
    "item_score = {k:v for k,v in zip(np.arange(100), np.random.rand(100))}\n",
    "user_pos_test = np.random.choice(100, 20, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5856250000000001"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_auc(item_score, user_pos_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc(true, pred):\n",
    "    \"\"\"Simple wrap up around sklearn's roc_auc_score\n",
    "    \"\"\"\n",
    "    try:\n",
    "        res = roc_auc_score(true, pred)\n",
    "    except Exception:\n",
    "        res = 0.\n",
    "    return res\n",
    "\n",
    "\n",
    "def ranklist_by_sorted(user_pos_test, test_items, rating, Ks):\n",
    "    \"\"\"\n",
    "    Retursn a binary list, where relevance is nonzero, based on a ranked list\n",
    "    with the n largest scores. Also returns the AUC\n",
    "    Parameters:\n",
    "    ----------\n",
    "    user_pos_test: List\n",
    "        List with the items that the user actually interacted with\n",
    "    test_items: List\n",
    "        List with the all items in the test dataset\n",
    "    rating: List\n",
    "        List with the ratings corresponding to test_items\n",
    "    Ks: Int or List\n",
    "        the k in @k\n",
    "    Returns:\n",
    "    ----------\n",
    "    r: binary list where nonzero in relevant\n",
    "    auc: testing roc_auc_score\n",
    "    \"\"\"\n",
    "    item_score = {}\n",
    "    for i in test_items:\n",
    "        item_score[i] = rating[i]\n",
    "\n",
    "    K_max = max(Ks)\n",
    "    K_max_item_score = heapq.nlargest(K_max, item_score, key=item_score.get)\n",
    "\n",
    "    r = []\n",
    "    for i in K_max_item_score:\n",
    "        if i in user_pos_test:\n",
    "            r.append(1)\n",
    "        else:\n",
    "            r.append(0)\n",
    "    auc = get_auc(item_score, user_pos_test)\n",
    "    return r, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_items, rating = list(item_score.keys()), list(item_score.values())\n",
    "Ks = [5,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 1, 0, 0, 0, 1, 0, 0, 0, 0], 0.5856250000000001)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranklist_by_sorted(user_pos_test, test_items, rating, Ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranklist_by_heapq(user_pos_test, test_items, rating, Ks):\n",
    "    \"\"\"\n",
    "    Retursn a binary list, where relevance is nonzero, based on a ranked list\n",
    "    with the n largest scores. For consistency with ranklist_by_sorted, also\n",
    "    returns auc=0 (since auc does not make sense within a mini batch)\n",
    "    Parameters:\n",
    "    ----------\n",
    "    user_pos_test: List\n",
    "        List with the items that the user actually interacted with\n",
    "    test_items: List\n",
    "        List with the all items in the test dataset\n",
    "    rating: List\n",
    "        List with the ratings corresponding to test_items\n",
    "    Ks: Int or List\n",
    "        the k in @k\n",
    "    Returns:\n",
    "    ----------\n",
    "    r: binary list where nonzero in relevant\n",
    "    \"\"\"\n",
    "    item_score = {}\n",
    "    for i in test_items:\n",
    "        item_score[i] = rating[i]\n",
    "\n",
    "    K_max = max(Ks)\n",
    "    K_max_item_score = heapq.nlargest(K_max, item_score, key=item_score.get)\n",
    "\n",
    "    r = []\n",
    "    for i in K_max_item_score:\n",
    "        if i in user_pos_test:\n",
    "            r.append(1)\n",
    "        else:\n",
    "            r.append(0)\n",
    "    auc = 0.\n",
    "    return r, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 1, 0, 0, 0, 1, 0, 0, 0, 0], 0.0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranklist_by_heapq(user_pos_test, test_items, rating, Ks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, getting altogether:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance(user_pos_test, r, auc, Ks):\n",
    "    \"\"\"wrap up around all other previous functions\n",
    "    ----------\n",
    "    user_pos_test: List\n",
    "        List with the items that the user actually interacted with\n",
    "    r: List\n",
    "        binary list where nonzero in relevant\n",
    "    auc: Float\n",
    "        sklearn's roc_auc_score\n",
    "    Ks: List\n",
    "        the k in @k\n",
    "    Returns:\n",
    "    ----------\n",
    "    dictionary of metrics\n",
    "    \"\"\"\n",
    "\n",
    "    precision, recall, ndcg, hit_ratio = [], [], [], []\n",
    "\n",
    "    for K in Ks:\n",
    "        precision.append(precision_at_k(r, K))\n",
    "        recall.append(recall_at_k(r, K, len(user_pos_test)))\n",
    "        ndcg.append(ndcg_at_k(r, K))\n",
    "        hit_ratio.append(hit_at_k(r, K))\n",
    "\n",
    "    return {'recall': np.array(recall), 'precision': np.array(precision),\n",
    "            'ndcg': np.array(ndcg), 'hit_ratio': np.array(hit_ratio), 'auc': auc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, auc = ranklist_by_sorted(user_pos_test, test_items, rating, Ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': array([0.1 , 0.15]),\n",
       " 'precision': array([0.4, 0.3]),\n",
       " 'ndcg': array([0.76536064, 0.93252109]),\n",
       " 'hit_ratio': array([1., 1.]),\n",
       " 'auc': 0.5856250000000001}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_performance(user_pos_test, r, auc, Ks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU test\n",
    "\n",
    "Before we leave the metrics behind, let's pause for one second and have a look to the functions above. They all provide scores/metrics for one user. This means that this will have run in a loop or distributed over the cores of the machine where we run the algorithm. Given the fact that the algorithm will run on a GPU, maybe we could take advantage and write some evaluation function that runs on the GPU. \n",
    "\n",
    "The code below is taken mostly from [here](https://github.com/sh0416/bpr/blob/master/train.py), adapated to the fact that here our rating matrix is large enough so that it wont fit in memory when move to dense (i.e. we cannot run lines like: `test_pred_mask = 1 - (train_w)` in that code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "n_users = 100\n",
    "n_items = 200\n",
    "n_embed = 12\n",
    "Ks=[5,10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's create some small, fake dataset to illustrate the use of this testing method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user and item embeddings\n",
    "user_emb = torch.from_numpy(np.random.rand(n_users, n_embed))\n",
    "item_emb = torch.from_numpy(np.random.rand(n_items, n_embed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Ratings Matrix\n",
    "def randbin(r,c,p):\n",
    "    return np.random.choice([0, 1], size=(r,c), p=[p, 1-p])\n",
    "R_tr = randbin(n_users, n_items, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Rating Matrix\n",
    "# removing all items in training\n",
    "temp_mtx = 1 - R_tr\n",
    "# finding the corresponding indexes\n",
    "temp_idx = np.where(temp_mtx)\n",
    "# setting the testing size as, for example, training//5\n",
    "test_fr = np.where(R_tr)[0].size//5\n",
    "# chosing indexes at random\n",
    "R_te_idx = np.random.choice(temp_idx[0].size, test_fr, replace=False)\n",
    "i,j = temp_idx[0][R_te_idx], temp_idx[1][R_te_idx]\n",
    "# setting them to 1\n",
    "R_te = np.zeros((n_users, n_items))\n",
    "R_te[i,j] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run the \"real thing\" `R_tr` and `R_te` will be sparse matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_tr = sp.csr_matrix(R_tr, dtype='float64')\n",
    "R_te = sp.csr_matrix(R_te, dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_mtx(X, n_folds=10):\n",
    "    \"\"\"\n",
    "    Split a matrix/Tensor into n_folds    \n",
    "    \"\"\"\n",
    "    X_folds = []\n",
    "    fold_len = X.shape[0]//n_folds\n",
    "    for i in range(n_folds):\n",
    "        start = i * fold_len\n",
    "        if i == n_folds -1:\n",
    "            end = X.shape[0]\n",
    "        else:\n",
    "            end = (i + 1) * fold_len\n",
    "        X_folds.append(X[start:end])\n",
    "    return X_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is the testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_and_recall_k(user_emb, item_emb, R_tr, R_te, Ks):\n",
    "    \"\"\"\n",
    "    Compute precision and recall using tensors    \n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    user_emb: Tensor\n",
    "        user embeddings of shape (n_users, n_emb)\n",
    "    item_emb: Tensor\n",
    "        item embeddings of shape (n_items, n_emb)\n",
    "    R_tr: scipy.sp matrix\n",
    "        training ratings shape (n_users, n_items)\n",
    "    R_te: scipy.sp matrix\n",
    "        testing ratings shape (n_users, n_items)\n",
    "    Ks: List\n",
    "        k order of recommendations (the k in precision@k)\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    precision[k],recall[k]: Dict\n",
    "        Dictionary where keys are the Ks and values are precision and recall\n",
    "    \"\"\"\n",
    "    # splits into n_folds\n",
    "    tr_folds = split_mtx(R_tr)\n",
    "    te_folds = split_mtx(R_te)\n",
    "    ue_folds = split_mtx(user_emb)\n",
    "\n",
    "    fold_prec, fold_rec = {}, {}\n",
    "    for ue_fold, tr_fold, te_fold in zip(ue_folds, tr_folds, te_folds):\n",
    "        \n",
    "        # score for all items, per user.\n",
    "        result = torch.sigmoid(torch.mm(ue_fold, item_emb.t()))\n",
    "        # this mask contains that is not training (negatives+testing)\n",
    "        test_pred_mask = torch.from_numpy(1 - tr_fold.todense())\n",
    "        # this mask contains only the true testing items\n",
    "        test_true_mask = torch.from_numpy(te_fold.todense())\n",
    "        if use_cuda:\n",
    "            test_pred_mask, test_true_mask = test_pred_mask.cuda(), test_true_mask.cuda()\n",
    "        test_pred = test_pred_mask * result\n",
    "        test_true = test_true_mask * result\n",
    "\n",
    "        _, test_indices = torch.topk(test_pred, dim=1, k=max(Ks))\n",
    "        for k in Ks:\n",
    "            topk_mask = torch.zeros_like(test_pred)\n",
    "            source = torch.tensor(1.0).cuda() if use_cuda else torch.tensor(1.0)\n",
    "            # this will create a mask with 1 located in positions test_indices[:, :k]\n",
    "            topk_mask.scatter_(dim=1, index=test_indices[:, :k], src=source)\n",
    "            # matrix with the actual predictions in positions test_indices[:, :k]\n",
    "            test_pred_topk = topk_mask * test_pred\n",
    "            # precision and recall\n",
    "            acc_result = (test_pred_topk != 0) & (test_pred_topk == test_true)\n",
    "            pr_k = acc_result.sum().float() / (user_emb.shape[0] * k)\n",
    "            rec_k = (acc_result.float().sum(dim=1) / test_true_mask.float().sum(dim=1))\n",
    "            try:\n",
    "                fold_prec[k].append(pr_k)\n",
    "                fold_rec[k].append(rec_k)\n",
    "            except KeyError:\n",
    "                fold_prec[k] = [pr_k]\n",
    "                fold_rec[k] = [rec_k]\n",
    "\n",
    "    precision, recall = {}, {}\n",
    "    for k in Ks:\n",
    "        precision[k] = np.sum(fold_prec[k])\n",
    "        recall[k] = torch.cat(fold_rec[k]).mean()\n",
    "    return precision,recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look to what happens inside that function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_folds = split_mtx(R_tr)\n",
    "te_folds = split_mtx(R_te)\n",
    "ue_folds = split_mtx(user_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10x200 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 395 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_folds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 torch.Size([10, 12])\n"
     ]
    }
   ],
   "source": [
    "print(len(ue_folds), ue_folds[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we have 10 folds/partition of the rating and user embedding matrices and we are ready to loop. Let's got through one loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_fold, te_fold, ue_fold = tr_folds[0], te_folds[0], ue_folds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the authors explain that they want to make all scores between 0 and 1, using a sigmoid. Below are the score for all items, for the N users in the corresponding fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 200])\n"
     ]
    }
   ],
   "source": [
    "result = torch.sigmoid(torch.mm(ue_fold, item_emb.t()))\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masks with 1 for all items that are NOT in training -> test+negatives\n",
    "test_pred_mask = torch.from_numpy(1 - tr_fold.todense())\n",
    "# masks with 1 for test items (is a copy of R_te per fold)\n",
    "test_true_mask = torch.from_numpy(te_fold.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix with scores for all items that are NOT in training -> test+negatives\n",
    "test_pred = test_pred_mask * result\n",
    "# matrix with scores for \"true\" test items \n",
    "test_true = test_true_mask * result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 200]) torch.Size([10, 200])\n"
     ]
    }
   ],
   "source": [
    "print(test_pred.shape, test_true.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the locations of the top K recommended items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, test_indices = torch.topk(test_pred, dim=1, k=max(Ks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 96, 185, 120,  22,  90, 159, 166, 175,  50, 115],\n",
       "        [ 70, 111,  86,   8, 147, 184, 119,  93, 144, 152],\n",
       "        [185,  70, 111,  96, 119,  95,   8, 120, 175, 168],\n",
       "        [119, 111,  86,  37, 185, 166, 175, 168, 150,  95],\n",
       "        [ 86, 119,  37,  70,  96,  95, 185, 115,  90,  35],\n",
       "        [111,  70, 119, 166,  96, 168, 130, 175, 159,  86],\n",
       "        [119,  86,  70,  96, 185,  37,  95, 150,  62,   3],\n",
       "        [119,  86,  70,  96, 111,  37, 185,   8, 168,  12],\n",
       "        [119,  70,  86,  95, 111,  37, 154, 150, 185,  12],\n",
       "        [119,  96,  86, 185, 111, 148, 154, 120,  37, 168]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's assume k=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=5\n",
    "topk_mask = torch.zeros_like(test_pred)\n",
    "source = torch.tensor(1.0).cuda() if use_cuda else torch.tensor(1.0)\n",
    "topk_mask.scatter_(dim=1, index=test_indices[:, :k], src=source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 22],\n",
       "        [ 90],\n",
       "        [ 96],\n",
       "        [120],\n",
       "        [185]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk_mask[0,].nonzero()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "effectively, the nonzero locations (75, 139, ...) in the first row correspond to the top 5 items in test_indices. Let's get these locations from the `test_pred` tensor and compute the precision and recall (or almost)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_topk = topk_mask * test_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if item in not in training and is in testing (i.e. not negative)\n",
    "acc_result = (test_pred_topk != 0) & (test_pred_topk == test_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the neccesary information per fold that will be used to calculate precision and recall for the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_k = acc_result.sum().float() / (user_emb.shape[0] * k)\n",
    "rec_k = (acc_result.float().sum(dim=1) / test_true_mask.float().sum(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0040)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0909, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1429,\n",
       "        0.0000])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_prec, fold_rec = {}, {}\n",
    "try:\n",
    "    fold_prec[k].append(pr_k)\n",
    "    fold_rec[k].append(rec_k)\n",
    "except KeyError:\n",
    "    fold_prec[k] = [pr_k]\n",
    "    fold_rec[k] = [rec_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{5: [tensor(0.0040)]}\n",
      "{5: [tensor([0.0000, 0.0000, 0.0909, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1429,\n",
      "        0.0000])]}\n"
     ]
    }
   ],
   "source": [
    "print(fold_prec)\n",
    "print(fold_rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally (remember, this would run into a loop of Ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall = {}, {}\n",
    "precision[k] = np.sum(fold_prec[k])\n",
    "recall[k] = torch.cat(fold_rec[k]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{5: 0.004}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{5: tensor(0.0234)}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the whole thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall = precision_and_recall_k(user_emb, item_emb, R_tr, R_te, Ks=[5, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{5: 0.038, 10: 0.043999996} {5: tensor(0.0233), 10: tensor(0.0492)}\n"
     ]
    }
   ],
   "source": [
    "print(precision, recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the final version of the code, I will rename `precision_and_recall_k` to `test_GPU` as \"opposed\" to the [Wang Xiang et al](https://arxiv.org/pdf/1905.08108.pdf) paper test funcion, which I will refer as `test_CPU`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
