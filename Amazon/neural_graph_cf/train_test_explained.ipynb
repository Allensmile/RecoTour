{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing\n",
    "\n",
    "In this notebook I will focus on how the model is trained and tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import pdb\n",
    "import heapq\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from utility.load_data import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually set parameters. These are simply the defaults in their `parse_args` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path=''\n",
    "data_path='Data/'\n",
    "proj_path=''\n",
    "dataset='gowalla'\n",
    "pretrain=0\n",
    "verbose=1\n",
    "epoch=500\n",
    "embed_size=64\n",
    "layer_size=[64]\n",
    "batch_size=1024\n",
    "regs=[1e-5,1e-5,1e-2]\n",
    "lr=0.01\n",
    "model_type='ngcf'\n",
    "adj_type='norm'\n",
    "alg_type='ngcf'\n",
    "gpu_id=0\n",
    "node_dropout_flag=0\n",
    "node_dropout=[0.1]\n",
    "mess_dropout=[0.1]\n",
    "Ks=[20,40,60,80,100]\n",
    "save_flag=0\n",
    "test_flag='part'\n",
    "report=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already know the model (explained in the notebook `ngcf_model_explained.ipynb`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGCF(object):\n",
    "    def __init__(self, data_config, pretrain_data):\n",
    "        self.model_type = 'ngcf'\n",
    "        self.adj_type = adj_type\n",
    "        self.alg_type = alg_type\n",
    "\n",
    "        self.pretrain_data = pretrain_data\n",
    "\n",
    "        self.n_users = data_config['n_users']\n",
    "        self.n_items = data_config['n_items']\n",
    "\n",
    "        self.n_fold = 100\n",
    "\n",
    "        self.norm_adj = data_config['norm_adj']\n",
    "        self.n_nonzero_elems = self.norm_adj.count_nonzero()\n",
    "\n",
    "        self.lr = lr\n",
    "\n",
    "        self.emb_dim = embed_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.weight_size = layer_size\n",
    "        self.n_layers = len(self.weight_size)\n",
    "\n",
    "        self.model_type += '_%s_%s_l%d' % (self.adj_type, self.alg_type, self.n_layers)\n",
    "\n",
    "        self.regs = regs\n",
    "        self.decay = self.regs[0]\n",
    "\n",
    "        self.verbose = verbose\n",
    "\n",
    "        '''\n",
    "        *********************************************************\n",
    "        Create Placeholder for Input Data & Dropout.\n",
    "        '''\n",
    "        # placeholder definition\n",
    "        self.users = tf.placeholder(tf.int32, shape=(None,))\n",
    "        self.pos_items = tf.placeholder(tf.int32, shape=(None,))\n",
    "        self.neg_items = tf.placeholder(tf.int32, shape=(None,))\n",
    "\n",
    "        # dropout: node dropout (adopted on the ego-networks);\n",
    "        #          ... since the usage of node dropout have higher computational cost,\n",
    "        #          ... please use the 'node_dropout_flag' to indicate whether use such technique.\n",
    "        #          message dropout (adopted on the convolution operations).\n",
    "        self.node_dropout_flag = node_dropout_flag\n",
    "        self.node_dropout = tf.placeholder(tf.float32, shape=[None])\n",
    "        self.mess_dropout = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "        \"\"\"\n",
    "        *********************************************************\n",
    "        Create Model Parameters (i.e., Initialize Weights).\n",
    "        \"\"\"\n",
    "        # initialization of model parameters\n",
    "        self.weights = self._init_weights()\n",
    "\n",
    "        \"\"\"\n",
    "        *********************************************************\n",
    "        Compute Graph-based Representations of all users & items via Message-Passing Mechanism of Graph Neural Networks.\n",
    "        Different Convolutional Layers:\n",
    "            1. ngcf: defined in 'Neural Graph Collaborative Filtering', SIGIR2019;\n",
    "            2. gcn:  defined in 'Semi-Supervised Classification with Graph Convolutional Networks', ICLR2018;\n",
    "            3. gcmc: defined in 'Graph Convolutional Matrix Completion', KDD2018;\n",
    "        \"\"\"\n",
    "        if self.alg_type in ['ngcf']:\n",
    "            self.ua_embeddings, self.ia_embeddings = self._create_ngcf_embed()\n",
    "\n",
    "        elif self.alg_type in ['gcn']:\n",
    "            self.ua_embeddings, self.ia_embeddings = self._create_gcn_embed()\n",
    "\n",
    "        elif self.alg_type in ['gcmc']:\n",
    "            self.ua_embeddings, self.ia_embeddings = self._create_gcmc_embed()\n",
    "\n",
    "        \"\"\"\n",
    "        *********************************************************\n",
    "        Establish the final representations for user-item pairs in batch.\n",
    "        \"\"\"\n",
    "        self.u_g_embeddings = tf.nn.embedding_lookup(self.ua_embeddings, self.users)\n",
    "        self.pos_i_g_embeddings = tf.nn.embedding_lookup(self.ia_embeddings, self.pos_items)\n",
    "        self.neg_i_g_embeddings = tf.nn.embedding_lookup(self.ia_embeddings, self.neg_items)\n",
    "\n",
    "        \"\"\"\n",
    "        *********************************************************\n",
    "        Inference for the testing phase.\n",
    "        \"\"\"\n",
    "        self.batch_ratings = tf.matmul(self.u_g_embeddings, self.pos_i_g_embeddings, transpose_a=False, transpose_b=True)\n",
    "\n",
    "        \"\"\"\n",
    "        *********************************************************\n",
    "        Generate Predictions & Optimize via BPR loss.\n",
    "        \"\"\"\n",
    "        self.mf_loss, self.emb_loss, self.reg_loss = self.create_bpr_loss(self.u_g_embeddings,\n",
    "                                                                          self.pos_i_g_embeddings,\n",
    "                                                                          self.neg_i_g_embeddings)\n",
    "        self.loss = self.mf_loss + self.emb_loss + self.reg_loss\n",
    "\n",
    "        self.opt = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(self.loss)\n",
    "\n",
    "    def _init_weights(self):\n",
    "        all_weights = dict()\n",
    "\n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "        if self.pretrain_data is None:\n",
    "            all_weights['user_embedding'] = tf.Variable(initializer([self.n_users, self.emb_dim]), name='user_embedding')\n",
    "            all_weights['item_embedding'] = tf.Variable(initializer([self.n_items, self.emb_dim]), name='item_embedding')\n",
    "            print('using xavier initialization')\n",
    "        else:\n",
    "            all_weights['user_embedding'] = tf.Variable(initial_value=self.pretrain_data['user_embed'], trainable=True,\n",
    "                                                        name='user_embedding', dtype=tf.float32)\n",
    "            all_weights['item_embedding'] = tf.Variable(initial_value=self.pretrain_data['item_embed'], trainable=True,\n",
    "                                                        name='item_embedding', dtype=tf.float32)\n",
    "            print('using pretrained initialization')\n",
    "\n",
    "        self.weight_size_list = [self.emb_dim] + self.weight_size\n",
    "\n",
    "        for k in range(self.n_layers):\n",
    "            all_weights['W_gc_%d' %k] = tf.Variable(\n",
    "                initializer([self.weight_size_list[k], self.weight_size_list[k+1]]), name='W_gc_%d' % k)\n",
    "            all_weights['b_gc_%d' %k] = tf.Variable(\n",
    "                initializer([1, self.weight_size_list[k+1]]), name='b_gc_%d' % k)\n",
    "\n",
    "            all_weights['W_bi_%d' % k] = tf.Variable(\n",
    "                initializer([self.weight_size_list[k], self.weight_size_list[k + 1]]), name='W_bi_%d' % k)\n",
    "            all_weights['b_bi_%d' % k] = tf.Variable(\n",
    "                initializer([1, self.weight_size_list[k + 1]]), name='b_bi_%d' % k)\n",
    "\n",
    "            all_weights['W_mlp_%d' % k] = tf.Variable(\n",
    "                initializer([self.weight_size_list[k], self.weight_size_list[k+1]]), name='W_mlp_%d' % k)\n",
    "            all_weights['b_mlp_%d' % k] = tf.Variable(\n",
    "                initializer([1, self.weight_size_list[k+1]]), name='b_mlp_%d' % k)\n",
    "\n",
    "        return all_weights\n",
    "\n",
    "    def _split_A_hat(self, X):\n",
    "        A_fold_hat = []\n",
    "\n",
    "        fold_len = (self.n_users + self.n_items) // self.n_fold\n",
    "        for i_fold in range(self.n_fold):\n",
    "            start = i_fold * fold_len\n",
    "            if i_fold == self.n_fold -1:\n",
    "                end = self.n_users + self.n_items\n",
    "            else:\n",
    "                end = (i_fold + 1) * fold_len\n",
    "\n",
    "            A_fold_hat.append(self._convert_sp_mat_to_sp_tensor(X[start:end]))\n",
    "        return A_fold_hat\n",
    "\n",
    "    def _split_A_hat_node_dropout(self, X):\n",
    "        A_fold_hat = []\n",
    "\n",
    "        fold_len = (self.n_users + self.n_items) // self.n_fold\n",
    "        for i_fold in range(self.n_fold):\n",
    "            start = i_fold * fold_len\n",
    "            if i_fold == self.n_fold -1:\n",
    "                end = self.n_users + self.n_items\n",
    "            else:\n",
    "                end = (i_fold + 1) * fold_len\n",
    "\n",
    "            # A_fold_hat.append(self._convert_sp_mat_to_sp_tensor(X[start:end]))\n",
    "            temp = self._convert_sp_mat_to_sp_tensor(X[start:end])\n",
    "            n_nonzero_temp = X[start:end].count_nonzero()\n",
    "            A_fold_hat.append(self._dropout_sparse(temp, 1 - self.node_dropout[0], n_nonzero_temp))\n",
    "\n",
    "        return A_fold_hat\n",
    "\n",
    "    def _create_ngcf_embed(self):\n",
    "        # Generate a set of adjacency sub-matrix.\n",
    "        if self.node_dropout_flag:\n",
    "            # node dropout.\n",
    "            A_fold_hat = self._split_A_hat_node_dropout(self.norm_adj)\n",
    "        else:\n",
    "            A_fold_hat = self._split_A_hat(self.norm_adj)\n",
    "\n",
    "        ego_embeddings = tf.concat([self.weights['user_embedding'], self.weights['item_embedding']], axis=0)\n",
    "\n",
    "        all_embeddings = [ego_embeddings]\n",
    "\n",
    "        for k in range(0, self.n_layers):\n",
    "\n",
    "            temp_embed = []\n",
    "            for f in range(self.n_fold):\n",
    "                temp_embed.append(tf.sparse_tensor_dense_matmul(A_fold_hat[f], ego_embeddings))\n",
    "\n",
    "            # sum messages of neighbors.\n",
    "            side_embeddings = tf.concat(temp_embed, 0)\n",
    "            # transformed sum messages of neighbors.\n",
    "            sum_embeddings = tf.nn.leaky_relu(\n",
    "                tf.matmul(side_embeddings, self.weights['W_gc_%d' % k]) + self.weights['b_gc_%d' % k])\n",
    "\n",
    "            # bi messages of neighbors.\n",
    "            bi_embeddings = tf.multiply(ego_embeddings, side_embeddings)\n",
    "            # transformed bi messages of neighbors.\n",
    "            bi_embeddings = tf.nn.leaky_relu(\n",
    "                tf.matmul(bi_embeddings, self.weights['W_bi_%d' % k]) + self.weights['b_bi_%d' % k])\n",
    "\n",
    "            # non-linear activation.\n",
    "            ego_embeddings = sum_embeddings + bi_embeddings\n",
    "\n",
    "            # message dropout.\n",
    "            ego_embeddings = tf.nn.dropout(ego_embeddings, 1 - self.mess_dropout[k])\n",
    "\n",
    "            # normalize the distribution of embeddings.\n",
    "            norm_embeddings = tf.math.l2_normalize(ego_embeddings, axis=1)\n",
    "\n",
    "            all_embeddings += [norm_embeddings]\n",
    "\n",
    "        all_embeddings = tf.concat(all_embeddings, 1)\n",
    "        u_g_embeddings, i_g_embeddings = tf.split(all_embeddings, [self.n_users, self.n_items], 0)\n",
    "        return u_g_embeddings, i_g_embeddings\n",
    "\n",
    "    def create_bpr_loss(self, users, pos_items, neg_items):\n",
    "        pos_scores = tf.reduce_sum(tf.multiply(users, pos_items), axis=1)\n",
    "        neg_scores = tf.reduce_sum(tf.multiply(users, neg_items), axis=1)\n",
    "\n",
    "        regularizer = tf.nn.l2_loss(users) + tf.nn.l2_loss(pos_items) + tf.nn.l2_loss(neg_items)\n",
    "        regularizer = regularizer/self.batch_size\n",
    "\n",
    "        maxi = tf.log(tf.nn.sigmoid(pos_scores - neg_scores))\n",
    "        mf_loss = tf.negative(tf.reduce_mean(maxi))\n",
    "\n",
    "        emb_loss = self.decay * regularizer\n",
    "\n",
    "        reg_loss = tf.constant(0.0, tf.float32, [1])\n",
    "\n",
    "        return mf_loss, emb_loss, reg_loss\n",
    "\n",
    "    def _convert_sp_mat_to_sp_tensor(self, X):\n",
    "        coo = X.tocoo().astype(np.float32)\n",
    "        indices = np.mat([coo.row, coo.col]).transpose()\n",
    "        return tf.SparseTensor(indices, coo.data, coo.shape)\n",
    "\n",
    "    def _dropout_sparse(self, X, keep_prob, n_nonzero_elems):\n",
    "        noise_shape = [n_nonzero_elems]\n",
    "        random_tensor = keep_prob\n",
    "        random_tensor += tf.random_uniform(noise_shape)\n",
    "        dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "        pre_out = tf.sparse_retain(X, dropout_mask)\n",
    "\n",
    "        return pre_out * tf.div(1., keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will not be using pretrained embeddings, but just in case they include the following function, which simply loads the np arrays with the user and item embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_data():\n",
    "    pretrain_path = '%spretrain/%s/%s.npz' % (proj_path, dataset, 'embedding')\n",
    "    try:\n",
    "        pretrain_data = np.load(pretrain_path)\n",
    "        print('load the pretrained embeddings.')\n",
    "    except Exception:\n",
    "        pretrain_data = None\n",
    "    return pretrain_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_users=29858, n_items=40981\n",
      "n_interactions=1027370\n",
      "n_train=810128, n_test=217242, sparsity=0.00084\n"
     ]
    }
   ],
   "source": [
    "data_generator = Data(path=data_path + dataset, batch_size=batch_size)\n",
    "USR_NUM, ITEM_NUM = data_generator.n_users, data_generator.n_items\n",
    "N_TRAIN, N_TEST = data_generator.n_train, data_generator.n_test\n",
    "BATCH_SIZE = batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "\n",
    "config = dict()\n",
    "config['n_users'] = data_generator.n_users\n",
    "config['n_items'] = data_generator.n_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing or loading the already computed adjancecy matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already load adj matrix (70839, 70839) 0.20827150344848633\n"
     ]
    }
   ],
   "source": [
    "plain_adj, norm_adj, mean_adj = data_generator.get_adj_mat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['norm_adj'] = mean_adj + sp.eye(mean_adj.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the `help` param in the `pretrain` argument reads: \"help='0: No pretrain, -1: Pretrain with the learned embeddings, 1:Pretrain with stored models.\" \n",
    "\n",
    "I insist, in our case we will not be using any pretrained weights, but just in case, let's discuss it as part of the process. Here they simply state that if `pretrain == -1`, we will load learned embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pretrain == -1:\n",
    "    pretrain_data = load_pretrained_data()\n",
    "else:\n",
    "    pretrain_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using xavier initialization\n"
     ]
    }
   ],
   "source": [
    "model = NGCF(data_config=config, pretrain_data=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the tensorflow `Saver`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "if save_flag == 1:\n",
    "    layer = '-'.join([str(l) for l in layer_size])\n",
    "    weights_save_path = '%sweights/%s/%s/%s/l%s_r%s' % (weights_path, dataset, model.model_type, layer,\n",
    "                                                        str(lr), '-'.join([str(r) for r in regs]))\n",
    "    ensureDir(weights_save_path)\n",
    "    save_saver = tf.train.Saver(max_to_keep=1)\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we move into the pretrain==1 option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without pretraining.\n"
     ]
    }
   ],
   "source": [
    "if pretrain == 1:\n",
    "    layer = '-'.join([str(l) for l in layer_size])\n",
    "\n",
    "    pretrain_path = '%sweights/%s/%s/%s/l%s_r%s' % (weights_path, dataset, model.model_type, layer,\n",
    "                                                    str(lr), '-'.join([str(r) for r in regs]))\n",
    "\n",
    "\n",
    "    ckpt = tf.train.get_checkpoint_state(os.path.dirname(pretrain_path + '/checkpoint'))\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        print('load the pretrained model parameters from: ', pretrain_path)\n",
    "\n",
    "        # *********************************************************\n",
    "        # get the performance from pretrained model.\n",
    "        if report != 1:\n",
    "            users_to_test = list(data_generator.test_set.keys())\n",
    "            ret = test(sess, model, users_to_test, drop_flag=True)\n",
    "            cur_best_pre_0 = ret['recall'][0]\n",
    "\n",
    "            pretrain_ret = 'pretrained model recall=[%.5f, %.5f], precision=[%.5f, %.5f], hit=[%.5f, %.5f],' \\\n",
    "                           'ndcg=[%.5f, %.5f]' % \\\n",
    "                           (ret['recall'][0], ret['recall'][-1],\n",
    "                            ret['precision'][0], ret['precision'][-1],\n",
    "                            ret['hit_ratio'][0], ret['hit_ratio'][-1],\n",
    "                            ret['ndcg'][0], ret['ndcg'][-1])\n",
    "            print(pretrain_ret)\n",
    "    else:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        cur_best_pre_0 = 0.\n",
    "        print('without pretraining.')\n",
    "\n",
    "else:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    cur_best_pre_0 = 0.\n",
    "    print('without pretraining.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_loger, pre_loger, rec_loger, ndcg_loger, hit_loger = [], [], [], [], []\n",
    "stopping_step = 0\n",
    "should_stop = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just execute one run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [73.7s]: train==[182.73946=182.68858 + 0.00000]\n"
     ]
    }
   ],
   "source": [
    "t1 = time()\n",
    "loss, mf_loss, emb_loss, reg_loss = 0., 0., 0., 0.\n",
    "n_batch = data_generator.n_train // batch_size + 1\n",
    "\n",
    "for idx in range(n_batch):\n",
    "    #batch_size lists with user_ids, item_ids they interacted with and item_ids they did not interact with\n",
    "    users, pos_items, neg_items = data_generator.sample()\n",
    "    # you know, the tf sess.run fun...\n",
    "    _, batch_loss, batch_mf_loss, batch_emb_loss, batch_reg_loss = sess.run(\n",
    "        [model.opt, model.loss, model.mf_loss, model.emb_loss, model.reg_loss],\n",
    "        feed_dict={model.users: users, \n",
    "                   model.pos_items: pos_items,\n",
    "                   model.node_dropout: node_dropout,\n",
    "                   model.mess_dropout: mess_dropout,model.neg_items: neg_items})\n",
    "    loss += batch_loss\n",
    "    mf_loss += batch_mf_loss\n",
    "    emb_loss += batch_emb_loss\n",
    "    reg_loss += batch_reg_loss\n",
    "\n",
    "\n",
    "perf_str = 'Epoch %d [%.1fs]: train==[%.5f=%.5f + %.5f]' % (1, time() - t1, loss, mf_loss, reg_loss)\n",
    "print(perf_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST\n",
    "\n",
    "Let's now go through ALL the functions that they use for testing and some comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### recall\n",
    "\n",
    "Counts how many of the first N ranked items are present in the actual ground truth items, then divided by the total number of ground truth items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(rank, ground_truth, N):\n",
    "    return len(set(rank[:N]) & set(ground_truth)) / float(len(set(ground_truth)))\n",
    "\n",
    "def recall_at_k(r, k, all_pos_num):\n",
    "    r = np.asfarray(r)[:k]\n",
    "    return np.sum(r) / all_pos_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank,  ground_truth, N = np.random.choice(50, 10, replace=False), np.arange(10), 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4 40 26 30 34 24  6 33 46  9]\n",
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "print(rank)\n",
    "print(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall(rank, ground_truth, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(r, k):\n",
    "    \"\"\"Score is precision @ k\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    Returns:\n",
    "        Precision @ k\n",
    "    Raises:\n",
    "        ValueError: len(r) must be >= k\n",
    "    \"\"\"\n",
    "    assert k >= 1\n",
    "    r = np.asarray(r)[:k]\n",
    "    return np.mean(r)\n",
    "\n",
    "\n",
    "def average_precision(r,cut):\n",
    "    \"\"\"Score is average precision (area under PR curve)\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    Returns:\n",
    "        Average precision\n",
    "    \"\"\"\n",
    "    r = np.asarray(r)\n",
    "    out = [precision_at_k(r, k + 1) for k in range(cut) if r[k]]\n",
    "    if not out:\n",
    "        return 0.\n",
    "    return np.sum(out)/float(min(cut, np.sum(r)))\n",
    "\n",
    "# This is literally their code, and I'd say that the \"cut\" is missing\n",
    "def mean_average_precision(rs):\n",
    "    \"\"\"Score is mean average precision\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    Returns:\n",
    "        Mean average precision\n",
    "    \"\"\"\n",
    "    return np.mean([average_precision(r) for r in rs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "r =  np.random.choice([0, 1], size=(10), p=[2./3, 1./3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 0 1 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_at_k(r, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.644"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(average_precision(r, 10), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Discounted Cumulative Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_at_k(r, k, method=1):\n",
    "    \"\"\"Score is discounted cumulative gain (dcg)\n",
    "    Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "    Returns:\n",
    "        Discounted cumulative gain\n",
    "    \"\"\"\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        if method == 0:\n",
    "            return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\n",
    "        elif method == 1:\n",
    "            return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "        else:\n",
    "            raise ValueError('method must be 0 or 1.')\n",
    "    return 0.\n",
    "\n",
    "\n",
    "def ndcg_at_k(r, k, method=1):\n",
    "    \"\"\"Score is normalized discounted cumulative gain (ndcg)\n",
    "    Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "    Returns:\n",
    "        Normalized discounted cumulative gain\n",
    "    \"\"\"\n",
    "    dcg_max = dcg_at_k(sorted(r, reverse=True), k, method)\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k, method) / dcg_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "r =  np.random.choice([0, 1], size=(10), p=[2./3, 1./3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 0 0 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43067655807339306"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcg_at_k(r, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_at_k(r, k):\n",
    "    r = np.array(r)[:k]\n",
    "    if np.sum(r) > 0:\n",
    "        return 1.\n",
    "    else:\n",
    "        return 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "r =  np.random.choice([0, 1], size=(10), p=[2./3, 1./3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 1 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hit_at_k(r, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1(pre, rec):\n",
    "    if pre + rec > 0:\n",
    "        return (2.0 * pre * rec) / (pre + rec)\n",
    "    else:\n",
    "        return 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc(ground_truth, prediction):\n",
    "    try:\n",
    "        res = roc_auc_score(y_true=ground_truth, y_score=prediction)\n",
    "    except Exception:\n",
    "        res = 0.\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ranklists\n",
    "\n",
    "Let's create random inputs for this functions to see how they work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_score = dict(zip(np.random.choice(100,100,replace=False), np.random.uniform(-3,3,size=100)))\n",
    "user_pos_test = np.random.choice(100,10,replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auc(item_score, user_pos_test):\n",
    "    item_score = sorted(item_score.items(), key=lambda kv: kv[1])\n",
    "    item_score.reverse()\n",
    "    item_sort = [x[0] for x in item_score]\n",
    "    posterior = [x[1] for x in item_score]\n",
    "\n",
    "    r = []\n",
    "    for i in item_sort:\n",
    "        if i in user_pos_test:\n",
    "            r.append(1)\n",
    "        else:\n",
    "            r.append(0)\n",
    "    auc = auc(ground_truth=r, prediction=posterior)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_score = sorted(item_score.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[80, 97, 72, 26, 65, 56, 22, 20, 21, 81, 75, 43, 66, 59, 10, 45, 13, 1, 30, 87]\n",
      "[2.9746857735193437, 2.9326028293313504, 2.8054599989033253, 2.7266906675816207, 2.72551539787077, 2.693244388364591, 2.515331035888453, 2.4195014304645, 2.3055150326796037, 2.2586438683891483, 2.187605285749754, 2.1663881347240306, 2.0752814001179978, 2.066231986401421, 1.844970580950406, 1.7957091835850498, 1.7953952429894757, 1.786816939930179, 1.7323788688710806, 1.6853299822045376]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_sort = [x[0] for x in item_score]\n",
    "posterior = [x[1] for x in item_score]\n",
    "print(item_sort[:20]), print(posterior[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now simply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = []\n",
    "for i in item_sort:\n",
    "    if i in user_pos_test:\n",
    "        r.append(1)\n",
    "    else:\n",
    "        r.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(r, posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now move to the ranking lists. `test_items` and `ratings` are the items used in the testing phase and their corresponding ratins. With them we build the `item_score`. Since we already created an `item_score` dict, let's use it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranklist_by_heapq(user_pos_test, test_items, rating, Ks):\n",
    "    item_score = {}\n",
    "    for i in test_items:\n",
    "        item_score[i] = rating[i]\n",
    "\n",
    "    K_max = max(Ks)\n",
    "    K_max_item_score = heapq.nlargest(K_max, item_score, key=item_score.get)\n",
    "\n",
    "    r = []\n",
    "    for i in K_max_item_score:\n",
    "        if i in user_pos_test:\n",
    "            r.append(1)\n",
    "        else:\n",
    "            r.append(0)\n",
    "    auc = 0.\n",
    "    return r, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's see how the function goes line by line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 40, 60, 80, 100]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_score = dict(zip(np.random.choice(100,100,replace=False), np.random.uniform(-3,3,size=100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_max_item_score = heapq.nlargest(K_max, item_score, key=item_score.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[81, 89, 30, 82, 1, 2, 94, 75, 76, 42, 48, 86, 77, 19, 31, 91, 11, 25, 3, 10, 55, 67, 56, 41, 85, 88, 57, 58, 93, 27, 34, 5, 20, 99, 29, 50, 79, 7, 24, 18, 53, 14, 59, 13, 83, 43, 51, 97, 70, 72, 15, 65, 96, 64, 52, 23, 47, 32, 73, 46, 37, 71, 66, 68, 78, 17, 80, 63, 98, 69, 62, 33, 36, 49, 28, 40, 61, 39, 92, 90, 45, 44, 16, 60, 8, 35, 21, 95, 22, 84, 54, 26, 4, 38, 74, 9, 0, 87, 12, 6]\n"
     ]
    }
   ],
   "source": [
    "print(K_max_item_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29 36 87 31 67  8 96 95 85 35]\n"
     ]
    }
   ],
   "source": [
    "print(user_pos_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = []\n",
    "for i in K_max_item_score:\n",
    "    if i in user_pos_test:\n",
    "        r.append(1)\n",
    "    else:\n",
    "        r.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the next function is exactly the same as `ranklist_by_heapq` only that it returns `r` and `auc` being auc\n",
    "\n",
    "`auc = get_auc(item_score, user_pos_test)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranklist_by_sorted(user_pos_test, test_items, rating, Ks):\n",
    "    item_score = {}\n",
    "    for i in test_items:\n",
    "        item_score[i] = rating[i]\n",
    "\n",
    "    K_max = max(Ks)\n",
    "    K_max_item_score = heapq.nlargest(K_max, item_score, key=item_score.get)\n",
    "\n",
    "    r = []\n",
    "    for i in K_max_item_score:\n",
    "        if i in user_pos_test:\n",
    "            r.append(1)\n",
    "        else:\n",
    "            r.append(0)\n",
    "    auc = get_auc(item_score, user_pos_test)\n",
    "    return r, auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test performance with early stopping\n",
    "\n",
    "The function below is simply an \"aggregation of all previous functions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_stopping(log_value, best_value, stopping_step, expected_order='acc', flag_step=100):\n",
    "    # early stopping strategy:\n",
    "    assert expected_order in ['acc', 'dec']\n",
    "\n",
    "    if (expected_order == 'acc' and log_value >= best_value) or (expected_order == 'dec' and log_value <= best_value):\n",
    "        stopping_step = 0\n",
    "        best_value = log_value\n",
    "    else:\n",
    "        stopping_step += 1\n",
    "\n",
    "    if stopping_step >= flag_step:\n",
    "        print(\"Early stopping is trigger at step: {} log:{}\".format(flag_step, log_value))\n",
    "        should_stop = True\n",
    "    else:\n",
    "        should_stop = False\n",
    "    return best_value, stopping_step, should_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance(user_pos_test, r, auc, Ks):\n",
    "    precision, recall, ndcg, hit_ratio = [], [], [], []\n",
    "\n",
    "    for K in Ks:\n",
    "        precision.append(precision_at_k(r, K))\n",
    "        recall.append(recall_at_k(r, K, len(user_pos_test)))\n",
    "        ndcg.append(ndcg_at_k(r, K))\n",
    "        hit_ratio.append(hit_at_k(r, K))\n",
    "\n",
    "    return {'recall': np.array(recall), 'precision': np.array(precision),\n",
    "            'ndcg': np.array(ndcg), 'hit_ratio': np.array(hit_ratio), 'auc': auc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_user(x):\n",
    "    # user u's ratings for user u\n",
    "    rating = x[0]\n",
    "    #uid\n",
    "    u = x[1]\n",
    "    #user u's items in the training set\n",
    "    try:\n",
    "        training_items = data_generator.train_items[u]\n",
    "    except Exception:\n",
    "        training_items = []\n",
    "    #user u's items in the test set\n",
    "    user_pos_test = data_generator.test_set[u]\n",
    "\n",
    "    all_items = set(range(ITEM_NUM))\n",
    "\n",
    "    test_items = list(all_items - set(training_items))\n",
    "\n",
    "    if test_flag == 'part':\n",
    "        r, auc = ranklist_by_heapq(user_pos_test, test_items, rating, Ks)\n",
    "    else:\n",
    "        r, auc = ranklist_by_sorted(user_pos_test, test_items, rating, Ks)\n",
    "\n",
    "    return get_performance(user_pos_test, r, auc, Ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(sess, model, users_to_test):\n",
    "    result = {'precision': np.zeros(len(Ks)), 'recall': np.zeros(len(Ks)), 'ndcg': np.zeros(len(Ks)),\n",
    "              'hit_ratio': np.zeros(len(Ks)), 'auc': 0.}\n",
    "\n",
    "    u_batch_size = BATCH_SIZE * 2\n",
    "    i_batch_size = BATCH_SIZE\n",
    "\n",
    "    test_users = users_to_test\n",
    "    n_test_users = len(test_users)\n",
    "    n_user_batchs = n_test_users // u_batch_size + 1\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for u_batch_id in tqdm(range(n_user_batchs)):\n",
    "        start = u_batch_id * u_batch_size\n",
    "        end = (u_batch_id + 1) * u_batch_size\n",
    "\n",
    "        user_batch = test_users[start: end]\n",
    "        item_batch = range(ITEM_NUM)\n",
    "        rate_batch = sess.run(model.batch_ratings, {model.users: user_batch,\n",
    "                                                    model.pos_items: item_batch,\n",
    "                                                    model.node_dropout: [0.] * len(layer_size),\n",
    "                                                    model.mess_dropout: [0.] * len(layer_size)})\n",
    "\n",
    "        user_batch_rating_uid = zip(rate_batch, user_batch)\n",
    "        batch_result = [test_one_user(x) for x in user_batch_rating_uid]\n",
    "        count += len(batch_result)\n",
    "\n",
    "        for re in batch_result:\n",
    "            result['precision'] += re['precision']/n_test_users\n",
    "            result['recall'] += re['recall']/n_test_users\n",
    "            result['ndcg'] += re['ndcg']/n_test_users\n",
    "            result['hit_ratio'] += re['hit_ratio']/n_test_users\n",
    "            result['auc'] += re['auc']/n_test_users\n",
    "\n",
    "\n",
    "    assert count == n_test_users\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_to_test = list(data_generator.test_set.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [08:12<00:00, 29.32s/it]\n"
     ]
    }
   ],
   "source": [
    "ret = test(sess, model, users_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': array([0.02360841, 0.01705154, 0.0140917 , 0.01222411, 0.01090194]),\n",
       " 'recall': array([0.07644048, 0.10947525, 0.13427972, 0.15442168, 0.17067501]),\n",
       " 'ndcg': array([0.13606039, 0.16202653, 0.17947334, 0.19256729, 0.20293639]),\n",
       " 'hit_ratio': array([0.3362248 , 0.42889678, 0.48894769, 0.53225266, 0.56427088]),\n",
       " 'auc': 0.0}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_best_pre_0, stopping_step, should_stop = early_stopping(ret['recall'][0], cur_best_pre_0,\n",
    "                                                            stopping_step, expected_order='acc', flag_step=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_ngcf)",
   "language": "python",
   "name": "conda_ngcf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
