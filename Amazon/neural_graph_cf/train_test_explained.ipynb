{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing\n",
    "\n",
    "In this notebook I will focus on how the model is trained and tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import pdb\n",
    "import heapq\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from utility.load_data import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually set parameters. These are simply the defaults in their `parse_args` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path=''\n",
    "data_path='Data/'\n",
    "proj_path=''\n",
    "dataset='gowalla'\n",
    "pretrain=0\n",
    "verbose=1\n",
    "epoch=500\n",
    "embed_size=64\n",
    "layer_size=[64]\n",
    "batch_size=1024\n",
    "regs=[1e-5,1e-5,1e-2]\n",
    "lr=0.01\n",
    "model_type='ngcf'\n",
    "adj_type='norm'\n",
    "alg_type='ngcf'\n",
    "gpu_id=0\n",
    "node_dropout_flag=0\n",
    "node_dropout=[0.1]\n",
    "mess_dropout=[0.1]\n",
    "Ks=[20,40,60,80,100]\n",
    "save_flag=0\n",
    "test_flag='part'\n",
    "report=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already know the model (explained in the notebook `ngcf_model_explained.ipynb`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGCF(object):\n",
    "    def __init__(self, data_config, pretrain_data):\n",
    "        self.model_type = 'ngcf'\n",
    "        self.adj_type = adj_type\n",
    "        self.alg_type = alg_type\n",
    "\n",
    "        self.pretrain_data = pretrain_data\n",
    "\n",
    "        self.n_users = data_config['n_users']\n",
    "        self.n_items = data_config['n_items']\n",
    "\n",
    "        self.n_fold = 100\n",
    "\n",
    "        self.norm_adj = data_config['norm_adj']\n",
    "        self.n_nonzero_elems = self.norm_adj.count_nonzero()\n",
    "\n",
    "        self.lr = lr\n",
    "\n",
    "        self.emb_dim = embed_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.weight_size = layer_size\n",
    "        self.n_layers = len(self.weight_size)\n",
    "\n",
    "        self.model_type += '_%s_%s_l%d' % (self.adj_type, self.alg_type, self.n_layers)\n",
    "\n",
    "        self.regs = regs\n",
    "        self.decay = self.regs[0]\n",
    "\n",
    "        self.verbose = verbose\n",
    "\n",
    "        '''\n",
    "        *********************************************************\n",
    "        Create Placeholder for Input Data & Dropout.\n",
    "        '''\n",
    "        # placeholder definition\n",
    "        self.users = tf.placeholder(tf.int32, shape=(None,))\n",
    "        self.pos_items = tf.placeholder(tf.int32, shape=(None,))\n",
    "        self.neg_items = tf.placeholder(tf.int32, shape=(None,))\n",
    "\n",
    "        # dropout: node dropout (adopted on the ego-networks);\n",
    "        #          ... since the usage of node dropout have higher computational cost,\n",
    "        #          ... please use the 'node_dropout_flag' to indicate whether use such technique.\n",
    "        #          message dropout (adopted on the convolution operations).\n",
    "        self.node_dropout_flag = node_dropout_flag\n",
    "        self.node_dropout = tf.placeholder(tf.float32, shape=[None])\n",
    "        self.mess_dropout = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "        \"\"\"\n",
    "        *********************************************************\n",
    "        Create Model Parameters (i.e., Initialize Weights).\n",
    "        \"\"\"\n",
    "        # initialization of model parameters\n",
    "        self.weights = self._init_weights()\n",
    "\n",
    "        \"\"\"\n",
    "        *********************************************************\n",
    "        Compute Graph-based Representations of all users & items via Message-Passing Mechanism of Graph Neural Networks.\n",
    "        Different Convolutional Layers:\n",
    "            1. ngcf: defined in 'Neural Graph Collaborative Filtering', SIGIR2019;\n",
    "            2. gcn:  defined in 'Semi-Supervised Classification with Graph Convolutional Networks', ICLR2018;\n",
    "            3. gcmc: defined in 'Graph Convolutional Matrix Completion', KDD2018;\n",
    "        \"\"\"\n",
    "        if self.alg_type in ['ngcf']:\n",
    "            self.ua_embeddings, self.ia_embeddings = self._create_ngcf_embed()\n",
    "\n",
    "        elif self.alg_type in ['gcn']:\n",
    "            self.ua_embeddings, self.ia_embeddings = self._create_gcn_embed()\n",
    "\n",
    "        elif self.alg_type in ['gcmc']:\n",
    "            self.ua_embeddings, self.ia_embeddings = self._create_gcmc_embed()\n",
    "\n",
    "        \"\"\"\n",
    "        *********************************************************\n",
    "        Establish the final representations for user-item pairs in batch.\n",
    "        \"\"\"\n",
    "        self.u_g_embeddings = tf.nn.embedding_lookup(self.ua_embeddings, self.users)\n",
    "        self.pos_i_g_embeddings = tf.nn.embedding_lookup(self.ia_embeddings, self.pos_items)\n",
    "        self.neg_i_g_embeddings = tf.nn.embedding_lookup(self.ia_embeddings, self.neg_items)\n",
    "\n",
    "        \"\"\"\n",
    "        *********************************************************\n",
    "        Inference for the testing phase.\n",
    "        \"\"\"\n",
    "        self.batch_ratings = tf.matmul(self.u_g_embeddings, self.pos_i_g_embeddings, transpose_a=False, transpose_b=True)\n",
    "\n",
    "        \"\"\"\n",
    "        *********************************************************\n",
    "        Generate Predictions & Optimize via BPR loss.\n",
    "        \"\"\"\n",
    "        self.mf_loss, self.emb_loss, self.reg_loss = self.create_bpr_loss(self.u_g_embeddings,\n",
    "                                                                          self.pos_i_g_embeddings,\n",
    "                                                                          self.neg_i_g_embeddings)\n",
    "        self.loss = self.mf_loss + self.emb_loss + self.reg_loss\n",
    "\n",
    "        self.opt = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(self.loss)\n",
    "\n",
    "    def _init_weights(self):\n",
    "        all_weights = dict()\n",
    "\n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "        if self.pretrain_data is None:\n",
    "            all_weights['user_embedding'] = tf.Variable(initializer([self.n_users, self.emb_dim]), name='user_embedding')\n",
    "            all_weights['item_embedding'] = tf.Variable(initializer([self.n_items, self.emb_dim]), name='item_embedding')\n",
    "            print('using xavier initialization')\n",
    "        else:\n",
    "            all_weights['user_embedding'] = tf.Variable(initial_value=self.pretrain_data['user_embed'], trainable=True,\n",
    "                                                        name='user_embedding', dtype=tf.float32)\n",
    "            all_weights['item_embedding'] = tf.Variable(initial_value=self.pretrain_data['item_embed'], trainable=True,\n",
    "                                                        name='item_embedding', dtype=tf.float32)\n",
    "            print('using pretrained initialization')\n",
    "\n",
    "        self.weight_size_list = [self.emb_dim] + self.weight_size\n",
    "\n",
    "        for k in range(self.n_layers):\n",
    "            all_weights['W_gc_%d' %k] = tf.Variable(\n",
    "                initializer([self.weight_size_list[k], self.weight_size_list[k+1]]), name='W_gc_%d' % k)\n",
    "            all_weights['b_gc_%d' %k] = tf.Variable(\n",
    "                initializer([1, self.weight_size_list[k+1]]), name='b_gc_%d' % k)\n",
    "\n",
    "            all_weights['W_bi_%d' % k] = tf.Variable(\n",
    "                initializer([self.weight_size_list[k], self.weight_size_list[k + 1]]), name='W_bi_%d' % k)\n",
    "            all_weights['b_bi_%d' % k] = tf.Variable(\n",
    "                initializer([1, self.weight_size_list[k + 1]]), name='b_bi_%d' % k)\n",
    "\n",
    "            all_weights['W_mlp_%d' % k] = tf.Variable(\n",
    "                initializer([self.weight_size_list[k], self.weight_size_list[k+1]]), name='W_mlp_%d' % k)\n",
    "            all_weights['b_mlp_%d' % k] = tf.Variable(\n",
    "                initializer([1, self.weight_size_list[k+1]]), name='b_mlp_%d' % k)\n",
    "\n",
    "        return all_weights\n",
    "\n",
    "    def _split_A_hat(self, X):\n",
    "        A_fold_hat = []\n",
    "\n",
    "        fold_len = (self.n_users + self.n_items) // self.n_fold\n",
    "        for i_fold in range(self.n_fold):\n",
    "            start = i_fold * fold_len\n",
    "            if i_fold == self.n_fold -1:\n",
    "                end = self.n_users + self.n_items\n",
    "            else:\n",
    "                end = (i_fold + 1) * fold_len\n",
    "\n",
    "            A_fold_hat.append(self._convert_sp_mat_to_sp_tensor(X[start:end]))\n",
    "        return A_fold_hat\n",
    "\n",
    "    def _split_A_hat_node_dropout(self, X):\n",
    "        A_fold_hat = []\n",
    "\n",
    "        fold_len = (self.n_users + self.n_items) // self.n_fold\n",
    "        for i_fold in range(self.n_fold):\n",
    "            start = i_fold * fold_len\n",
    "            if i_fold == self.n_fold -1:\n",
    "                end = self.n_users + self.n_items\n",
    "            else:\n",
    "                end = (i_fold + 1) * fold_len\n",
    "\n",
    "            # A_fold_hat.append(self._convert_sp_mat_to_sp_tensor(X[start:end]))\n",
    "            temp = self._convert_sp_mat_to_sp_tensor(X[start:end])\n",
    "            n_nonzero_temp = X[start:end].count_nonzero()\n",
    "            A_fold_hat.append(self._dropout_sparse(temp, 1 - self.node_dropout[0], n_nonzero_temp))\n",
    "\n",
    "        return A_fold_hat\n",
    "\n",
    "    def _create_ngcf_embed(self):\n",
    "        # Generate a set of adjacency sub-matrix.\n",
    "        if self.node_dropout_flag:\n",
    "            # node dropout.\n",
    "            A_fold_hat = self._split_A_hat_node_dropout(self.norm_adj)\n",
    "        else:\n",
    "            A_fold_hat = self._split_A_hat(self.norm_adj)\n",
    "\n",
    "        ego_embeddings = tf.concat([self.weights['user_embedding'], self.weights['item_embedding']], axis=0)\n",
    "\n",
    "        all_embeddings = [ego_embeddings]\n",
    "\n",
    "        for k in range(0, self.n_layers):\n",
    "\n",
    "            temp_embed = []\n",
    "            for f in range(self.n_fold):\n",
    "                temp_embed.append(tf.sparse_tensor_dense_matmul(A_fold_hat[f], ego_embeddings))\n",
    "\n",
    "            # sum messages of neighbors.\n",
    "            side_embeddings = tf.concat(temp_embed, 0)\n",
    "            # transformed sum messages of neighbors.\n",
    "            sum_embeddings = tf.nn.leaky_relu(\n",
    "                tf.matmul(side_embeddings, self.weights['W_gc_%d' % k]) + self.weights['b_gc_%d' % k])\n",
    "\n",
    "            # bi messages of neighbors.\n",
    "            bi_embeddings = tf.multiply(ego_embeddings, side_embeddings)\n",
    "            # transformed bi messages of neighbors.\n",
    "            bi_embeddings = tf.nn.leaky_relu(\n",
    "                tf.matmul(bi_embeddings, self.weights['W_bi_%d' % k]) + self.weights['b_bi_%d' % k])\n",
    "\n",
    "            # non-linear activation.\n",
    "            ego_embeddings = sum_embeddings + bi_embeddings\n",
    "\n",
    "            # message dropout.\n",
    "            ego_embeddings = tf.nn.dropout(ego_embeddings, 1 - self.mess_dropout[k])\n",
    "\n",
    "            # normalize the distribution of embeddings.\n",
    "            norm_embeddings = tf.math.l2_normalize(ego_embeddings, axis=1)\n",
    "\n",
    "            all_embeddings += [norm_embeddings]\n",
    "\n",
    "        all_embeddings = tf.concat(all_embeddings, 1)\n",
    "        u_g_embeddings, i_g_embeddings = tf.split(all_embeddings, [self.n_users, self.n_items], 0)\n",
    "        return u_g_embeddings, i_g_embeddings\n",
    "\n",
    "    def create_bpr_loss(self, users, pos_items, neg_items):\n",
    "        pos_scores = tf.reduce_sum(tf.multiply(users, pos_items), axis=1)\n",
    "        neg_scores = tf.reduce_sum(tf.multiply(users, neg_items), axis=1)\n",
    "\n",
    "        regularizer = tf.nn.l2_loss(users) + tf.nn.l2_loss(pos_items) + tf.nn.l2_loss(neg_items)\n",
    "        regularizer = regularizer/self.batch_size\n",
    "\n",
    "        maxi = tf.log(tf.nn.sigmoid(pos_scores - neg_scores))\n",
    "        mf_loss = tf.negative(tf.reduce_mean(maxi))\n",
    "\n",
    "        emb_loss = self.decay * regularizer\n",
    "\n",
    "        reg_loss = tf.constant(0.0, tf.float32, [1])\n",
    "\n",
    "        return mf_loss, emb_loss, reg_loss\n",
    "\n",
    "    def _convert_sp_mat_to_sp_tensor(self, X):\n",
    "        coo = X.tocoo().astype(np.float32)\n",
    "        indices = np.mat([coo.row, coo.col]).transpose()\n",
    "        return tf.SparseTensor(indices, coo.data, coo.shape)\n",
    "\n",
    "    def _dropout_sparse(self, X, keep_prob, n_nonzero_elems):\n",
    "        noise_shape = [n_nonzero_elems]\n",
    "        random_tensor = keep_prob\n",
    "        random_tensor += tf.random_uniform(noise_shape)\n",
    "        dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "        pre_out = tf.sparse_retain(X, dropout_mask)\n",
    "\n",
    "        return pre_out * tf.div(1., keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will not be using pretrained embeddings. However, I will include here the corresponding code for completion, since it is important.\n",
    "\n",
    "The function below simply load pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_data():\n",
    "    pretrain_path = '%spretrain/%s/%s.npz' % (proj_path, dataset, 'embedding')\n",
    "    try:\n",
    "        pretrain_data = np.load(pretrain_path)\n",
    "        print('load the pretrained embeddings.')\n",
    "    except Exception:\n",
    "        pretrain_data = None\n",
    "    return pretrain_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_users=29858, n_items=40981\n",
      "n_interactions=1027370\n",
      "n_train=810128, n_test=217242, sparsity=0.00084\n"
     ]
    }
   ],
   "source": [
    "data_generator = Data(path=data_path + dataset, batch_size=batch_size)\n",
    "USR_NUM, ITEM_NUM = data_generator.n_users, data_generator.n_items\n",
    "N_TRAIN, N_TEST = data_generator.n_train, data_generator.n_test\n",
    "BATCH_SIZE = batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "\n",
    "config = dict()\n",
    "config['n_users'] = data_generator.n_users\n",
    "config['n_items'] = data_generator.n_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing or loading the already computed adjancecy matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already load adj matrix (70839, 70839) 0.2664508819580078\n"
     ]
    }
   ],
   "source": [
    "plain_adj, norm_adj, mean_adj = data_generator.get_adj_mat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['norm_adj'] = mean_adj + sp.eye(mean_adj.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the `help` param in the `pretrain` argument reads: \"help='0: No pretrain, -1: Pretrain with the learned embeddings, 1:Pretrain with stored models.\" \n",
    "\n",
    "I insist, in our case we will not be using any pretrained weights, but just in case, let's discuss it as part of the process. Here they simply state that if `pretrain == -1`, we will load learned embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pretrain == -1:\n",
    "    pretrain_data = load_pretrained_data()\n",
    "else:\n",
    "    pretrain_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0809 09:22:54.730366 140170820278016 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using xavier initialization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0809 09:22:55.528345 140170820278016 deprecation.py:506] From <ipython-input-4-be6f8a5a6d64>:200: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0809 09:22:55.766374 140170820278016 deprecation.py:323] From /home/ubuntu/anaconda3/envs/ngcf/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model = NGCF(data_config=config, pretrain_data=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the tensorflow `Saver`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "if save_flag == 1:\n",
    "    layer = '-'.join([str(l) for l in layer_size])\n",
    "    weights_save_path = '%sweights/%s/%s/%s/l%s_r%s' % (weights_path, dataset, model.model_type, layer,\n",
    "                                                        str(lr), '-'.join([str(r) for r in regs]))\n",
    "    ensureDir(weights_save_path)\n",
    "    save_saver = tf.train.Saver(max_to_keep=1)\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we move into the `pretrain==1` option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without pretraining.\n"
     ]
    }
   ],
   "source": [
    "if pretrain == 1:\n",
    "    layer = '-'.join([str(l) for l in layer_size])\n",
    "\n",
    "    pretrain_path = '%sweights/%s/%s/%s/l%s_r%s' % (weights_path, dataset, model.model_type, layer,\n",
    "                                                    str(lr), '-'.join([str(r) for r in regs]))\n",
    "\n",
    "\n",
    "    ckpt = tf.train.get_checkpoint_state(os.path.dirname(pretrain_path + '/checkpoint'))\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        # 1. Load pretrained weights\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        print('load the pretrained model parameters from: ', pretrain_path)\n",
    "\n",
    "        # *********************************************************\n",
    "        # get the performance from pretrained model.\n",
    "        if report != 1:\n",
    "            # 2. Use the pretained model and compute the performance metrics\n",
    "            users_to_test = list(data_generator.test_set.keys())\n",
    "            ret = test(sess, model, users_to_test, drop_flag=True)\n",
    "            cur_best_pre_0 = ret['recall'][0]\n",
    "\n",
    "            pretrain_ret = 'pretrained model recall=[%.5f, %.5f], precision=[%.5f, %.5f], hit=[%.5f, %.5f],' \\\n",
    "                           'ndcg=[%.5f, %.5f]' % \\\n",
    "                           (ret['recall'][0], ret['recall'][-1],\n",
    "                            ret['precision'][0], ret['precision'][-1],\n",
    "                            ret['hit_ratio'][0], ret['hit_ratio'][-1],\n",
    "                            ret['ndcg'][0], ret['ndcg'][-1])\n",
    "            print(pretrain_ret)\n",
    "    else:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        cur_best_pre_0 = 0.\n",
    "        print('without pretraining.')\n",
    "\n",
    "else:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    cur_best_pre_0 = 0.\n",
    "    print('without pretraining.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_loger, pre_loger, rec_loger, ndcg_loger, hit_loger = [], [], [], [], []\n",
    "stopping_step = 0\n",
    "should_stop = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN\n",
    "\n",
    "Let's just execute one run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [73.8s]: train==[179.00768=178.95593 + 0.00000]\n"
     ]
    }
   ],
   "source": [
    "t1 = time()\n",
    "loss, mf_loss, emb_loss, reg_loss = 0., 0., 0., 0.\n",
    "n_batch = data_generator.n_train // batch_size + 1\n",
    "\n",
    "for idx in range(n_batch):\n",
    "    #batch_size lists with user_ids, item_ids they interacted with and item_ids they did not interact with\n",
    "    users, pos_items, neg_items = data_generator.sample()\n",
    "    # you know, the tf sess.run fun...\n",
    "    _, batch_loss, batch_mf_loss, batch_emb_loss, batch_reg_loss = sess.run(\n",
    "        [model.opt, model.loss, model.mf_loss, model.emb_loss, model.reg_loss],\n",
    "        feed_dict={model.users: users, \n",
    "                   model.pos_items: pos_items,\n",
    "                   model.node_dropout: node_dropout,\n",
    "                   model.mess_dropout: mess_dropout,model.neg_items: neg_items})\n",
    "    loss += batch_loss\n",
    "    mf_loss += batch_mf_loss\n",
    "    emb_loss += batch_emb_loss\n",
    "    reg_loss += batch_reg_loss\n",
    "\n",
    "\n",
    "perf_str = 'Epoch %d [%.1fs]: train==[%.5f=%.5f + %.5f]' % (1, time() - t1, loss, mf_loss, reg_loss)\n",
    "print(perf_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST\n",
    "\n",
    "Now we have a trained model. Before we run the testing function, let's have a look to all the helpers that will be required.\n",
    "\n",
    "**Recall**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(rank, ground_truth, N):\n",
    "    return len(set(rank[:N]) & set(ground_truth)) / float(len(set(ground_truth)))\n",
    "\n",
    "def recall_at_k(r, k, all_pos_num):\n",
    "    r = np.asfarray(r)[:k]\n",
    "    return np.sum(r) / all_pos_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank,  ground_truth, N = np.random.choice(50, 10, replace=False), np.arange(10), 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23 35 31 33 39 26 28 49 38 24]\n",
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "print(rank)\n",
    "print(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall(rank, ground_truth, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision**\n",
    "\n",
    "throughout all these functions, r is binary (nonzero is relevant) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(r, k):\n",
    "    assert k >= 1\n",
    "    r = np.asarray(r)[:k]\n",
    "    return np.mean(r)\n",
    "\n",
    "\n",
    "def average_precision(r,cut):\n",
    "    r = np.asarray(r)\n",
    "    out = [precision_at_k(r, k + 1) for k in range(cut) if r[k]]\n",
    "    if not out:\n",
    "        return 0.\n",
    "    return np.sum(out)/float(min(cut, np.sum(r)))\n",
    "\n",
    "# This is literally their code, and I'd say that the \"cut\" is missing. \n",
    "# Fortunately they are not using this function later\n",
    "def mean_average_precision(rs):\n",
    "    return np.mean([average_precision(r) for r in rs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "r =  np.random.choice([0, 1], size=(10), p=[2./3, 1./3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_at_k(r, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.417"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(average_precision(r, 10), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalize Discounted Cumulative Gain**\n",
    "\n",
    "here r can be binary or real scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_at_k(r, k, method=1):\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        if method == 0:\n",
    "            return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\n",
    "        elif method == 1:\n",
    "            return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "        else:\n",
    "            raise ValueError('method must be 0 or 1.')\n",
    "    return 0.\n",
    "\n",
    "\n",
    "def ndcg_at_k(r, k, method=1):\n",
    "    dcg_max = dcg_at_k(sorted(r, reverse=True), k, method)\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k, method) / dcg_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "r =  np.random.choice([0, 1], size=(10), p=[2./3, 1./3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 1 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8175293653079347"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcg_at_k(r, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**hit ratio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_at_k(r, k):\n",
    "    r = np.array(r)[:k]\n",
    "    if np.sum(r) > 0:\n",
    "        return 1.\n",
    "    else:\n",
    "        return 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "r =  np.random.choice([0, 1], size=(10), p=[0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hit_at_k(r, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AUC**\n",
    "\n",
    "sklearn's `roc_auc_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc(ground_truth, prediction):\n",
    "    try:\n",
    "        res = roc_auc_score(y_true=ground_truth, y_score=prediction)\n",
    "    except Exception:\n",
    "        res = 0.\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ranklists**\n",
    "\n",
    "Let's create random inputs for this functions to see how they work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_score = dict(zip(np.random.choice(100,100,replace=False), np.random.uniform(-3,3,size=100)))\n",
    "user_pos_test = np.random.choice(200,10,replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auc(item_score, user_pos_test):\n",
    "    item_score = sorted(item_score.items(), key=lambda kv: kv[1])\n",
    "    item_score.reverse()\n",
    "    item_sort = [x[0] for x in item_score]\n",
    "    posterior = [x[1] for x in item_score]\n",
    "\n",
    "    r = []\n",
    "    for i in item_sort:\n",
    "        if i in user_pos_test:\n",
    "            r.append(1)\n",
    "        else:\n",
    "            r.append(0)\n",
    "    auc = auc(ground_truth=r, prediction=posterior)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_score = sorted(item_score.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33, 23, 4, 82, 43, 37, 81, 89, 88, 41, 78, 69, 61, 18, 28, 71, 6, 60, 2, 64]\n",
      "[2.9564403176243577, 2.9330196467457696, 2.867083011350485, 2.848406768391536, 2.833077727251103, 2.7326935137282167, 2.5872690717311677, 2.581285827625182, 2.3968584069271213, 2.363423159165553, 2.355520875361422, 2.2837836700121983, 2.2695655669557286, 2.2597432075553563, 2.163855500098677, 1.9845766756313736, 1.9785417879707943, 1.9085644552504348, 1.7402995931098575, 1.5021268043628462]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_sort = [x[0] for x in item_score]\n",
    "posterior = [x[1] for x in item_score]\n",
    "print(item_sort[:20]), print(posterior[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now simply, if the sorted items are among the user positive items, we append them to `r`, building a binary list where positive is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = []\n",
    "for i in item_sort:\n",
    "    if i in user_pos_test:\n",
    "        r.append(1)\n",
    "    else:\n",
    "        r.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The funcion `get_auc` will then return the result of `auc` which is in itself sklearn's `roc_auc_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6252631578947369"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(r, posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of `ranklist_by_heapq`, `test_items` and `ratings` are the items used in the testing phase and their corresponding ratins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranklist_by_heapq(user_pos_test, test_items, rating, Ks):\n",
    "    item_score = {}\n",
    "    for i in test_items:\n",
    "        item_score[i] = rating[i]\n",
    "\n",
    "    K_max = max(Ks)\n",
    "    K_max_item_score = heapq.nlargest(K_max, item_score, key=item_score.get)\n",
    "\n",
    "    r = []\n",
    "    for i in K_max_item_score:\n",
    "        if i in user_pos_test:\n",
    "            r.append(1)\n",
    "        else:\n",
    "            r.append(0)\n",
    "    auc = 0.\n",
    "    return r, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's see how the function goes line by line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 40, 60, 80, 100]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_max = max(Ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_score = dict(zip(np.random.choice(100,100,replace=False), np.random.uniform(-3,3,size=100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_max_item_score = heapq.nlargest(K_max, item_score, key=item_score.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60, 7, 89, 67, 6, 92, 55, 38, 31, 69, 65, 54, 40, 27, 85, 94, 58, 10, 99, 90, 91, 79, 26, 13, 88, 62, 95, 17, 76, 2, 3, 51, 96, 47, 52, 11, 57, 22, 75, 87, 15, 64, 12, 68, 86, 18, 33, 36, 74, 63, 42, 61, 59, 49, 39, 97, 53, 98, 28, 34, 8, 80, 45, 14, 72, 44, 56, 24, 21, 5, 46, 66, 83, 9, 81, 30, 4, 48, 43, 50, 37, 78, 32, 20, 73, 41, 84, 77, 16, 0, 19, 93, 29, 23, 1, 82, 25, 71, 35, 70]\n"
     ]
    }
   ],
   "source": [
    "print(K_max_item_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[127  78  60 149 181  38  66 140  25 121]\n"
     ]
    }
   ],
   "source": [
    "print(user_pos_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = []\n",
    "for i in K_max_item_score:\n",
    "    if i in user_pos_test:\n",
    "        r.append(1)\n",
    "    else:\n",
    "        r.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the next function is exactly the same as `ranklist_by_heapq` only that it returns `r` and `auc` being auc\n",
    "\n",
    "`auc = get_auc(item_score, user_pos_test)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranklist_by_sorted(user_pos_test, test_items, rating, Ks):\n",
    "    item_score = {}\n",
    "    for i in test_items:\n",
    "        item_score[i] = rating[i]\n",
    "\n",
    "    K_max = max(Ks)\n",
    "    K_max_item_score = heapq.nlargest(K_max, item_score, key=item_score.get)\n",
    "\n",
    "    r = []\n",
    "    for i in K_max_item_score:\n",
    "        if i in user_pos_test:\n",
    "            r.append(1)\n",
    "        else:\n",
    "            r.append(0)\n",
    "    auc = get_auc(item_score, user_pos_test)\n",
    "    return r, auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test performance with early stopping**\n",
    "\n",
    "Let's first have a look to the early stopping function. Is not really that complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_stopping(log_value, best_value, stopping_step, expected_order='acc', flag_step=100):\n",
    "    # early stopping strategy:\n",
    "    assert expected_order in ['acc', 'dec']\n",
    "\n",
    "    if (expected_order == 'acc' and log_value >= best_value) or (expected_order == 'dec' and log_value <= best_value):\n",
    "        stopping_step = 0\n",
    "        best_value = log_value\n",
    "    else:\n",
    "        stopping_step += 1\n",
    "\n",
    "    if stopping_step >= flag_step:\n",
    "        print(\"Early stopping is trigger at step: {} log:{}\".format(flag_step, log_value))\n",
    "        should_stop = True\n",
    "    else:\n",
    "        should_stop = False\n",
    "    return best_value, stopping_step, should_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's move again to the evaluation. The function below is simply an \"aggregation\" of all previous metrics/functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance(user_pos_test, r, auc, Ks):\n",
    "    precision, recall, ndcg, hit_ratio = [], [], [], []\n",
    "\n",
    "    for K in Ks:\n",
    "        precision.append(precision_at_k(r, K))\n",
    "        recall.append(recall_at_k(r, K, len(user_pos_test)))\n",
    "        ndcg.append(ndcg_at_k(r, K))\n",
    "        hit_ratio.append(hit_at_k(r, K))\n",
    "\n",
    "    return {'recall': np.array(recall), 'precision': np.array(precision),\n",
    "            'ndcg': np.array(ndcg), 'hit_ratio': np.array(hit_ratio), 'auc': auc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now is when the test happens. First we test a user and then we will loop through all users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_user(x):\n",
    "    # user u's ratings for user u\n",
    "    rating = x[0]\n",
    "    #uid\n",
    "    u = x[1]\n",
    "    #user u's items in the training set\n",
    "    try:\n",
    "        training_items = data_generator.train_items[u]\n",
    "    except Exception:\n",
    "        training_items = []\n",
    "    #user u's items in the test set\n",
    "    user_pos_test = data_generator.test_set[u]\n",
    "\n",
    "    all_items = set(range(ITEM_NUM))\n",
    "\n",
    "    # test items will be all items that are not in the training set\n",
    "    test_items = list(all_items - set(training_items))\n",
    "\n",
    "    # we get the binary rank (r) and the auc\n",
    "    if test_flag == 'part':\n",
    "        r, auc = ranklist_by_heapq(user_pos_test, test_items, rating, Ks)\n",
    "    else:\n",
    "        r, auc = ranklist_by_sorted(user_pos_test, test_items, rating, Ks)\n",
    "\n",
    "    # and finally we add the auc to all other metrics \n",
    "    # (precision, recall, ndcg and hit_ratio) at different Ks values\n",
    "    return get_performance(user_pos_test, r, auc, Ks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the only thing left is just test the performance for all users. Note that, within the `test` function they compute the ratings per user for all items:\n",
    "\n",
    "        user_batch = test_users[start: end]\n",
    "        item_batch = range(ITEM_NUM)\n",
    "        rate_batch = sess.run(model.batch_ratings, {model.users: user_batch,\n",
    "                                                    model.pos_items: item_batch,\n",
    "                                                    model.node_dropout: [0.] * len(layer_size),\n",
    "                                                    model.mess_dropout: [0.] * len(layer_size)})\n",
    "\n",
    "Then within the `test_one_user()` function, and in particular within the ranklists functions, only those items that are not in training will be taken into account. This is, within the `test_one_user` you will find:\n",
    "\n",
    "    test_items = list(all_items - set(training_items))\n",
    "\n",
    "The `test_items` are passed to `ranklist_by_sorted(user_pos_test, test_items, rating, Ks)` and within that function you have:\n",
    "\n",
    "    item_score = {}\n",
    "    for i in test_items:\n",
    "        item_score[i] = rating[i]\n",
    "\n",
    "So only items in the test_set will be tested. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(sess, model, users_to_test):\n",
    "    result = {'precision': np.zeros(len(Ks)), 'recall': np.zeros(len(Ks)), 'ndcg': np.zeros(len(Ks)),\n",
    "              'hit_ratio': np.zeros(len(Ks)), 'auc': 0.}\n",
    "\n",
    "    u_batch_size = BATCH_SIZE * 2 # I guess this is for speed\n",
    "    i_batch_size = BATCH_SIZE\n",
    "\n",
    "    test_users = users_to_test\n",
    "    n_test_users = len(test_users)\n",
    "    n_user_batchs = n_test_users // u_batch_size + 1\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for u_batch_id in tqdm(range(n_user_batchs)):\n",
    "        start = u_batch_id * u_batch_size\n",
    "        end = (u_batch_id + 1) * u_batch_size\n",
    "\n",
    "        # for every user we will run the model for all items. This is heavy\n",
    "        # Since this is testing, we set dropout to 0\n",
    "        user_batch = test_users[start: end]\n",
    "        item_batch = range(ITEM_NUM)\n",
    "        rate_batch = sess.run(model.batch_ratings, {model.users: user_batch,\n",
    "                                                    model.pos_items: item_batch,\n",
    "                                                    model.node_dropout: [0.] * len(layer_size),\n",
    "                                                    model.mess_dropout: [0.] * len(layer_size)})\n",
    "\n",
    "        user_batch_rating_uid = zip(rate_batch, user_batch)\n",
    "        batch_result = [test_one_user(x) for x in user_batch_rating_uid]\n",
    "        count += len(batch_result)\n",
    "\n",
    "        for re in batch_result:\n",
    "            result['precision'] += re['precision']/n_test_users\n",
    "            result['recall'] += re['recall']/n_test_users\n",
    "            result['ndcg'] += re['ndcg']/n_test_users\n",
    "            result['hit_ratio'] += re['hit_ratio']/n_test_users\n",
    "            result['auc'] += re['auc']/n_test_users\n",
    "\n",
    "\n",
    "    assert count == n_test_users\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_to_test = list(data_generator.test_set.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [08:02<00:00, 28.74s/it]\n"
     ]
    }
   ],
   "source": [
    "ret = test(sess, model, users_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': array([0.02270748, 0.0166823 , 0.01370934, 0.01189422, 0.01066582]),\n",
       " 'recall': array([0.0722882 , 0.10526895, 0.12927107, 0.14885057, 0.1658667 ]),\n",
       " 'ndcg': array([0.12699322, 0.1534744 , 0.17019852, 0.18291935, 0.1937012 ]),\n",
       " 'hit_ratio': array([0.32142139, 0.41871525, 0.4769576 , 0.51912385, 0.55362047]),\n",
       " 'auc': 0.0}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, train and test would be wrapped up in a loop over number of epochs and the early stop strategy is run like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_best_pre_0, stopping_step, should_stop = early_stopping(ret['recall'][0], cur_best_pre_0,\n",
    "                                                            stopping_step, expected_order='acc', flag_step=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07228820021501922 0 False\n"
     ]
    }
   ],
   "source": [
    "print(cur_best_pre_0, stopping_step, should_stop)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_ngcf)",
   "language": "python",
   "name": "conda_ngcf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
