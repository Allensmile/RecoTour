{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will concentrate solely on the model. The training process will be described in a separate notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import multiprocessing\n",
    "import heapq\n",
    "import random as rd\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from utility.load_data import Data\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "\n",
    "import pdb\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will try to go through their code as it is in their release, so for now, no eager execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path=''\n",
    "data_path='Data/'\n",
    "dataset='gowalla'\n",
    "pretrain=0\n",
    "verbosedt=1\n",
    "epoch=500\n",
    "emb_dim=64\n",
    "layer_size=[64]\n",
    "batch_size=1024\n",
    "regs=[1e-5,1e-5,1e-2]\n",
    "lr=0.01\n",
    "model_type='ngcf'\n",
    "adj_type='norm'\n",
    "alg_type='ngcf'\n",
    "gpu_id=0\n",
    "node_dropout_flag=0\n",
    "node_dropout=[0.1]\n",
    "mess_dropout=[0.1]\n",
    "Ks=[20, 40, 60, 80, 100]\n",
    "save_flag=0\n",
    "test_flag='part'\n",
    "report=0\n",
    "pretrain_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_users=29858, n_items=40981\n",
      "n_interactions=1027370\n",
      "n_train=810128, n_test=217242, sparsity=0.00084\n"
     ]
    }
   ],
   "source": [
    "data_generator = Data(path='Data/gowalla', batch_size=batch_size)\n",
    "USR_NUM, ITEM_NUM = data_generator.n_users, data_generator.n_items\n",
    "N_TRAIN, N_TEST = data_generator.n_train, data_generator.n_test\n",
    "BATCH_SIZE = data_generator.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "\n",
    "config = dict()\n",
    "config['n_users'] = data_generator.n_users\n",
    "config['n_items'] = data_generator.n_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already load adj matrix (70839, 70839) 0.22672343254089355\n"
     ]
    }
   ],
   "source": [
    "plain_adj, norm_adj, mean_adj = data_generator.get_adj_mat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use their default configuration: `Each decay factor between two connected nodes is set as 1/(out degree of the node) and each node is also assigned with 1 for self-connections`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['norm_adj'] = mean_adj + sp.eye(mean_adj.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = config['n_users']\n",
    "n_items = config['n_items']\n",
    "\n",
    "n_fold = 100\n",
    "norm_adj = config['norm_adj']\n",
    "n_nonzero_elems = norm_adj.count_nonzero()\n",
    "\n",
    "weight_size = layer_size\n",
    "n_layers = len(weight_size)\n",
    "\n",
    "model_type += '_%s_%s_l%d' % (adj_type, alg_type, n_layers)\n",
    "decay = regs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ngcf_norm_ngcf_l1'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialise the weights. You will notice that they do not consider the weights of the \"graph layers\" when loading pre-trained weights...I do not know why that is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_weights():\n",
    "    all_weights = dict()\n",
    "\n",
    "    initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "    if pretrain_data is None:\n",
    "        all_weights['user_embedding'] = tf.Variable(initializer([n_users, emb_dim]), name='user_embedding')\n",
    "        all_weights['item_embedding'] = tf.Variable(initializer([n_items, emb_dim]), name='item_embedding')\n",
    "        print('using xavier initialization')\n",
    "    else:\n",
    "        all_weights['user_embedding'] = tf.Variable(initial_value=pretrain_data['user_embed'], trainable=True,\n",
    "                                                    name='user_embedding', dtype=tf.float32)\n",
    "        all_weights['item_embedding'] = tf.Variable(initial_value=pretrain_data['item_embed'], trainable=True,\n",
    "                                                    name='item_embedding', dtype=tf.float32)\n",
    "        print('using pretrained initialization')\n",
    "\n",
    "    weight_size_list = [emb_dim] + weight_size\n",
    "\n",
    "    for k in range(n_layers):\n",
    "        # k = 0 are the embeddings\n",
    "        all_weights['W_gc_%d' %k] = tf.Variable(\n",
    "            initializer([weight_size_list[k], weight_size_list[k+1]]), name='W_gc_%d' % k)\n",
    "        all_weights['b_gc_%d' %k] = tf.Variable(\n",
    "            initializer([1, weight_size_list[k+1]]), name='b_gc_%d' % k)\n",
    "\n",
    "        all_weights['W_bi_%d' % k] = tf.Variable(\n",
    "            initializer([weight_size_list[k], weight_size_list[k + 1]]), name='W_bi_%d' % k)\n",
    "        all_weights['b_bi_%d' % k] = tf.Variable(\n",
    "            initializer([1, weight_size_list[k+1]]), name='b_bi_%d' % k)\n",
    "\n",
    "        all_weights['W_mlp_%d' % k] = tf.Variable(\n",
    "            initializer([weight_size_list[k], weight_size_list[k+1]]), name='W_mlp_%d' % k)\n",
    "        all_weights['b_mlp_%d' % k] = tf.Variable(\n",
    "            initializer([1, weight_size_list[k+1]]), name='b_mlp_%d' % k)\n",
    "\n",
    "    return all_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = tf.placeholder(tf.int32, shape=(None,))\n",
    "pos_items = tf.placeholder(tf.int32, shape=(None,))\n",
    "neg_items = tf.placeholder(tf.int32, shape=(None,))\n",
    "\n",
    "# dropout: node dropout (adopted on the ego-networks);\n",
    "#          ... since the usage of node dropout have higher computational cost,\n",
    "#          ... please use the 'node_dropout_flag' to indicate whether use such technique.\n",
    "#          message dropout (adopted on the convolution operations).\n",
    "node_dropout = tf.placeholder(tf.float32, shape=[None])\n",
    "mess_dropout = tf.placeholder(tf.float32, shape=[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0807 13:56:29.461400 140613546653440 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using xavier initialization\n"
     ]
    }
   ],
   "source": [
    "weights = _init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['user_embedding', 'item_embedding', 'W_gc_0', 'b_gc_0', 'W_bi_0', 'b_bi_0', 'W_mlp_0', 'b_mlp_0'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29858, 64)\n",
      "(40981, 64)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 64)\n"
     ]
    }
   ],
   "source": [
    "print(weights['user_embedding'].shape)\n",
    "print(weights['item_embedding'].shape)\n",
    "print(weights['W_gc_0'].shape)\n",
    "print(weights['W_bi_0'].shape)\n",
    "print(weights['W_mlp_0'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_sp_mat_to_sp_tensor(X):\n",
    "    coo = X.tocoo().astype(np.float32)\n",
    "    indices = np.mat([coo.row, coo.col]).transpose()\n",
    "    return tf.SparseTensor(indices, coo.data, coo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randint(1,5, (5, 3))\n",
    "X_csr = sp.csc_matrix(X)\n",
    "X_coo = X_csr.tocoo().astype(np.float32)\n",
    "indices = np.mat([X_coo.row, X_coo.col]).transpose()\n",
    "res = tf.SparseTensor(indices, X_coo.data, X_coo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.framework.sparse_tensor.SparseTensor at 0x7fe29463bfd0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _dropout_sparse(X, keep_prob, n_nonzero_elems):\n",
    "    \"\"\"\n",
    "    Dropout for sparse tensors.\n",
    "    \"\"\"\n",
    "    noise_shape = [n_nonzero_elems]\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += tf.random_uniform(noise_shape)\n",
    "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "    pre_out = tf.sparse_retain(X, dropout_mask)\n",
    "    \n",
    "    return pre_out * tf.math.divide(1., keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go step by step to understand what the `_dropout_sparse` function does. Let's create a sparse tensor, which is the input that `_dropout_sparse` yakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((10,10))\n",
    "coord = [(np.random.randint(10), np.random.randint(10)) for i in range(5)]\n",
    "for i,j in coord: X[i,j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sp_mtx = sp.csr_matrix(X)\n",
    "n_nonzero_elems = X_sp_mtx.count_nonzero()\n",
    "X_sp_tsr = _convert_sp_mat_to_sp_tensor(X_sp_mtx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a tensor with values uniformly randomly distributed between 0 and 1 to which we have added `1 - node_dropout[0] -> the keep_prob` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_shape = [n_nonzero_elems]\n",
    "random_tensor = 1 - node_dropout[0]\n",
    "random_tensor += tf.random_uniform(noise_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we create a mask dropping rows (i.e. nodes) according to the `node_dropout` param."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if you run this in eager execution (just don't use `placeholders`) you will see that dropout_mask containes booleans, that will indicate whether we drop or not a certain row/node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Cast:0' shape=(5,) dtype=bool>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0807 13:58:25.327770 140613546653440 deprecation.py:323] From /home/ubuntu/anaconda3/envs/ngcf/lib/python3.6/site-packages/tensorflow/python/ops/sparse_ops.py:1719: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "pre_out = tf.sparse_retain(X_sp_tsr, dropout_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just now split the Adjancency matrix so is tractable, nothing major here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_A_hat(X):\n",
    "    \"split the Adjancency matrix so is tractable\"\n",
    "    A_fold_hat = []\n",
    "\n",
    "    fold_len = (n_users + n_items) // n_fold\n",
    "    for i_fold in range(n_fold):\n",
    "        start = i_fold * fold_len\n",
    "        if i_fold == n_fold -1:\n",
    "            end = n_users + n_items\n",
    "        else:\n",
    "            end = (i_fold + 1) * fold_len\n",
    "\n",
    "        A_fold_hat.append(_convert_sp_mat_to_sp_tensor(X[start:end]))\n",
    "    return A_fold_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_A_hat_node_dropout(X):\n",
    "    A_fold_hat = []\n",
    "\n",
    "    fold_len = (n_users + n_items) // n_fold\n",
    "    for i_fold in range(n_fold):\n",
    "        start = i_fold * fold_len\n",
    "        if i_fold == n_fold -1:\n",
    "            end = n_users + n_items\n",
    "        else:\n",
    "            end = (i_fold + 1) * fold_len\n",
    "\n",
    "        temp = _convert_sp_mat_to_sp_tensor(X[start:end])\n",
    "        n_nonzero_temp = X[start:end].count_nonzero()\n",
    "        A_fold_hat.append(_dropout_sparse(temp, 1 - node_dropout[0], n_nonzero_temp))\n",
    "\n",
    "    return A_fold_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_dropout_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a set of adjacency sub-matrix.\n",
    "if node_dropout_flag:\n",
    "    # node dropout.\n",
    "    A_fold_hat = _split_A_hat_node_dropout(norm_adj)\n",
    "else:\n",
    "    A_fold_hat = _split_A_hat(norm_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(A_fold_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(708), Dimension(70839)])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_fold_hat[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here they build embeddings based on, or thinking in ego-networks, which, quoting directly [this page](http://www.analytictech.com/networks/egonet.htm): Ego networks consist of a focal node (\"ego\") and the nodes to whom ego is directly connected to (these are called \"alters\") plus the ties, if any, among the alters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_embeddings = tf.concat([weights['user_embedding'], weights['item_embedding']], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(70839), Dimension(64)])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ego_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook we only use 1 layer. This is, in their Figure 2 there will be only one `Embedding Propagation Layer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`A_fold_hat[0]` is a subset of the adjacency matrix, with dimensions `TensorShape([Dimension(708), Dimension(70839)])`. This matrix has non-zero elements in those locations where there are node connections. For example, the first row will correspond to the 1st user that interacted with the 1st 127 items (0 to 126). \n",
    "`row_0 (1st element is the user_id) = 0 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "nz_idx=np.where(norm_adj.todense()[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0, 29858, 29859, 29860, 29861, 29862, 29863, 29864, 29865,\n",
       "       29866, 29867, 29868, 29869, 29870, 29871, 29872, 29873, 29874,\n",
       "       29875, 29876, 29877, 29878, 29879, 29880, 29881, 29882, 29883,\n",
       "       29884, 29885, 29886, 29887, 29888, 29889, 29890, 29891, 29892,\n",
       "       29893, 29894, 29895, 29896, 29897, 29898, 29899, 29900, 29901,\n",
       "       29902, 29903, 29904, 29905, 29906, 29907, 29908, 29909, 29910,\n",
       "       29911, 29912, 29913, 29914, 29915, 29916, 29917, 29918, 29919,\n",
       "       29920, 29921, 29922, 29923, 29924, 29925, 29926, 29927, 29928,\n",
       "       29929, 29930, 29931, 29932, 29933, 29934, 29935, 29936, 29937,\n",
       "       29938, 29939, 29940, 29941, 29942, 29943, 29944, 29945, 29946,\n",
       "       29947, 29948, 29949, 29950, 29951, 29952, 29953, 29954, 29955,\n",
       "       29956, 29957, 29958, 29959, 29960, 29961, 29962, 29963, 29964,\n",
       "       29965, 29966, 29967, 29968, 29969, 29970, 29971, 29972, 29973,\n",
       "       29974, 29975, 29976, 29977, 29978, 29979, 29980, 29981, 29982,\n",
       "       29983, 29984])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nz_idx[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember we added a diagonal to account for node-self-connections. We can see that the location of the 1st non-zero element, apart from that in the diagonal (1,1) is 29859, so all good. When multiplying that row by the tensor `ego_embeddings` you will get the summation of the embeddings of the user 0 plus the weighted embeddings (remember the matrix is normalised) corresponding to items from 0-126. \n",
    "\n",
    "(1,70839) x (70839 x 64) = (1,64)\n",
    "\n",
    "Let's have a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = tf.sparse_tensor_dense_matmul(A_fold_hat[0], ego_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(708), Dimension(64)])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we move down the adjacency matrix to row index 29858, remember from there in advance is filled with `R.T`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "nz_idx_2=np.where(norm_adj.todense()[29858, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,   140,   241,  1056,  1445,  2539,  3216,  3403,  5216,\n",
       "        9443, 24111, 26313, 29184, 29858])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nz_idx_2[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the users that interacted with item 0, so when we multiply the corresponding A_fold_hat with  ego_embeddings we get the summation of the embeddings for those users plus the embedding for item 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_embed = []\n",
    "for f in range(n_fold):\n",
    "    temp_embed.append(tf.sparse_tensor_dense_matmul(A_fold_hat[f], ego_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "side_embeddings = tf.concat(temp_embed, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(70839), Dimension(64)])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "side_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 1st row of this matrix contains the weighted sum of all the item embeddings that user_id 0 interacted with plus the embeddings of that user while rwo # n_users+1 contains the summation of all user embeddings that interacted with item_id 0 plus the embeddings of that item.  \n",
    "\n",
    "At this stage we have multiplied all embeddings by the adjacency matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(64), Dimension(64)])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights['W_gc_0'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This below corresponds to the 1st term in their expression (3). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_embeddings = tf.nn.leaky_relu(\n",
    "    tf.matmul(side_embeddings, weights['W_gc_%d' % k]) + weights['b_gc_%d' % k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(70839), Dimension(64)])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, ego_embeddings is simply the concatenation (over rows) of the user and item embeddings and  side_embeddings contains the weighted sum of the item embeddings that a certain user interacted with (plus the embeddings of that user) and the weighted sum of the user embeddings that an item \"interacted\" with (plus the embeddings of that item). \n",
    "\n",
    "We now move to the second term of their expression (3). They call `bi_embeddings` to the element-wise multiplication of the two tensors (and then we apply the same operation as before). In their own words: \"we additionally\n",
    "encode the interaction between ei and eu into the message being passed via ei ⊙ eu , where ⊙ denotes the element-wise product. This makes the message dependent on the affinity between ei and eu , e.g., passing more messages from the similar items. This not only increases the model representation ability, but also boosts the performance for recommendation \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bi messages of neighbors.\n",
    "bi_embeddings = tf.multiply(ego_embeddings, side_embeddings)\n",
    "# transformed bi messages of neighbors.\n",
    "bi_embeddings = tf.nn.leaky_relu(\n",
    "    tf.matmul(bi_embeddings, weights['W_bi_%d' % k]) + weights['b_bi_%d' % k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(70839), Dimension(64)])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Their expression (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_embeddings = sum_embeddings + bi_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0807 14:23:39.308569 140613546653440 deprecation.py:506] From <ipython-input-60-c7abd9f97775>:2: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# message dropout.\n",
    "ego_embeddings = tf.nn.dropout(ego_embeddings, 1 - mess_dropout[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here they add an extra normalization that I can't find in the paper, since the Laplacian norm that they refer to is already applied we built the Adjacency Matrix..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the distribution of embeddings.\n",
    "norm_embeddings = tf.math.l2_normalize(ego_embeddings, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(70839), Dimension(64)])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper they show how they concatenate the output from all the `Embedding Propagation Layer` + the so called `ego_embeddings`. This would be done recursively if n_layers would be > 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = [ego_embeddings] + [norm_embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = tf.concat(all_embeddings, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(70839), Dimension(128)])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_g_embeddings, i_g_embeddings = tf.split(all_embeddings, [n_users, n_items], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29858, 128)\n",
      "(40981, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(u_g_embeddings.shape), print(i_g_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it, that is the model. Nonetheless, and even though we will refer to this function in the corresponding training notebook, let's described here the pairwise Bayesian Personalized Ranking (BPR) loss as implemented in their paper. This loss considers the relative order between observed and unobserved user-item interactions. Specifically, BPR assumes that the observed interactions, which are more reflective of a user’s preferences, should be assigned higher prediction values than unobserved ones. Let's see it in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bpr_loss(users, pos_items, neg_items):\n",
    "    pos_scores = tf.reduce_sum(tf.multiply(users, pos_items), axis=1)\n",
    "    neg_scores = tf.reduce_sum(tf.multiply(users, neg_items), axis=1)\n",
    "\n",
    "    # regularization term\n",
    "    regularizer = tf.nn.l2_loss(users) + tf.nn.l2_loss(pos_items) + tf.nn.l2_loss(neg_items)\n",
    "    regularizer = regularizer/batch_size\n",
    "    emb_loss = decay * regularizer\n",
    "\n",
    "    # First term of their expression 11\n",
    "    maxi = tf.log(tf.nn.sigmoid(pos_scores - neg_scores))\n",
    "    mf_loss = tf.negative(tf.reduce_mean(maxi))\n",
    "\n",
    "    # ? no idea...a 0.0 constant\n",
    "    reg_loss = tf.constant(0.0, tf.float32, [1])\n",
    "\n",
    "    return mf_loss, emb_loss, reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_g_embeddings = tf.nn.embedding_lookup(tf.identity(u_g_embeddings), users)\n",
    "pos_i_g_embeddings = tf.nn.embedding_lookup(i_g_embeddings, pos_items)\n",
    "neg_i_g_embeddings = tf.nn.embedding_lookup(i_g_embeddings, neg_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_loss, emb_loss, reg_loss = create_bpr_loss(u_g_embeddings,pos_i_g_embeddings,neg_i_g_embeddings)\n",
    "loss = mf_loss + emb_loss + reg_loss\n",
    "opt = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_8:0' shape=(1,) dtype=float32>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_ngcf)",
   "language": "python",
   "name": "conda_ngcf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
