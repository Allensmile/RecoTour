{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data for the model\n",
    "\n",
    "In this notebook my intention is going through the [original](https://github.com/xiangwang1223/neural_graph_collaborative_filtering) implementation keeping only what I consider necessary to eventually understand the model. There are some elements in the authors' original code that is related to the different experiments carried out in the [paper](https://arxiv.org/pdf/1905.08108.pdf) that I will not include here. \n",
    "\n",
    "The code below is my adaptation. When this differs significantly from that of the original paper I will include the details. Also below I will be using the toy dataset that I have generated with `generate_toy_data.py`, including only a few users and items (and random interactions). To generate a toy dataset with random interactions (just to play with) simply run, for example:\n",
    "    \n",
    "    python generate_toy_data.py --n_users 100 --n_items 200 --min_interaction 11 --max_interactions 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rd\n",
    "import scipy.sparse as sp\n",
    "import pdb\n",
    "\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"/Users/javier/ml_exercises_python/RecoTour/Amazon/neural_graph_cf/Data/gowalla/\")\n",
    "batch_size = 16\n",
    "\n",
    "train_file = path/'train.txt'\n",
    "test_file = path/'test.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users and items are numbered from 0 to (n_users-1) and (n_items-1), so let's count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of users and items. \n",
    "n_users, n_items = 0, 0\n",
    "n_train, n_test = 0, 0\n",
    "\n",
    "exist_users = []\n",
    "with open(train_file) as f:\n",
    "    for l in f.readlines():\n",
    "        if len(l) > 0:\n",
    "            l = l.strip('\\n').split(' ')\n",
    "            # first element is the user_id, then items\n",
    "            uid = int(l[0])\n",
    "            items = [int(i) for i in l[1:]]\n",
    "            exist_users.append(uid)\n",
    "            n_items = max(n_items, max(items))\n",
    "            n_users = max(n_users, uid)\n",
    "            n_train += len(items)\n",
    "\n",
    "# same as before but for testing\n",
    "with open(test_file) as f:\n",
    "    for l in f.readlines():\n",
    "        if len(l) > 0:\n",
    "            l = l.strip('\\n')\n",
    "            try:\n",
    "                items = [int(i) for i in l.split(' ')[1:]]\n",
    "            except Exception:\n",
    "                continue\n",
    "            n_items = max(n_items, max(items))\n",
    "            n_test += len(items)\n",
    "n_items += 1\n",
    "n_users += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40981 29858\n"
     ]
    }
   ],
   "source": [
    "print(n_items, n_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All OK. Let's build the interactions/ratings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = sp.dok_matrix((n_users, n_items), dtype=np.float32)\n",
    "train_set, test_set = {}, {}\n",
    "with open(train_file) as f_train, open(test_file) as f_test:\n",
    "    for l in f_train.readlines():\n",
    "        if len(l) == 0: break\n",
    "        l = l.strip('\\n')\n",
    "        items = [int(i) for i in l.split(' ')]\n",
    "        uid, train_items = items[0], items[1:]\n",
    "        #Â simply 1 if user interacted with item, otherwise, 0.\n",
    "        for i in train_items:\n",
    "            R[uid, i] = 1.\n",
    "        train_set[uid] = train_items\n",
    "\n",
    "    for l in f_test.readlines():\n",
    "        if len(l) == 0: break\n",
    "        l = l.strip('\\n')\n",
    "        try:\n",
    "            items = [int(i) for i in l.split(' ')]\n",
    "        except Exception:\n",
    "            continue\n",
    "        uid, test_items = items[0], items[1:]\n",
    "        test_set[uid] = test_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<29858x40981 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 810128 stored elements in Dictionary Of Keys format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[7580, 3730, 5983, 5990, 7608, 1213, 6017, 7510, 7513, 8343]\n"
     ]
    }
   ],
   "source": [
    "print(train_set[0][:10])\n",
    "print(test_set[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They use a number of difference adjacency matrices, see [here](https://github.com/xiangwang1223/neural_graph_collaborative_filtering): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_adj_single(adj):\n",
    "    # rowsum = out-degree of the node    \n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    # inverted and set to 0 if no connections\n",
    "    d_inv = np.power(rowsum, -1).flatten()\n",
    "    d_inv[np.isinf(d_inv)] = 0.\n",
    "    # sparse diagonal matrix with the normalizing factors in the diagonal\n",
    "    d_mat_inv = sp.diags(d_inv)\n",
    "    # dot product resulting in a row-normalised version of the input matrix\n",
    "    norm_adj = d_mat_inv.dot(adj)\n",
    "    return norm_adj.tocoo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is used to check the expression 8 in their paper, where the Laplacian Matrix is formulated as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\text{D}^{\\frac{-1}{2}}\\text{A}\\text{D}^{\\frac{1}{2}}\n",
    "$$\n",
    "\n",
    "where D is the diagonal degree matrix and A:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix} \n",
    "0 & \\text{R} \\\\\n",
    "\\text{R}^{\\text{T}} & 0 \n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_adj_if_equal(adj):\n",
    "    dense_A = np.array(adj.todense())\n",
    "    degree = np.sum(dense_A, axis=1, keepdims=False)\n",
    "    temp = np.dot(np.diag(np.power(degree, -1)), dense_A)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build A, the adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_mat = sp.dok_matrix((n_users + n_items, n_users + n_items), dtype=np.float32)\n",
    "adj_mat = adj_mat.tolil()\n",
    "\n",
    "# A:\n",
    "s = time()\n",
    "adj_mat[:n_users, n_users:] = R.tolil()\n",
    "adj_mat[n_users:, :n_users] = R.tolil().T\n",
    "print(time()-s)\n",
    "adj_mat = adj_mat.todok()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<30000x30000 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 485112 stored elements in Dictionary Of Keys format>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "along with the \"normal\" adjancecy matrix, we generate two additional ones\n",
    "\n",
    "`norm_adj_mat`: each decay factor bewteen two connected nodes is set as `1/(out degree of the node + self-conncetion)`\n",
    "\n",
    "`mean_adj_mat`: each decay factor bewteen two connected nodes is set as `1/(out degree of the node)`\n",
    "\n",
    "eventually a forth one will also be used (in fact is the default one) which will be\n",
    "\n",
    "`ngcf_adj_mat`: each decay factor bewteen two connected nodes is set as `1/(out degree of the node)` and each node is also assigned with 1 for self-connections. This is: `norm_adj_mat + sp.eye(mean_adj.shape[0])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_adj_mat = normalized_adj_single(adj_mat + sp.eye(adj_mat.shape[0]))\n",
    "mean_adj_mat = normalized_adj_single(adj_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look to the 1st row and search for non-zero elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid0_nonzero = np.where(adj_mat[0].todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([104, 111, 114, 118, 119, 129, 131, 134, 138, 140, 151, 156, 158,\n",
       "        167, 178, 184, 189, 193, 194, 197, 198, 208, 210, 212, 222, 232,\n",
       "        245, 259, 263, 265, 266, 269, 272, 275, 281, 288, 290, 299]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uid0_nonzero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the training data for the 1st user (id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 11, 14, 18, 19, 29, 31, 34, 38, 40, 51, 56, 58, 67, 78, 84, 89, 93, 94, 97, 98, 108, 110, 112, 122, 132, 145, 159, 163, 165, 166, 169, 172, 175, 181, 188, 190, 199]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(train_set[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uid0_nonzero[0]) == len(train_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the 1st element different than 0 is 104, which is equal to the number of users plus the first item_id that user 0 interacted with (4). Note that if we included self-connections, the elements in the diagonal would also be diff than 0. \n",
    "\n",
    "Let's now create \"negative pools\", simply collections of N items that users never interacted with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_pools = {}\n",
    "for u in train_set.keys():\n",
    "    neg_items = list(set(range(n_items)) - set(train_set[u]))\n",
    "    pools = np.random.choice(neg_items, 10)\n",
    "    neg_pools[u] = pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([107, 152,  59, 154, 149, 102,  33,  99, 195, 100])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_pools[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions sample positive and negative (never seen or interacted with) items either directly from the dataset, or from the previously generated \"negative pools\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_pos_items_for_u(u, num):\n",
    "    pos_items = train_set[u]\n",
    "    n_pos_items = len(pos_items)\n",
    "    pos_batch = []\n",
    "    while True:\n",
    "        # Once we have sample num positive items, stop\n",
    "        if len(pos_batch) == num: break\n",
    "        pos_id = np.random.randint(low=0, high=n_pos_items, size=1)[0]\n",
    "        pos_i_id = pos_items[pos_id]\n",
    "        if pos_i_id not in pos_batch: pos_batch.append(pos_i_id)\n",
    "    return pos_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_neg_items_for_u(u, num):\n",
    "    neg_items = []\n",
    "    while True:\n",
    "        # Once we have sample num negative items, stop\n",
    "        if len(neg_items) == num: break\n",
    "        neg_id = np.random.randint(low=0, high=n_items,size=1)[0]\n",
    "        if neg_id not in train_set[u] and neg_id not in neg_items:\n",
    "            neg_items.append(neg_id)\n",
    "    return neg_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_neg_items_for_u_from_pools(u, num):\n",
    "    # this line must be a bug because no train_items[u] will ever be in neg_pools[u], \n",
    "    # neg_items = list(set(range(n_items)) - set(train_set[u]))\n",
    "    # pools = np.random.choice(neg_items, 100)\n",
    "    #Â neg_pools[u] = pools\n",
    "    neg_items = list(set(neg_pools[u]) - set(train_set[u]))\n",
    "    return rd.sample(neg_items, num)\n",
    "\n",
    "# To me this should be\n",
    "def sample_neg_items_for_u_from_pools(u, num):\n",
    "    return rd.sample(neg_pools[u], num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "users, pos_items, neg_items = [], [], []\n",
    "for u in np.random.choice(exist_users, 5):\n",
    "    users.append(u)\n",
    "    pos_items += sample_pos_items_for_u(u, 1)\n",
    "    neg_items += sample_neg_items_for_u(u, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39, 42, 62, 51, 94]\n",
      "[23, 162, 50, 188, 125]\n",
      "[170, 184, 95, 30, 137]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(users), print(pos_items), print(neg_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see if item 50 and 95 are positive and negative respectively for user 62 (for example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(50 in train_set[62])\n",
    "print(95 not in train_set[62])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is about it for us, because the functions below will not be used in this repo. \n",
    "\n",
    "These functions correspond to their study of the effect of sparsity. Have a look to their section 4.3.2 Performance Comparison w.r.t. Interaction Sparsity Levels: *\".... In particular, based on interaction number per user, we divide the test set into four groups, each of which has the same total interactions...\"*\n",
    "\n",
    "Nonetheless, here is the code and an explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sparsity_split():\n",
    "    all_users_to_test = list(test_set.keys())\n",
    "    user_n_iid = dict()\n",
    "\n",
    "    # generate a dictionary to store (key=n_iids, value=a list of uid).\n",
    "    for uid in all_users_to_test:\n",
    "        #Â train and test items for user_id\n",
    "        train_iids = train_set[uid]\n",
    "        test_iids = test_set[uid]\n",
    "\n",
    "        # number of \"interactions\"\n",
    "        n_iids = len(train_iids) + len(test_iids)\n",
    "\n",
    "        if n_iids not in user_n_iid.keys():\n",
    "            #Â dictionary where the keys are the number of interactions \n",
    "            # and the values are the users that have that number of interactions\n",
    "            user_n_iid[n_iids] = [uid]\n",
    "        else:\n",
    "            user_n_iid[n_iids].append(uid)\n",
    "    split_uids = list()\n",
    "\n",
    "    # split the whole user set into four subset.\n",
    "    temp = []\n",
    "    count = 1\n",
    "    fold = 4\n",
    "    # total number of interactions in the dataset\n",
    "    n_count = (n_train + n_test) \n",
    "    n_rates = 0\n",
    "\n",
    "    split_state = []\n",
    "    for idx, n_iids in enumerate(sorted(user_n_iid)):\n",
    "        temp += user_n_iid[n_iids]\n",
    "        # n_rates -> number of ratings\n",
    "        # n_iids  -> key corresponding to a certain number of interactions (e.g. 10 ratins)\n",
    "        #Â len(user_n_iid[n_iids]) -> number of users that interacted with 10 items\n",
    "        n_rates += n_iids * len(user_n_iid[n_iids])\n",
    "        n_count -= n_iids * len(user_n_iid[n_iids])\n",
    "        # when number of rates/interaction has reached 25% of the total number of interactions, \n",
    "        # append the corresponding users to split_uids (remember we loop over sorted(user_n_iid))\n",
    "        if n_rates >= count * 0.25 * (n_train + n_test):\n",
    "            split_uids.append(temp)\n",
    "\n",
    "            state = '#inter per user<=[%d], #users=[%d], #all rates=[%d]' %(n_iids, len(temp), n_rates)\n",
    "            split_state.append(state)\n",
    "            print(state)\n",
    "\n",
    "            temp = []\n",
    "            n_rates = 0\n",
    "            fold -= 1 # don't think we need this if we manually state 0.25\n",
    "        \n",
    "        if idx == len(user_n_iid.keys()) - 1 or n_count == 0:\n",
    "            split_uids.append(temp)\n",
    "\n",
    "            state = '#inter per user<=[%d], #users=[%d], #all rates=[%d]' % (n_iids, len(temp), n_rates)\n",
    "            split_state.append(state)\n",
    "            print(state)\n",
    "    return split_uids, split_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparsity_split():\n",
    "    #Â here, once the previous function is understood, there is not much to explain\n",
    "    try:\n",
    "        split_uids, split_state = [], []\n",
    "        lines = open(path + '/sparsity.split', 'r').readlines()\n",
    "\n",
    "        for idx, line in enumerate(lines):\n",
    "            if idx % 2 == 0:\n",
    "                split_state.append(line.strip())\n",
    "                print(line.strip())\n",
    "            else:\n",
    "                split_uids.append([int(uid) for uid in line.strip().split(' ')])\n",
    "        print('get sparsity split.')\n",
    "\n",
    "    except Exception:\n",
    "        split_uids, split_state = create_sparsity_split()\n",
    "        f = open(path + '/sparsity.split', 'w')\n",
    "        for idx in range(len(split_state)):\n",
    "            f.write(split_state[idx] + '\\n')\n",
    "            f.write(' '.join([str(uid) for uid in split_uids[idx]]) + '\\n')\n",
    "        print('create sparsity split.')\n",
    "\n",
    "    return split_uids, split_state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
