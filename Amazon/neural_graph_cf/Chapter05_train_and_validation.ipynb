{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing\n",
    "\n",
    "Let's start by manually defining some neccesary parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "import scipy.sparse as sp\n",
    "import multiprocessing\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from time import time\n",
    "from functools import partial\n",
    "from utils.dataset import Data\n",
    "from utils.metrics import ranklist_by_heapq, get_performance\n",
    "# from utils.parser import parse_args\n",
    "from model.ngcf import NGCF_BPR\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_users=1000, n_items=2000\n",
      "n_interactions=30780\n",
      "n_train=24228, n_test=6552, sparsity=0.01539\n",
      "already load adj matrix (3000, 3000) 0.010658025741577148\n"
     ]
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "Ks = [10, 20]\n",
    "\n",
    "data_path = \"Data/toy_data/\"\n",
    "batch_size = 32\n",
    "data_generator = Data(data_path, batch_size, val=False)\n",
    "n_users = data_generator.n_users\n",
    "n_items = data_generator.n_items\n",
    "\n",
    "_, _, mean_adj = data_generator.get_adj_mat()\n",
    "adjacency_matrix = mean_adj + sp.eye(mean_adj.shape[0])\n",
    "\n",
    "emb_size = 12\n",
    "layers = [12, 6]\n",
    "node_dropout = 0.1\n",
    "mess_dropout = [0.1]*len(layers)\n",
    "regularization = 1e-5\n",
    "lr = 0.01\n",
    "n_fold = 10\n",
    "\n",
    "pretrain = 0\n",
    "\n",
    "print_every, eval_every, save_every = 1, 1, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NGCF_BPR(n_users, n_items, emb_size, adjacency_matrix, layers,\n",
    "    node_dropout, mess_dropout, regularization, n_fold, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NGCF_BPR(\n",
       "  (embeddings_user): Embedding(1000, 12)\n",
       "  (embeddings_item): Embedding(2000, 12)\n",
       "  (g_embeddings_user): Embedding(1000, 30)\n",
       "  (g_embeddings_item): Embedding(2000, 30)\n",
       "  (W1): ModuleList(\n",
       "    (0): Linear(in_features=12, out_features=12, bias=True)\n",
       "    (1): Linear(in_features=12, out_features=6, bias=True)\n",
       "  )\n",
       "  (W2): ModuleList(\n",
       "    (0): Linear(in_features=12, out_features=12, bias=True)\n",
       "    (1): Linear(in_features=12, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the initial and the graph embeddings could well be simply tensors (`nn.Parameter(torch.rand(1000, 12))`). They do not need to be defined as modules `nn.Embedding`. However, I simply prefer to do it this way, is just a matter of personal taste. The difference in syntax is minimal. For example, if defined as tensors one could slice them as `g_embeddings_user[idx]` where idx can simply be an `int` while if defined as modules, we need to pass the index of the lookup table as a `LongTensor` like `g_embeddings_user(idx)`. Also, accessing the weights is direct if defined as tensors while you need to refer to the attribute `.weight` if module. \n",
    "\n",
    "Other than that, one can see that the model components are rather simple. We need initial embeddings, these will be concatenated over rows, multiplied by the Laplacian matrix, and then passed through a series of dense layers recursively. \n",
    "\n",
    "You can see that the so-called graph embeddings (`g_embeddings`) have dimension equal to the input embeddings size (12) plus the output dimension of every linear layer (12 and 6). \n",
    "\n",
    "Let's now move to the training phase. The training phase is your typical pytorch training function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_generator, optimizer):\n",
    "    model.train()\n",
    "    n_batch = data_generator.n_train // data_generator.batch_size + 1\n",
    "    running_loss=0\n",
    "    for _ in range(n_batch):\n",
    "        # tuple (users, positive items, negative items)     \n",
    "        u, i, j = data_generator.sample()\n",
    "        # if you had defined the embeddings as tensors, you would not need to do this\n",
    "        u, i, j = torch.LongTensor(u), torch.LongTensor(i), torch.LongTensor(j)\n",
    "        optimizer.zero_grad()\n",
    "        # the forward pass returns the user, pos and neg embeddings    \n",
    "        u_emb, p_emb, n_emb =  model(u, i, j)\n",
    "        # the embeddings are then pass to the Bayesian Personalised Ranking loss\n",
    "        loss = model.bpr_loss(u_emb,p_emb,n_emb)\n",
    "        # and from here on, the usual\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So...it is indeed typical, although we use the `bpr_loss`. We have not talked about it yet, so let's have a look.\n",
    "The definition in the [paper](https://arxiv.org/pdf/1905.08108.pdf) is:\n",
    "\n",
    "$$\n",
    "Loss = \\sum_{(u,i,j) \\in \\mathcal{O}} -ln \\big(\\sigma(\\hat{y}_{ui} - \\hat{y}_{uj})\\big) + \\lambda ||\\Theta||^{2}_{2}\n",
    "$$\n",
    "\n",
    "Where $\\mathcal{O} = \\{ (u,i,j)|(u,i) \\in  R^{+}, (u,j) \\in R^{-} \\}$ is the set of training tuples with $R^{+}$ and $R^{-}$ corresponding to observed and unobserved interactions (aka positive and negative) respectively. $\\sigma$ is the sigmoid function and $||\\Theta|| = \\{ \\text{E}, \\{ \\textbf{W}^{l}_{1},\\textbf{W}^{l}_{2} \\}^{L}_{l=1}  \\}$ are all training parameters. \n",
    "\n",
    "In pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpr_loss(self, u, i, j):\n",
    "    # first term\n",
    "    y_ui = torch.mul(u, i).sum(dim=1)\n",
    "    y_uj = torch.mul(u, j).sum(dim=1)\n",
    "    log_prob = (torch.log(torch.sigmoid(y_ui-y_uj))).mean()\n",
    "\n",
    "    # regularization\n",
    "    l2norm = (torch.sum(u**2)/2. + torch.sum(i**2)/2. + torch.sum(j**2)/2.).mean()\n",
    "    l2reg  = reg*l2norm\n",
    "\n",
    "    # Loss\n",
    "    return -log_prob + l2reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "okay, so now we now how the training happens, let's move to the validation/testing. Here, we will first use the authors `early_stopping` function. I am sure there are more \"pytorchian\" ways of doing it, but this function is simple and does the job, so let's use it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_stopping(log_value, best_value, stopping_step, expected_order='asc', patience=10):\n",
    "\n",
    "    # better is higher or lower\n",
    "    assert expected_order in ['asc', 'dec']\n",
    "\n",
    "    if (expected_order == 'asc' and log_value >= best_value) or (expected_order == 'dec' and log_value <= best_value):\n",
    "        stopping_step = 0\n",
    "        best_value = log_value\n",
    "    else:\n",
    "        stopping_step += 1\n",
    "\n",
    "    if stopping_step >= patience:\n",
    "        print(\"Early stopping is trigger at step: {} log:{}\".format(patience, log_value))\n",
    "        should_stop = True\n",
    "    else:\n",
    "        should_stop = False\n",
    "\n",
    "    return best_value, stopping_step, should_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how we test on one user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_user(x):\n",
    "    \"\"\"\n",
    "    x will be a zip object where the 1st element will be the user id and the 2nd\n",
    "    will be the scores for all items in the dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    u = x[0]\n",
    "    rating = x[1]\n",
    "\n",
    "    try:\n",
    "        training_items = data_generator.train_items[u]\n",
    "    except Exception:\n",
    "        training_items = []\n",
    "\n",
    "    # items that the user did interact with during testing\n",
    "    user_pos_test = data_generator.test_set[u]\n",
    "    all_items = set(range(data_generator.n_items))\n",
    "    # test_items include negative items and  user_pos_test\n",
    "    test_items = list(all_items - set(training_items))\n",
    "\n",
    "    # and now we compute the metrics as described in the notebook Chapter03_metrics.ipynb.\n",
    "    r, auc = ranklist_by_heapq(user_pos_test, test_items, rating, Ks)\n",
    "\n",
    "    return get_performance(user_pos_test, r, auc, Ks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now that we know how to test in one user, let's do it for the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, data_generator):\n",
    "\n",
    "    result = {\n",
    "        'precision': np.zeros(len(Ks)),\n",
    "        'recall': np.zeros(len(Ks)),\n",
    "        'ndcg': np.zeros(len(Ks)),\n",
    "        'hit_ratio': np.zeros(len(Ks)),\n",
    "        'auc': 0.\n",
    "        }\n",
    "\n",
    "    # here we can use larger batches\n",
    "    u_batch_size = data_generator.batch_size * 2\n",
    "\n",
    "    # test users are all users really\n",
    "    test_users = list(data_generator.test_set.keys())\n",
    "    n_test_users = len(test_users)\n",
    "    n_user_batchs = n_test_users // u_batch_size + 1\n",
    "    \n",
    "    # n_test_items are normally all items\n",
    "    n_test_items = data_generator.n_items\n",
    "\n",
    "    count = 0\n",
    "    p = Pool(cores)\n",
    "    for u_batch_id in range(n_user_batchs):\n",
    "        start = u_batch_id * u_batch_size\n",
    "        end = (u_batch_id + 1) * u_batch_size\n",
    "\n",
    "        user_batch = test_users[start: end]\n",
    "        item_batch = np.arange(n_test_items)\n",
    "    \n",
    "        # ratings are simply the matrix multiplication of the graph embeddings. One option \n",
    "        # could be wrap this up into a sigmoid, to keep all between 0,1\n",
    "        rate_batch  = torch.mm(model.g_embeddings_user.weight, model.g_embeddings_item.weight.t())\n",
    "\n",
    "        # detach and to CPU so it can be parallelised through the cores\n",
    "        rate_batch_np = rate_batch.detach().numpy()\n",
    "        batch_result = p.map(test_one_user, zip(user_batch,rate_batch_np))\n",
    "\n",
    "        count += len(batch_result)\n",
    "\n",
    "        for re in batch_result:\n",
    "            result['precision'] += re['precision']/n_test_users\n",
    "            result['recall'] += re['recall']/n_test_users\n",
    "            result['ndcg'] += re['ndcg']/n_test_users\n",
    "            result['hit_ratio'] += re['hit_ratio']/n_test_users\n",
    "            result['auc'] += re['auc']/n_test_users\n",
    "    assert count == n_test_users\n",
    "    p.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how all comes together! (Note that the process here is extremely inefficient since we are splitting a 3000x3000 matrix into 10 folds and using a 32 batch for only 1000 users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 46.13s, Loss = 519.7186\n",
      "VALIDATION. \n",
      "Epoch: 0, 0.57s \n",
      " Recall@10: 0.0048, Recall@20: 0.0090 \n",
      "Precision@10: 0.0034, Precision@20: 0.0032 \n",
      "Hit_ratio@10: 0.0340, Hit_ratio@20: 0.0600 \n",
      "NDCG@10: 0.0147, NDCG@20: 0.0215\n",
      "Epoch:1 46.39s, Loss = 518.2577\n",
      "VALIDATION. \n",
      "Epoch: 1, 0.59s \n",
      " Recall@10: 0.0055, Recall@20: 0.0115 \n",
      "Precision@10: 0.0032, Precision@20: 0.0036 \n",
      "Hit_ratio@10: 0.0320, Hit_ratio@20: 0.0680 \n",
      "NDCG@10: 0.0141, NDCG@20: 0.0236\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "stopping_step, should_stop = 0, False\n",
    "for epoch in range(2):\n",
    "    t1 = time()\n",
    "    loss = train(model, data_generator, optimizer)\n",
    "    if epoch % print_every  == (print_every - 1):\n",
    "        print(\"Epoch:{} {:.2f}s, Loss = {:.4f}\".\n",
    "            format(epoch, time()-t1, loss))\n",
    "    if epoch % eval_every  == (eval_every - 1):\n",
    "        t2 = time()\n",
    "        res = test(model, data_generator)\n",
    "        print(\"VALIDATION.\",\"\\n\"\n",
    "            \"Epoch: {}, {:.2f}s\".format(epoch, time()-t2),\"\\n\",\n",
    "            \"Recall@{}: {:.4f}, Recall@{}: {:.4f}\".format(Ks[0], res['recall'][0],  Ks[-1], res['recall'][-1]), \"\\n\"\n",
    "            \"Precision@{}: {:.4f}, Precision@{}: {:.4f}\".format(Ks[0], res['precision'][0],  Ks[-1], res['precision'][-1]), \"\\n\"\n",
    "            \"Hit_ratio@{}: {:.4f}, Hit_ratio@{}: {:.4f}\".format(Ks[0], res['hit_ratio'][0],  Ks[-1], res['hit_ratio'][-1]), \"\\n\"\n",
    "            \"NDCG@{}: {:.4f}, NDCG@{}: {:.4f}\".format(Ks[0], res['ndcg'][0],  Ks[-1], res['ndcg'][-1])\n",
    "            )\n",
    "        cur_best_pre, stopping_step, should_stop = \\\n",
    "        early_stopping(res['recall'][0], cur_best_pre, stopping_step)\n",
    "    if epoch % save_every == (save_every - 1):\n",
    "        torch.save(model.state_dict(), modelpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you remember, in the notebook `Chapter03_metrics.ipynb` I described another form of testing inspired by the code in [this repo](https://github.com/sh0416/bpr/blob/master/train.py). Let's revisit the code. A full explanation of the code flow is in that notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "def split_mtx(X, n_folds=10):\n",
    "    \"\"\"\n",
    "    Split a matrix/tensor in n_folds folds\n",
    "    \n",
    "    There is some redundancy with the split methods within the \n",
    "    NGCF_BPR class...I am ok with that, or almost.\n",
    "    \"\"\"\n",
    "    X_folds = []\n",
    "    fold_len = X.shape[0]//n_folds\n",
    "    for i in range(n_folds):\n",
    "        start = i * fold_len\n",
    "        if i == n_folds -1:\n",
    "            end = X.shape[0]\n",
    "        else:\n",
    "            end = (i + 1) * fold_len\n",
    "        X_folds.append(X[start:end])\n",
    "    return X_folds\n",
    "\n",
    "# this was named \"precision_and_recall_k\"\n",
    "def test_GPU(user_emb, item_emb, R_tr, R_te, Ks):\n",
    "\n",
    "    tr_folds = split_mtx(R_tr)\n",
    "    te_folds = split_mtx(R_te)\n",
    "    ue_folds = split_mtx(user_emb.weight)\n",
    "\n",
    "    fold_prec, fold_rec = {}, {}\n",
    "    for ue_fold, tr_fold, te_fold in zip(ue_folds, tr_folds, te_folds):\n",
    "\n",
    "        result = torch.sigmoid(torch.mm(ue_fold, item_emb.weight.t()))\n",
    "        test_pred_mask = torch.from_numpy(1 - tr_fold.todense())\n",
    "        test_true_mask = torch.from_numpy(te_fold.todense())\n",
    "        if use_cuda:\n",
    "            test_pred_mask, test_true_mask = test_pred_mask.cuda(), test_true_mask.cuda()\n",
    "        test_pred = test_pred_mask * result\n",
    "        test_true = test_true_mask * result\n",
    "\n",
    "        _, test_indices = torch.topk(test_pred, dim=1, k=max(Ks))\n",
    "        for k in Ks:\n",
    "            topk_mask = torch.zeros_like(test_pred)\n",
    "            source = torch.tensor(1.0).cuda() if use_cuda else torch.tensor(1.0)\n",
    "            topk_mask.scatter_(dim=1, index=test_indices[:, :k], src=source)\n",
    "            test_pred_topk = topk_mask * test_pred\n",
    "            acc_result = (test_pred_topk != 0) & (test_pred_topk == test_true)\n",
    "            pr_k = acc_result.sum().float() / (user_emb.weight.shape[0] * k)\n",
    "            rec_k = (acc_result.float().sum(dim=1) / test_true_mask.float().sum(dim=1))\n",
    "            try:\n",
    "                fold_prec[k].append(pr_k)\n",
    "                fold_rec[k].append(rec_k)\n",
    "            except KeyError:\n",
    "                fold_prec[k] = [pr_k]\n",
    "                fold_rec[k] = [rec_k]\n",
    "\n",
    "    precision, recall = {}, {}\n",
    "    for k in Ks:\n",
    "        precision[k] = np.sum(fold_prec[k])\n",
    "        recall[k] = torch.cat(fold_rec[k]).mean()\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to use it, one would simply replace `test` with `test_GPU` and: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre, rec = test_GPU(\n",
    "    model.g_embeddings_user, \n",
    "    model.g_embeddings_item, \n",
    "    data_generator.Rtr, \n",
    "    data_generator.Rte, \n",
    "    Ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10: 0.0062, 20: 0.0069999998}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
