{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have run 25 experiments. Normally 30 epochs per set up which took around 30h on a p2.xlarge instance in AWS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"../datasets/Amazon/models/\")\n",
    "fname = \"results_df.p\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.read_pickle(DATA_PATH/fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are all experiments I run, order by name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelname</th>\n",
       "      <th>iter_loss</th>\n",
       "      <th>best_hr</th>\n",
       "      <th>best_ndcg</th>\n",
       "      <th>best_iter</th>\n",
       "      <th>train_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GMF_bs_1024_lr_001_n_emb_8_lrs_wolrs.pt</td>\n",
       "      <td>0.062381</td>\n",
       "      <td>0.684681</td>\n",
       "      <td>0.457842</td>\n",
       "      <td>26</td>\n",
       "      <td>77.529552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GMF_bs_512_lr_0001_n_emb_64_lrnr_adam_lrs_wolrs.pt</td>\n",
       "      <td>0.022108</td>\n",
       "      <td>0.429437</td>\n",
       "      <td>0.261455</td>\n",
       "      <td>30</td>\n",
       "      <td>178.078980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GMF_bs_512_lr_001_n_emb_16_lrs_wolrs.pt</td>\n",
       "      <td>0.047171</td>\n",
       "      <td>0.643982</td>\n",
       "      <td>0.426472</td>\n",
       "      <td>26</td>\n",
       "      <td>107.935192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>GMF_bs_512_lr_001_n_emb_64_lrnr_adam_lrs_wolrs.pt</td>\n",
       "      <td>0.029333</td>\n",
       "      <td>0.541425</td>\n",
       "      <td>0.356743</td>\n",
       "      <td>29</td>\n",
       "      <td>181.950120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GMF_bs_512_lr_001_n_emb_8_lrnr_RMSprop_lrs_wlrs.pt</td>\n",
       "      <td>0.137572</td>\n",
       "      <td>0.535116</td>\n",
       "      <td>0.367750</td>\n",
       "      <td>17</td>\n",
       "      <td>104.829468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GMF_bs_512_lr_001_n_emb_8_lrnr_SGD_lrs_wlrs.pt</td>\n",
       "      <td>0.160001</td>\n",
       "      <td>0.103824</td>\n",
       "      <td>0.047371</td>\n",
       "      <td>13</td>\n",
       "      <td>94.832950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GMF_bs_512_lr_001_n_emb_8_lrnr_adam_lrs_wlrs.pt</td>\n",
       "      <td>0.066104</td>\n",
       "      <td>0.687512</td>\n",
       "      <td>0.452048</td>\n",
       "      <td>29</td>\n",
       "      <td>94.790884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GMF_bs_512_lr_001_n_emb_8_lrs_wolrs.pt</td>\n",
       "      <td>0.065170</td>\n",
       "      <td>0.700403</td>\n",
       "      <td>0.464460</td>\n",
       "      <td>29</td>\n",
       "      <td>94.523185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>GMF_bs_512_lr_003_n_emb_16_lrnr_adam_lrs_wolrs.pt</td>\n",
       "      <td>0.106611</td>\n",
       "      <td>0.914795</td>\n",
       "      <td>0.631789</td>\n",
       "      <td>29</td>\n",
       "      <td>103.750941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>GMF_bs_512_lr_003_n_emb_32_lrnr_adam_lrs_wolrs.pt</td>\n",
       "      <td>0.176158</td>\n",
       "      <td>0.999435</td>\n",
       "      <td>0.629910</td>\n",
       "      <td>29</td>\n",
       "      <td>122.771667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>GMF_bs_512_lr_003_n_emb_32_lrnr_adam_lrs_wolrs_BCELoss.pt</td>\n",
       "      <td>0.329084</td>\n",
       "      <td>0.623072</td>\n",
       "      <td>0.394799</td>\n",
       "      <td>29</td>\n",
       "      <td>128.700660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>GMF_bs_512_lr_003_n_emb_64_lrnr_adam_lrs_wlrs.pt</td>\n",
       "      <td>0.141146</td>\n",
       "      <td>0.993296</td>\n",
       "      <td>0.665000</td>\n",
       "      <td>30</td>\n",
       "      <td>187.978307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>GMF_bs_512_lr_003_n_emb_64_lrnr_adam_lrs_wolrs.pt</td>\n",
       "      <td>0.179048</td>\n",
       "      <td>0.999605</td>\n",
       "      <td>0.698754</td>\n",
       "      <td>30</td>\n",
       "      <td>178.450981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>GMF_bs_512_lr_003_n_emb_8_lrnr_adam_lrs_wolrs.pt</td>\n",
       "      <td>0.102676</td>\n",
       "      <td>0.790247</td>\n",
       "      <td>0.584697</td>\n",
       "      <td>30</td>\n",
       "      <td>92.316454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MLP_bs_512_reg_00_lr_001_n_emb_128_ll_64_dp_wodp_lrnr_adam_lrs_wolrs.pt</td>\n",
       "      <td>0.058263</td>\n",
       "      <td>0.942530</td>\n",
       "      <td>0.613962</td>\n",
       "      <td>29</td>\n",
       "      <td>300.465492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MLP_bs_512_reg_00_lr_001_n_emb_16_ll_8_dp_wodp_lrnr_adam_lrs_wolrs.pt</td>\n",
       "      <td>0.068377</td>\n",
       "      <td>0.629977</td>\n",
       "      <td>0.416804</td>\n",
       "      <td>21</td>\n",
       "      <td>116.732269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MLP_bs_512_reg_00_lr_001_n_emb_32_ll_16_dp_wodp_lrnr_adam_lrs_wolrs.pt</td>\n",
       "      <td>0.046923</td>\n",
       "      <td>0.652912</td>\n",
       "      <td>0.438650</td>\n",
       "      <td>24</td>\n",
       "      <td>135.463252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MLP_bs_512_reg_00_lr_001_n_emb_64_ll_32_dp_wodp_lrnr_adam_lrs_wolrs.pt</td>\n",
       "      <td>0.064112</td>\n",
       "      <td>0.653380</td>\n",
       "      <td>0.438573</td>\n",
       "      <td>11</td>\n",
       "      <td>188.649840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MLP_bs_512_reg_00_lr_003_n_emb_128_ll_64_dp_wodp_lrnr_adam_lrs_wolrs.pt</td>\n",
       "      <td>0.143259</td>\n",
       "      <td>0.866852</td>\n",
       "      <td>0.596439</td>\n",
       "      <td>1</td>\n",
       "      <td>292.242160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MLP_bs_512_reg_00_lr_003_n_emb_16_ll_8_dp_wodp_lrnr_adam_lrs_wolrs.pt</td>\n",
       "      <td>0.086542</td>\n",
       "      <td>0.616626</td>\n",
       "      <td>0.454966</td>\n",
       "      <td>22</td>\n",
       "      <td>117.441799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MLP_bs_512_reg_00_lr_003_n_emb_32_ll_16_dp_wodp_lrnr_adam_lrs_wolrs.pt</td>\n",
       "      <td>0.089721</td>\n",
       "      <td>0.598604</td>\n",
       "      <td>0.462805</td>\n",
       "      <td>25</td>\n",
       "      <td>133.081373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>MLP_bs_512_reg_00_lr_003_n_emb_64_ll_32_dp_wodp_lrnr_adam_lrs_wolrs.pt</td>\n",
       "      <td>0.139804</td>\n",
       "      <td>0.999903</td>\n",
       "      <td>0.737139</td>\n",
       "      <td>17</td>\n",
       "      <td>190.503096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NeuMF_wpret_frozen_adam_lrs_wlrs.pt</td>\n",
       "      <td>0.155713</td>\n",
       "      <td>0.996491</td>\n",
       "      <td>0.632440</td>\n",
       "      <td>5</td>\n",
       "      <td>93.412811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NeuMF_wpret_frozen_adam_lrs_wolrs.pt</td>\n",
       "      <td>0.155926</td>\n",
       "      <td>0.996217</td>\n",
       "      <td>0.629047</td>\n",
       "      <td>1</td>\n",
       "      <td>87.346323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NeuMF_wpret_trainable_adam_lrs_wlrs.pt</td>\n",
       "      <td>0.149770</td>\n",
       "      <td>0.998379</td>\n",
       "      <td>0.673731</td>\n",
       "      <td>20</td>\n",
       "      <td>245.364013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  modelname  \\\n",
       "1                                   GMF_bs_1024_lr_001_n_emb_8_lrs_wolrs.pt   \n",
       "6                        GMF_bs_512_lr_0001_n_emb_64_lrnr_adam_lrs_wolrs.pt   \n",
       "2                                   GMF_bs_512_lr_001_n_emb_16_lrs_wolrs.pt   \n",
       "11                        GMF_bs_512_lr_001_n_emb_64_lrnr_adam_lrs_wolrs.pt   \n",
       "4                        GMF_bs_512_lr_001_n_emb_8_lrnr_RMSprop_lrs_wlrs.pt   \n",
       "3                            GMF_bs_512_lr_001_n_emb_8_lrnr_SGD_lrs_wlrs.pt   \n",
       "5                           GMF_bs_512_lr_001_n_emb_8_lrnr_adam_lrs_wlrs.pt   \n",
       "0                                    GMF_bs_512_lr_001_n_emb_8_lrs_wolrs.pt   \n",
       "19                        GMF_bs_512_lr_003_n_emb_16_lrnr_adam_lrs_wolrs.pt   \n",
       "20                        GMF_bs_512_lr_003_n_emb_32_lrnr_adam_lrs_wolrs.pt   \n",
       "21                GMF_bs_512_lr_003_n_emb_32_lrnr_adam_lrs_wolrs_BCELoss.pt   \n",
       "12                         GMF_bs_512_lr_003_n_emb_64_lrnr_adam_lrs_wlrs.pt   \n",
       "13                        GMF_bs_512_lr_003_n_emb_64_lrnr_adam_lrs_wolrs.pt   \n",
       "18                         GMF_bs_512_lr_003_n_emb_8_lrnr_adam_lrs_wolrs.pt   \n",
       "10  MLP_bs_512_reg_00_lr_001_n_emb_128_ll_64_dp_wodp_lrnr_adam_lrs_wolrs.pt   \n",
       "7     MLP_bs_512_reg_00_lr_001_n_emb_16_ll_8_dp_wodp_lrnr_adam_lrs_wolrs.pt   \n",
       "8    MLP_bs_512_reg_00_lr_001_n_emb_32_ll_16_dp_wodp_lrnr_adam_lrs_wolrs.pt   \n",
       "9    MLP_bs_512_reg_00_lr_001_n_emb_64_ll_32_dp_wodp_lrnr_adam_lrs_wolrs.pt   \n",
       "17  MLP_bs_512_reg_00_lr_003_n_emb_128_ll_64_dp_wodp_lrnr_adam_lrs_wolrs.pt   \n",
       "14    MLP_bs_512_reg_00_lr_003_n_emb_16_ll_8_dp_wodp_lrnr_adam_lrs_wolrs.pt   \n",
       "15   MLP_bs_512_reg_00_lr_003_n_emb_32_ll_16_dp_wodp_lrnr_adam_lrs_wolrs.pt   \n",
       "16   MLP_bs_512_reg_00_lr_003_n_emb_64_ll_32_dp_wodp_lrnr_adam_lrs_wolrs.pt   \n",
       "24                                      NeuMF_wpret_frozen_adam_lrs_wlrs.pt   \n",
       "23                                     NeuMF_wpret_frozen_adam_lrs_wolrs.pt   \n",
       "22                                   NeuMF_wpret_trainable_adam_lrs_wlrs.pt   \n",
       "\n",
       "    iter_loss   best_hr  best_ndcg best_iter  train_time  \n",
       "1    0.062381  0.684681   0.457842        26   77.529552  \n",
       "6    0.022108  0.429437   0.261455        30  178.078980  \n",
       "2    0.047171  0.643982   0.426472        26  107.935192  \n",
       "11   0.029333  0.541425   0.356743        29  181.950120  \n",
       "4    0.137572  0.535116   0.367750        17  104.829468  \n",
       "3    0.160001  0.103824   0.047371        13   94.832950  \n",
       "5    0.066104  0.687512   0.452048        29   94.790884  \n",
       "0    0.065170  0.700403   0.464460        29   94.523185  \n",
       "19   0.106611  0.914795   0.631789        29  103.750941  \n",
       "20   0.176158  0.999435   0.629910        29  122.771667  \n",
       "21   0.329084  0.623072   0.394799        29  128.700660  \n",
       "12   0.141146  0.993296   0.665000        30  187.978307  \n",
       "13   0.179048  0.999605   0.698754        30  178.450981  \n",
       "18   0.102676  0.790247   0.584697        30   92.316454  \n",
       "10   0.058263  0.942530   0.613962        29  300.465492  \n",
       "7    0.068377  0.629977   0.416804        21  116.732269  \n",
       "8    0.046923  0.652912   0.438650        24  135.463252  \n",
       "9    0.064112  0.653380   0.438573        11  188.649840  \n",
       "17   0.143259  0.866852   0.596439         1  292.242160  \n",
       "14   0.086542  0.616626   0.454966        22  117.441799  \n",
       "15   0.089721  0.598604   0.462805        25  133.081373  \n",
       "16   0.139804  0.999903   0.737139        17  190.503096  \n",
       "24   0.155713  0.996491   0.632440         5   93.412811  \n",
       "23   0.155926  0.996217   0.629047         1   87.346323  \n",
       "22   0.149770  0.998379   0.673731        20  245.364013  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.sort_values('modelname')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way one should read the `modelname` is, for example:\n",
    "    \n",
    "    MLP_bs_512_reg_00_lr_001_n_emb_128_ll_64_dp_wodp_lrnr_adam_lrs_wolrs.pt\n",
    "    \n",
    "This is the MLP model, with batch size of 512, no regularization, learning rate 0.01, using embeddings of dimension 128, the last layer has 64 units, no dropout, Adam optimizer and no learning rate scheduler. \n",
    "\n",
    "Let's start by having a look to the GMF results:\n",
    "\n",
    "### GMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results[df_results.modelname.str.contains('GMF')].sort_values('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelname</th>\n",
       "      <th>iter_loss</th>\n",
       "      <th>best_hr</th>\n",
       "      <th>best_ndcg</th>\n",
       "      <th>best_iter</th>\n",
       "      <th>train_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GMF_bs_512_lr_003_n_emb_64_lrnr_adam_lrs_wolrs.pt</td>\n",
       "      <td>0.179048</td>\n",
       "      <td>0.999605</td>\n",
       "      <td>0.698754</td>\n",
       "      <td>30</td>\n",
       "      <td>178.450981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GMF_bs_512_lr_003_n_emb_32_lrnr_adam_lrs_wolrs.pt</td>\n",
       "      <td>0.176158</td>\n",
       "      <td>0.999435</td>\n",
       "      <td>0.629910</td>\n",
       "      <td>29</td>\n",
       "      <td>122.771667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GMF_bs_512_lr_003_n_emb_64_lrnr_adam_lrs_wlrs.pt</td>\n",
       "      <td>0.141146</td>\n",
       "      <td>0.993296</td>\n",
       "      <td>0.665000</td>\n",
       "      <td>30</td>\n",
       "      <td>187.978307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GMF_bs_512_lr_003_n_emb_16_lrnr_adam_lrs_wolrs.pt</td>\n",
       "      <td>0.106611</td>\n",
       "      <td>0.914795</td>\n",
       "      <td>0.631789</td>\n",
       "      <td>29</td>\n",
       "      <td>103.750941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GMF_bs_512_lr_003_n_emb_8_lrnr_adam_lrs_wolrs.pt</td>\n",
       "      <td>0.102676</td>\n",
       "      <td>0.790247</td>\n",
       "      <td>0.584697</td>\n",
       "      <td>30</td>\n",
       "      <td>92.316454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GMF_bs_512_lr_001_n_emb_8_lrs_wolrs.pt</td>\n",
       "      <td>0.065170</td>\n",
       "      <td>0.700403</td>\n",
       "      <td>0.464460</td>\n",
       "      <td>29</td>\n",
       "      <td>94.523185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GMF_bs_512_lr_001_n_emb_8_lrnr_adam_lrs_wlrs.pt</td>\n",
       "      <td>0.066104</td>\n",
       "      <td>0.687512</td>\n",
       "      <td>0.452048</td>\n",
       "      <td>29</td>\n",
       "      <td>94.790884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GMF_bs_1024_lr_001_n_emb_8_lrs_wolrs.pt</td>\n",
       "      <td>0.062381</td>\n",
       "      <td>0.684681</td>\n",
       "      <td>0.457842</td>\n",
       "      <td>26</td>\n",
       "      <td>77.529552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GMF_bs_512_lr_001_n_emb_16_lrs_wolrs.pt</td>\n",
       "      <td>0.047171</td>\n",
       "      <td>0.643982</td>\n",
       "      <td>0.426472</td>\n",
       "      <td>26</td>\n",
       "      <td>107.935192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GMF_bs_512_lr_003_n_emb_32_lrnr_adam_lrs_wolrs_BCELoss.pt</td>\n",
       "      <td>0.329084</td>\n",
       "      <td>0.623072</td>\n",
       "      <td>0.394799</td>\n",
       "      <td>29</td>\n",
       "      <td>128.700660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GMF_bs_512_lr_001_n_emb_64_lrnr_adam_lrs_wolrs.pt</td>\n",
       "      <td>0.029333</td>\n",
       "      <td>0.541425</td>\n",
       "      <td>0.356743</td>\n",
       "      <td>29</td>\n",
       "      <td>181.950120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>GMF_bs_512_lr_001_n_emb_8_lrnr_RMSprop_lrs_wlrs.pt</td>\n",
       "      <td>0.137572</td>\n",
       "      <td>0.535116</td>\n",
       "      <td>0.367750</td>\n",
       "      <td>17</td>\n",
       "      <td>104.829468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>GMF_bs_512_lr_0001_n_emb_64_lrnr_adam_lrs_wolrs.pt</td>\n",
       "      <td>0.022108</td>\n",
       "      <td>0.429437</td>\n",
       "      <td>0.261455</td>\n",
       "      <td>30</td>\n",
       "      <td>178.078980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>GMF_bs_512_lr_001_n_emb_8_lrnr_SGD_lrs_wlrs.pt</td>\n",
       "      <td>0.160001</td>\n",
       "      <td>0.103824</td>\n",
       "      <td>0.047371</td>\n",
       "      <td>13</td>\n",
       "      <td>94.832950</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    modelname  iter_loss  \\\n",
       "0           GMF_bs_512_lr_003_n_emb_64_lrnr_adam_lrs_wolrs.pt   0.179048   \n",
       "1           GMF_bs_512_lr_003_n_emb_32_lrnr_adam_lrs_wolrs.pt   0.176158   \n",
       "2            GMF_bs_512_lr_003_n_emb_64_lrnr_adam_lrs_wlrs.pt   0.141146   \n",
       "3           GMF_bs_512_lr_003_n_emb_16_lrnr_adam_lrs_wolrs.pt   0.106611   \n",
       "4            GMF_bs_512_lr_003_n_emb_8_lrnr_adam_lrs_wolrs.pt   0.102676   \n",
       "5                      GMF_bs_512_lr_001_n_emb_8_lrs_wolrs.pt   0.065170   \n",
       "6             GMF_bs_512_lr_001_n_emb_8_lrnr_adam_lrs_wlrs.pt   0.066104   \n",
       "7                     GMF_bs_1024_lr_001_n_emb_8_lrs_wolrs.pt   0.062381   \n",
       "8                     GMF_bs_512_lr_001_n_emb_16_lrs_wolrs.pt   0.047171   \n",
       "9   GMF_bs_512_lr_003_n_emb_32_lrnr_adam_lrs_wolrs_BCELoss.pt   0.329084   \n",
       "10          GMF_bs_512_lr_001_n_emb_64_lrnr_adam_lrs_wolrs.pt   0.029333   \n",
       "11         GMF_bs_512_lr_001_n_emb_8_lrnr_RMSprop_lrs_wlrs.pt   0.137572   \n",
       "12         GMF_bs_512_lr_0001_n_emb_64_lrnr_adam_lrs_wolrs.pt   0.022108   \n",
       "13             GMF_bs_512_lr_001_n_emb_8_lrnr_SGD_lrs_wlrs.pt   0.160001   \n",
       "\n",
       "     best_hr  best_ndcg best_iter  train_time  \n",
       "0   0.999605   0.698754        30  178.450981  \n",
       "1   0.999435   0.629910        29  122.771667  \n",
       "2   0.993296   0.665000        30  187.978307  \n",
       "3   0.914795   0.631789        29  103.750941  \n",
       "4   0.790247   0.584697        30   92.316454  \n",
       "5   0.700403   0.464460        29   94.523185  \n",
       "6   0.687512   0.452048        29   94.790884  \n",
       "7   0.684681   0.457842        26   77.529552  \n",
       "8   0.643982   0.426472        26  107.935192  \n",
       "9   0.623072   0.394799        29  128.700660  \n",
       "10  0.541425   0.356743        29  181.950120  \n",
       "11  0.535116   0.367750        17  104.829468  \n",
       "12  0.429437   0.261455        30  178.078980  \n",
       "13  0.103824   0.047371        13   94.832950  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_results[df_results.modelname.str.contains('GMF')]\n",
    "     .sort_values('best_hr', ascending=False)\n",
    "     .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of conclusion drawn from that table:\n",
    "\n",
    "1. The more embeddings (n_emb) the better.\n",
    "\n",
    "    * As we can see in the table, the best results are achieved when n_emb equals 64.\n",
    "\n",
    "\n",
    "2. Learning rate scheduling does not seem to help much.\n",
    "\n",
    "    * these days most DL problems benefit from using some form of learning rate scheduling. In particular, [Cyclic Learning Rates](https://arxiv.org/pdf/1506.01186.pdf) seem to lead to the best results. In this particular exercise, this does not seem to be the case.\n",
    "\n",
    "        \n",
    "3. This problem allows for high (0.03) learning rates, at least compared to what I am used to. \n",
    "\n",
    "    * The most significant jump in both HR@10 and NDCG@10 happens when incresing the learning rate from 0.01 to 0.03\n",
    "\n",
    "\n",
    "4. When building recommendation algorithms one should be careful when selecting the measure of success.\n",
    "\n",
    "    * This is perhaps the most important aspect of all. And deserves a bit more attent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = df_gmf.modelname\n",
    "modelname = [re.sub(\".h5|.pt|.params|_reg_00|_loss_mse\", \"\", n) for n in modelname]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GMF_bs_512_lr_001_n_emb_8_lrs_wolrs',\n",
       " 'GMF_bs_1024_lr_001_n_emb_8_lrs_wolrs',\n",
       " 'GMF_bs_512_lr_001_n_emb_16_lrs_wolrs',\n",
       " 'GMF_bs_512_lr_001_n_emb_8_lrnr_SGD_lrs_wlrs',\n",
       " 'GMF_bs_512_lr_001_n_emb_8_lrnr_RMSprop_lrs_wlrs',\n",
       " 'GMF_bs_512_lr_001_n_emb_8_lrnr_adam_lrs_wlrs',\n",
       " 'GMF_bs_512_lr_0001_n_emb_64_lrnr_adam_lrs_wolrs',\n",
       " 'GMF_bs_512_lr_001_n_emb_64_lrnr_adam_lrs_wolrs',\n",
       " 'GMF_bs_512_lr_003_n_emb_64_lrnr_adam_lrs_wlrs',\n",
       " 'GMF_bs_512_lr_003_n_emb_64_lrnr_adam_lrs_wolrs',\n",
       " 'GMF_bs_512_lr_003_n_emb_8_lrnr_adam_lrs_wolrs',\n",
       " 'GMF_bs_512_lr_003_n_emb_16_lrnr_adam_lrs_wolrs',\n",
       " 'GMF_bs_512_lr_003_n_emb_32_lrnr_adam_lrs_wolrs',\n",
       " 'GMF_bs_512_lr_003_n_emb_32_lrnr_adam_lrs_wolrs_BCELoss']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_jrz)",
   "language": "python",
   "name": "conda_jrz"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
