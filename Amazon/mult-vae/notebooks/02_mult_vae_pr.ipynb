{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Variation Autoencoder for Collaborative Filtering\n",
    "\n",
    "I must admit that when it comes to variational autoencoders I find that there is a \"*notable*\" difference between the complexity of the math and that of the code (or maybe is just me that I am not a mathematician). Nonetheless, I think that speaking about VAEs and not discussing log likelihoods, Evidence Lower Bound (EBLO) and some other mathematical terms is almost like \"cheating\". With that in mind I will try to give some mathematical context to the Multinomial VAE (Mult-VAE) for collaborative filtering and then move to the code. Bear in mind that the whole purpose of the math below is to \"smoothly\" get to the loss function we will be using for this exercise. \n",
    "\n",
    "The references I used for the following writing are: \n",
    "\n",
    "* [Auto-Encoding Variational Bayes](https://arxiv.org/pdf/1312.6114.pdf): a must-read paper to understand variational autoencoders.\n",
    "* [Variational Autoencoders for Collaborative Filtering](https://arxiv.org/pdf/1802.05814.pdf): the paper on which this whole repo is based.\n",
    "* [Variational Autoencoders with Gluon](https://gluon.mxnet.io/chapter13_unsupervised-learning/vae-gluon.html): one of the reason why like [Gluon](https://mxnet.incubator.apache.org/api/python/docs/api/gluon/index.html) is because some of the tutorials they have on their site are just amazing. This is one example.\n",
    "* [EM Demystified: An Expectation-Maximization Tutorial](https://vannevar.ece.uw.edu/techsite/papers/documents/UWEETR-2010-0002.pdf): one of the references within Gluon's tutorial is this tutorial, which I find dense, but useful when it comes to understanding some of the concepts I will discuss below.\n",
    "\n",
    "Overall, I find these 4 references fantastic and I strongly encourage everyone that is reading these lines and is interested in variational autoencoders to give them a read. In fact, the writing here is mostly based in the Gluon tutorial, adapted to the case of VAEs for collaborative filtering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The EM algorithm\n",
    "\n",
    "The problem we face here is trying to find out the likelihood, or better, the log-likelihood $\\ell(\\theta)$ of a given dataset of, for example, user clicks. The way I normally think of this is that we are trying to find the underlying probability that a dataset exists, so that we can then do inference with it. A standard way of doing this is using an Expectation-Maximization (EM) algorithm. When using EM, instead of using directly the data $x$ (clicks in this example) to maximize $\\ell$, we use a latent representation of $x$, $z$, to maximize a lower bound likelihood $\\mathcal{L}(q,\\theta)$, where $q$ is any valid probability distribution and $\\theta$ are the parameters of the log-likelihood function $\\ell(\\theta) = \\sum_i \\log\\left( p_{\\theta}(x_i) \\right)$. This can be formulated as follows: \n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "\\ell(\\theta) &= \\sum_i \\log\\left( p_{\\theta}(x_i) \\right) \\\\\n",
    "& = \\sum_i \\log\\left( \\int p_{\\theta}(x_i, z) dz \\right)\\\\\n",
    "&= \\sum_i \\log\\left( \\mathbb{E}_{z \\sim Q} \\left[ \\frac {p_{\\theta}(x_i, z)}{q(z)} \\right]\\right) \\\\ \n",
    "& {\\ge}\\underbrace{ \\sum_i \\mathbb{E}_{z \\sim Q} \\left[\\log\\left( \\frac {p_{\\theta}(x_i,z)}{q(z)} \\right)\\right]}_{ELBO: \\mathcal{L}(q,\\theta)} \\hspace{5cm} (1)\n",
    "\\end{split} \n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "Please go [here](https://gluon.mxnet.io/chapter13_unsupervised-learning/vae-gluon.html) and references therein, in particular equation 4.1 of the [tutorial](https://vannevar.ece.uw.edu/techsite/papers/documents/UWEETR-2010-0002.pdf) mentioned before, for a proper derivation of Eq (1), which derives from Jensen’s inequality and holds for any $q(z)$ as long as it is a valid probability distribution.\n",
    "\n",
    "Looking at Eq (1) we can see that our job is basically to maximize ELBO $\\mathcal{L}(q,\\theta)$ so we maximize $\\ell(\\theta)$. To that aim, we **need** to chose $q(z)$ at a given iteration so that is the inferred posterior $p(z\\vert x)$, i.e. at $t$-th iteration, we chose $q$ so that:  \n",
    "\n",
    "$$\n",
    "q^t(z) = p(z\\vert x_i; \\hat\\theta^{t-1}) = \\frac{p(x_i\\vert z; \\hat\\theta^{t-1})p(z; \\hat\\theta^{t-1})}{\\int p(x_i\\vert z; \\hat\\theta^{t-1})p(z; \\hat\\theta^{t-1}) dz} \\hspace{5cm} (2)\n",
    "$$\n",
    "\n",
    "Choosing that $q$ is basically the essence of the E-step. Then, in the M-step we maximize over the parameters $\\theta$. \n",
    "\n",
    "In words, the chain of improvements through E-step and M-step can be described as follows: during the E-step we chose $q$ so that **is** the inferred posterior, i.e. Eq (1) becomes an equality. Then, during the M-step we maximize over $\\theta$ so that we ensure that $\\mathcal L(q^t,\\theta^{t-1}) \\le \\mathcal L(q^t,\\theta^t)$. Finally, Jensen's inequality ensures that $ \\mathcal L(q^t,\\theta^t) \\le \\ell(\\theta^{t}) $. \n",
    "\n",
    "Altogether:\n",
    "\n",
    "$$\n",
    "\\ell(\\theta^{t-1}) \\underset{E-step}{=} \\mathcal L(q^t,\\theta^{t-1}) \\underset{M-step}{\\le} \\mathcal L(q^t,\\theta^t) \\underset{Jensen}{\\le} \\ell(\\theta^{t}) \\hspace{5cm} (3)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Autoencoders\n",
    "\n",
    "As mentioned before, to maximize ELBO $\\mathcal{L}(q,\\theta)$ we need to chose $q$ so that is the inferred posterior at $t$-th iteration during the E-step. However, for complex distribution $p_{\\theta}(x|z)$, the computation of the posterior $p_{\\theta}(z|x)$ is intractable. To solve this problem we can use variational inference methods. In particular, [Kingma and Welling 2014](https://arxiv.org/pdf/1312.6114.pdf) introduced a \"simple\" neural-network approach, *Auto-Encoding Variational Bayes*, where the variational inference/optimization task of finding the optimal $q$ becomes a matter of finding the best parameters of a neural network via backpropagation and stochastic gradient descent. \n",
    "\n",
    "Because from now on I want to adapt the text to the Mult-VAE encoder, let me add here a couple of lines about notation. Following [Liang et al., 2018]() I will use $u \\in \\{1,\\dots,U\\}$ to index users and $i \\in \\{1,\\dots,I\\}$ to index items. The user-by-item **binary** interaction matrix (i.e. the click matrix) is $\\mathbf{X} \\in \\mathbb{N}^{U\\times I}$ and I will use lower case $\\mathbf{x}_u =[X_{u1},\\dots,X_{uI}]^\\top \\in \\mathbb{N}^I$ to refer to the click history of an individual user $u$.\n",
    "\n",
    "Moving on and quoting **directly** the [Gluon tutorial](https://gluon.mxnet.io/chapter13_unsupervised-learning/vae-gluon.html), this is how Auto-Encoding Variational Bayes works:\n",
    "\n",
    "1. Select a prior for latent representation of $\\textbf{x}_u$, $p_{\\theta}(\\textbf{z}_u)$\n",
    "2. Use a neural network to parameterize the distribution $p_{\\theta}(\\textbf{x}_u\\vert \\textbf{z}_u)$. Because this part of the model maps the latent variable/representation $\\textbf{z}_u$ to the observed data $\\textbf{x}_u$, it is referred as a \"*decoder*\" network. \n",
    "3. Rather than explicitly calculating the intractable posterior $p_{\\theta}(\\textbf{z}_u\\vert \\textbf{x}_u)$, we use another another neural network to parameterize the distribution $q_\\phi(\\textbf{z}_u\\vert \\textbf{x}_u)$ as the approximate posterior. Since $q_\\phi$ maps the observed data $\\textbf{x}_u$ to the latent space of $\\textbf{z}_u$'s, is referred as the \"*encoder*\" network.\n",
    "4. The objective is still to maxmize ELBO $\\mathcal{L}(q,\\theta)$. But now instead of separately finding the optimal $\\phi$ (this would be equivalent to chosing $q$ in EM) and $\\theta$ like EM, we can find the parameters $\\theta$ and $\\phi$ jointly via standard stochastic gradient descent.\n",
    "\n",
    "Since we have an encoder-decoder structure, we refer to this as variational auto-encoder (VAE).\n",
    "\n",
    "Now, given the distribution $p_{\\theta}(\\textbf{x}_u\\vert \\textbf{z}_u)$ and the approximate posterior $q_\\phi(\\textbf{z}_u\\vert \\textbf{x}_u)$, we can re-write the ELBO $\\mathcal{L}(q,\\theta)$ in Eq (1) in terms of $\\theta$ and $\\phi$ as: \n",
    "\n",
    "$$\n",
    "\\begin{split}\\begin{equation*}\n",
    "\\begin{split}\n",
    "- \\mathcal L(\\textbf{x}_u, \\phi,\\theta) & = - \\mathbb{E}_{\\textbf{z}_u \\sim Q_\\phi(\\textbf{z}_u|\\textbf{x}_u)} \\left[\\log p_{\\theta}(\\textbf{x}_u \\vert \\textbf{z}_u) + \\log p_\\theta(\\textbf{z}_u) - \\log q_\\phi(\\textbf{z}_u\\vert \\textbf{x}_u) \\right] \\\\\n",
    "& = - \\mathbb{E}_{\\textbf{z}_u \\sim Q_\\phi(\\textbf{z}_u|\\textbf{x}_u)} \\left[\\log p_{\\theta}(\\textbf{x}_u \\vert \\textbf{z}_u) \\right] + D_{KL}\\left[\\log q_\\phi(\\textbf{z}_u\\vert \\textbf{x}_u) \\| p_\\theta(\\textbf{z}_u)\\right] \\hspace{5cm} (4) \n",
    "\\end{split}\n",
    "\\end{equation*}\\end{split}\n",
    "$$\n",
    "\n",
    "Where $D_{KL}$ is the [Kullback–Leibler divergence](). See [this wikipedia page]() to understand how the expansion of $p(x,z)$ allows us to re-write the equation in that form. Also, and more importantly, note that maximizing $\\mathcal L(\\textbf{x}_u, \\phi,\\theta)$ involves minimizing $D_{KL}$, which makes sense, since $D_{KL}$ measures the dissimilarity of the approximate posterior  $q_\\phi(\\textbf{z}_u\\vert \\textbf{x}_u)$ from the true posterior $p_{\\theta}(\\textbf{z}_u\\vert \\textbf{x}_u)$.\n",
    "\n",
    "Coming back to the paper [Auto-Encoding Variational Bayes](), the authors described there what is often referred as the **Gaussian VAE**, where we have a Gaussian prior $p(\\textbf{z}_u) \\sim \\mathcal N(0, I)$ and the approximate posterior is also assumed to be Gaussian $q_\\phi(\\textbf{z}_u\\vert \\textbf{x}_u) \\sim \\mathcal N(\\mu_\\phi(\\textbf{x}_u), \\sigma_\\phi(\\textbf{x}_u) I)$, where $\\mu_\\phi(\\textbf{x}_u)$ and  $\\sigma_\\phi(\\textbf{x}_u)$ are the outputs of some neural network, for example, the outputs of a MLP. With this set up, Kingma and Welling show that the Gaussian VAE can be trained by minimizing the negative ELBO and Eq (4) can be written as:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "- \\mathcal L(\\textbf{x}_u, \\phi,\\theta) \\approx \\frac{1}{L} \\sum_s^L \\left[-\\log p_{\\theta}(\\textbf{x}_u \\vert \\textbf{z}_{us}) \\right] + D_{KL}\\left[\\log q_\\phi(\\textbf{z}_u\\vert \\textbf{x}_u) \\| p_\\theta(\\textbf{z}_u)\\right] \\hspace{5cm} (5)\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "The **Mult-VAE** uses a very similar that set up. As before, for a user $u$, the latent representation $\\textbf{z}_u$ is assumed to be drawn from a standard Gaussian prior $p(\\textbf{z}_u) \\sim \\mathcal N(0, I)$. Such representation is then transformed by a non-linear function $f_{\\theta}(.)$ (e.g. a MLP) and the output is normalized via a Softmax function to produce a probability distribution over all items **$I$**, $\\pi(\\textbf{z}_u)$. If we then defined the total number of clicks as $N_u = \\sum_i x_{ui}$, **the click history of user $u$ is assumed to be drawn from a Multinomial distribution with parameters**: \n",
    "\n",
    "$$\n",
    "\\textbf{x}_u \\sim \\text{Mult}(N_u, \\pi(\\textbf{z}_u)) \\hspace{5cm} (6)\n",
    "$$\n",
    "\n",
    "Under this assumptions:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\log(p_{\\theta}(\\textbf{x}_u\\vert \\textbf{z}_u)) = \\sum_i x_{ui} \\log \\pi_i(\\textbf{z}_{u}) \\hspace{5cm} (7)\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "\n",
    "and therefore, \"abusing notation\" a bit, Eq (5) can be written as:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "- \\mathcal L(\\textbf{x}_u, \\phi,\\theta) \\approx - \\frac{1}{L} \\sum_s^L \\sum_i x_{ui} \\left[ \\log \\pi_i(\\textbf{z}_{us})  \\right] + D_{KL}\\left[\\log q_\\phi(\\textbf{z}_u\\vert \\textbf{x}_u) \\| p_\\theta(\\textbf{z}_u)\\right] \\hspace{5cm} (8)\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "Note that Eq (8) involves sampling $\\textbf{z}_{us} \\sim q_\\phi(\\textbf{z}_u\\vert \\textbf{x}_u)$. When sampling is involved, backpropagation is not trivial (how one would take gradients with respect to $\\phi$?). This was solved by Kingma and Welling by introducing the reparameterization trick. If we remember: $q_\\phi(\\textbf{z}_u\\vert \\textbf{x}_u) \\sim \\mathcal N(\\mu_\\phi(\\textbf{x}_u), \\sigma_\\phi(\\textbf{x}_u) I)$. Therefore, Instead of sampling $\\textbf{z}_u$ from that distribution, we construct $\\textbf{z}_u = \\mu(\\textbf{x}_u) + \\sigma(\\textbf{x}_u) \\cdot \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,I)$, and we sample directly $\\epsilon$. This way $\\textbf{z}_u$ depends deterministically on $\\mu(\\textbf{x}_u)$ and  $\\sigma(\\textbf{x}_u)$ and we can compute gradients. \n",
    "\n",
    "In summary, given the Gaussian prior and approximate posterior discussed above, the training loss we need to compute after the reparameterization trick for the Mult-VAE is:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\mathcal L(\\textbf{x}_u, \\phi,\\theta) = - \\frac{1}{N_u}\\sum_i x_{ui} \\left[ \\textit{log_softmax}(\\textbf{z}_u) \\right] + \\frac{1}{2N_u}\\left[-\\sum_i\\left(\\log\\sigma_i^2 + 1\\right) + \\sum_i\\sigma_i^2 + \\sum_i\\mu^2_i\\right] \\hspace{5cm} (9)\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "If you want to have a look to math behind the $D_{KL}$ expression, see [here]().\n",
    "\n",
    "Before we move to the code there is one, and just one more aspect we need to discuss. In their paper, Liang et al. propose an alternative interpretation of ELBO. The authors interpret the first term in Eq (9) as the reconstruction loss, while the second can be viewed as regularization. Quoting directly from the authors: \"*It is this perspective we work with because it allows us to make a trade-off that forms the crux of our method. From this perspective, it is natural to extend the ELBO by introducing a parameter $\\beta$ to control the strength of\n",
    "regularization*\". With this formulation Eq (9) simply turns into:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\mathcal L(\\textbf{x}_u, \\phi,\\theta) = - \\frac{1}{N_u}\\sum_i x_{ui} \\left[ \\textit{log_softmax}(\\textbf{z}_u) \\right] + \\beta \\frac{1}{2N_u}\\left[-\\sum_i\\left(\\log\\sigma_i^2 + 1\\right) + \\sum_i\\sigma_i^2 + \\sum_i\\mu^2_i\\right] \\hspace{5cm} (10)\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "The authors proposed to use $\\beta < 1$. If you remember, I previously mentioned that maximizing the likelihood involved minimizing $D_{KL}$. Since $D_{KL}$ measures the dissimilarity of the approximate posterior and the true posterior, by using $\\beta < 1$ we are less able to predict \"accurately\" future user histories from historical data. In other words, we are potentially harming the loss. However, as I have discussed in some other posts/notebooks, when we build recommendation systems we are often not interested in achieving the best loss, but the best ranking metric. With that in mind, the authors showed that using $\\beta < 1$ leads to better Normalised Discounted Cumulative Gain (NDCG) and Recall (R) metrics. They call this *partially regularized multinomial variational autoencoder* or $\\text{Mult-VAE}^{\\text{PR}}$\n",
    "\n",
    "And...\"*that's it!*\". After all this we are ready to move to the code. There are two comments to make as we move to the code. I have implemented the Mult-VAE using `Mxnet` and `Pytorch`. Given the fact that these two implementations are nearly identical, I will concentrate here only on the `Mxnet` implementation, please go to the repo if you want to have a look to the `Pytorch` one. Secondly, the paper also includes a Multinomial Denoising Autoencoder, referred as Mult-DAE. However, given its simplicity relative to the formulation of the Mult-VAE discussed here I will not discussed the math of the Mult-DAE and focus simply on the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\text{Mult-VAE}^{\\text{PR}}$, the code\n",
    "\n",
    "After the explanation in the section above you might expect the code to look rather complex. However, you might feel disappointed/pleased when you see how simple it really is. \n",
    "\n",
    "In the [original publications](https://arxiv.org/pdf/1802.05814.pdf) the authors used a one hidden layer MLP as generative model. There they say that deeper architectures do not improve the results. In notebook 04 I will show the results and we will see that they are actually right. I have run over a hundred experiments and I find that the set up described in the paper leads to the best results. With that it mind, let's first have a look the model $ I \\rightarrow 600 \\rightarrow 200 \\rightarrow 600 \\rightarrow I$, where $I$ is the total number of items: \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"800\" src=\"figures/multvae_arch.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fig1. \n",
    "\n",
    "In code, the model in the figure above is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "from mxnet import autograd\n",
    "from mxnet.gluon import nn, HybridBlock\n",
    "\n",
    "class VAEEncoder(HybridBlock):\n",
    "    def __init__(self, q_dims: List[int], dropout: List[float]):\n",
    "        super().__init__()\n",
    "\n",
    "        # last dim multiplied by two for the reparameterization trick\n",
    "        q_dims_ = q_dims[:-1] + [q_dims[-1] * 2]\n",
    "        with self.name_scope():\n",
    "            self.q_layers = nn.HybridSequential(prefix=\"q_net\")\n",
    "            for p, inp, out in zip(dropout, q_dims_[:-1], q_dims_[1:]):\n",
    "                self.q_layers.add(nn.Dropout(p))\n",
    "                self.q_layers.add(nn.Dense(in_units=inp, units=out))\n",
    "\n",
    "    def hybrid_forward(self, F, X):\n",
    "        h = F.L2Normalization(X)\n",
    "        for i, layer in enumerate(self.q_layers):\n",
    "            h = layer(h)\n",
    "            if i != len(self.q_layers) - 1:\n",
    "                h = F.tanh(h)\n",
    "            else:\n",
    "                mu, logvar = F.split(h, axis=1, num_outputs=2)\n",
    "        return mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before I move on, let me mention (and appreciate) one of the many nice \"little\" things that `Mxnet`'s `Gluon` has to offer. You will notice the use of `HybridBlock` and the use of the input `F` (the backend) when we define the forward pass, or more precisely, the `hybrid_forward` pass. One could write a full post on the joys of `HybridBlocks` and how nicely and easily the guys that developed `Gluon` brought together the flexibility of imperative frameworks (i.e. `Pytorch`) and the speed of declarative frameworks (i.e. `Tensorflow`) together. If you want to learn the details go [here](https://gluon.mxnet.io/chapter07_distributed-learning/hybridize.html), but believe me, this is **FAST**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(HybridBlock):\n",
    "    def __init__(self, p_dims: List[int], dropout: List[float]):\n",
    "        super().__init__()\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.p_layers = nn.HybridSequential(prefix=\"p_net\")\n",
    "            for p, inp, out in zip(dropout, p_dims[:-1], p_dims[1:]):\n",
    "                self.p_layers.add(nn.Dropout(p))\n",
    "                self.p_layers.add(nn.Dense(in_units=inp, units=out))\n",
    "\n",
    "    def hybrid_forward(self, F, X):\n",
    "        h = X\n",
    "        for i, layer in enumerate(self.p_layers):\n",
    "            h = layer(h)\n",
    "            if i != len(self.p_layers) - 1:\n",
    "                h = F.tanh(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiVAE(HybridBlock):\n",
    "    def __init__(\n",
    "        self,\n",
    "        p_dims: List[int],\n",
    "        dropout_enc: List[float],\n",
    "        dropout_dec: List[float],\n",
    "        q_dims: List[int] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encode = VAEEncoder(q_dims, dropout_enc)\n",
    "        self.decode = Decoder(p_dims, dropout_dec)\n",
    "\n",
    "    def hybrid_forward(self, F, X):\n",
    "        mu, logvar = self.encode(X)\n",
    "        if autograd.is_training():\n",
    "            std = F.exp(0.5 * logvar)\n",
    "            eps = F.random.normal_like(std)\n",
    "            mu = (eps * std) + mu\n",
    "        return self.decode(mu), mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and, that's it...wait! We need the loss. Let me bring Eq 10 again\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\mathcal L(\\textbf{x}_u, \\phi,\\theta) = - \\frac{1}{N_u}\\sum_i x_{ui} \\left[ \\textit{log_softmax}(\\textbf{z}_u) \\right] + \\beta \\frac{1}{2N_u}\\left[-\\sum_i\\left(\\log\\sigma_i^2 + 1\\right) + \\sum_i\\sigma_i^2 + \\sum_i\\mu^2_i\\right] \\hspace{5cm} (10)\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "In code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss_fn(inp, out, mu, logvar, anneal):\n",
    "    # firt term\n",
    "    neg_ll = -nd.mean(nd.sum(nd.log_softmax(out) * inp, -1))\n",
    "    # second term without beta\n",
    "    KLD = -0.5 * nd.mean(nd.sum(1 + logvar - nd.power(mu, 2) - nd.exp(logvar), axis=1))\n",
    "    # \"full\" loss (anneal is beta)\n",
    "    return neg_ll + anneal * KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I mentioned before, in the paper the authors also use a Multinomial Denoising Autoencoder (Mult-DAE}. The architecture is identical to that of the $\\text{Mult-VAE}^{\\text{PR}}$ apart from the fact that there is no variational aspect here. Therefore, not $D_{KL}$ regularization or reparameterization trick. The `Decoder` is identical to that shown above. The Encoder is as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAEEncoder(HybridBlock):\n",
    "    def __init__(self, q_dims: List[int], dropout: List[float]):\n",
    "        super().__init__()\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.q_layers = nn.HybridSequential(prefix=\"q_net\")\n",
    "            for p, inp, out in zip(dropout, q_dims[:-1], q_dims[1:]):\n",
    "                self.q_layers.add(nn.Dropout(p))\n",
    "                self.q_layers.add(nn.Dense(in_units=inp, units=out))\n",
    "\n",
    "    def hybrid_forward(self, F, X):\n",
    "        h = F.L2Normalization(X)\n",
    "        for layer in self.q_layers:\n",
    "            h = F.tanh(layer(h))\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the model itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiDAE(HybridBlock):\n",
    "    def __init__(\n",
    "        self,\n",
    "        p_dims: List[int],\n",
    "        dropout_enc: List[float],\n",
    "        dropout_dec: List[float],\n",
    "        q_dims: List[int] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encode = DAEEncoder(q_dims, dropout_enc)\n",
    "        self.decode = Decoder(p_dims, dropout_dec)\n",
    "\n",
    "    def hybrid_forward(self, F, X):\n",
    "        return self.decode(self.encode(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so, given the following parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = 50000\n",
    "q_dims = [I] + [600, 200]\n",
    "p_dims = [200, 600] + [I]\n",
    "dropout_enc = [0.5, 0.]\n",
    "dropout_dec = [0., 0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model = MultiVAE(\n",
    "    p_dims=p_dims,\n",
    "    q_dims=q_dims,\n",
    "    dropout_enc=dropout_enc,\n",
    "    dropout_dec=dropout_dec,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiVAE(\n",
       "  (encode): VAEEncoder(\n",
       "    (q_layers): HybridSequential(\n",
       "      (0): Dropout(p = 0.5, axes=())\n",
       "      (1): Dense(50000 -> 600, linear)\n",
       "      (2): Dropout(p = 0.0, axes=())\n",
       "      (3): Dense(600 -> 400, linear)\n",
       "    )\n",
       "  )\n",
       "  (decode): Decoder(\n",
       "    (p_layers): HybridSequential(\n",
       "      (0): Dropout(p = 0.0, axes=())\n",
       "      (1): Dense(200 -> 600, linear)\n",
       "      (2): Dropout(p = 0.0, axes=())\n",
       "      (3): Dense(600 -> 50000, linear)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dae_model = MultiDAE(\n",
    "    p_dims=p_dims,\n",
    "    q_dims=q_dims,\n",
    "    dropout_enc=dropout_enc,\n",
    "    dropout_dec=dropout_dec,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiDAE(\n",
       "  (encode): DAEEncoder(\n",
       "    (q_layers): HybridSequential(\n",
       "      (0): Dropout(p = 0.5, axes=())\n",
       "      (1): Dense(50000 -> 600, linear)\n",
       "      (2): Dropout(p = 0.0, axes=())\n",
       "      (3): Dense(600 -> 200, linear)\n",
       "    )\n",
       "  )\n",
       "  (decode): Decoder(\n",
       "    (p_layers): HybridSequential(\n",
       "      (0): Dropout(p = 0.0, axes=())\n",
       "      (1): Dense(200 -> 600, linear)\n",
       "      (2): Dropout(p = 0.0, axes=())\n",
       "      (3): Dense(600 -> 50000, linear)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dae_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with all this, we can now move to notebook 03 and see how we can train and evaluate the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
